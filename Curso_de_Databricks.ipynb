{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNPby8AfcRzSYEOcSGGOOrH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seldoncode/tutorial/blob/main/Curso_de_Databricks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Curso de Databricks**\n",
        "## **√çndice**\n",
        "\n",
        "### **M√≥dulo 1: Introducci√≥n y Conceptos Fundamentales**\n",
        "1. **Tema 1: Introducci√≥n a Databricks**\n",
        "   - 1.1 ¬øQu√© es Databricks y por qu√© es importante?\n",
        "   - 1.2 Arquitectura Lakehouse y sus ventajas\n",
        "   - 1.3 Casos de uso: Data Engineering, Data Science, Analytics, ML\n",
        "   - 1.4 Ecosistema: Relaci√≥n con Apache Spark y la nube\n",
        "\n",
        "2. **Tema 2: Fundamentos de Apache Spark**\n",
        "   - 2.1 ¬øQu√© es Apache Spark y procesamiento distribuido?\n",
        "   - 2.2 RDDs, DataFrames y Datasets\n",
        "   - 2.3 Transformaciones vs Acciones (lazy evaluation)\n",
        "   - 2.4 Spark UI b√°sico para monitoreo\n",
        "\n",
        "### **M√≥dulo 2: Primeros Pasos Pr√°cticos**\n",
        "3. **Tema 3: Configuraci√≥n del Entorno**\n",
        "   - 3.1 Creaci√≥n de cuenta en Databricks (Free Edition)\n",
        "   - 3.2 Tour por la interfaz: Workspace, Data, Compute, Workflows\n",
        "   - 3.3 Creaci√≥n del primer cluster\n",
        "   - 3.4 Configuraci√≥n b√°sica de clusters (autoscaling, autotermination)\n",
        "\n",
        "4. **Tema 4: Introducci√≥n a Notebooks**\n",
        "   - 4.1 Creaci√≥n y ejecuci√≥n de notebooks\n",
        "   - 4.2 Lenguajes disponibles y cambio entre ellos\n",
        "   - 4.3 Magic commands (%python, %sql, %fs, %sh)\n",
        "   - 4.4 Widgets para parametrizaci√≥n\n",
        "\n",
        "### **M√≥dulo 3: Trabajando con Datos**\n",
        "5. **Tema 5: Gesti√≥n de Datos en Databricks**\n",
        "   - 5.1 DBFS (Databricks File System) y rutas\n",
        "   - 5.2 Carga de datos desde m√∫ltiples fuentes (CSV, JSON, Parquet)\n",
        "   - 5.3 **Introducci√≥n a Delta Lake: ventajas y caracter√≠sticas**\n",
        "   - 5.4 Cat√°logo de datos y tablas (managed vs external)\n",
        "\n",
        "6. **Tema 6: Manipulaci√≥n de Datos con PySpark**\n",
        "   - 6.1 Creaci√≥n de DataFrames\n",
        "   - 6.2 Operaciones b√°sicas: select, filter, withColumn\n",
        "   - 6.3 Transformaciones comunes: groupBy, join, orderBy\n",
        "   - 6.4 Manejo de valores nulos y duplicados\n",
        "\n",
        "7. **Tema 7: SQL en Databricks**\n",
        "   - 7.1 SQL en notebooks\n",
        "   - 7.2 Creaci√≥n de tablas y vistas\n",
        "   - 7.3 Consultas SQL b√°sicas y avanzadas\n",
        "   - 7.4 **SQL Warehouses (introducci√≥n b√°sica)**\n",
        "\n",
        "### **M√≥dulo 4: Delta Lake (Fundamental para Databricks)**\n",
        "8. **Tema 8: Trabajando con Delta Lake**\n",
        "   - 8.1 Creaci√≥n de tablas Delta\n",
        "   - 8.2 ACID transactions en la pr√°ctica\n",
        "   - 8.3 Time Travel y versionado\n",
        "   - 8.4 Operaciones MERGE, UPDATE, DELETE\n",
        "\n",
        "### **M√≥dulo 5: An√°lisis y Visualizaci√≥n**\n",
        "9. **Tema 9: An√°lisis de Datos**\n",
        "   - 9.1 Agregaciones y estad√≠sticas descriptivas\n",
        "   - 9.2 Window functions\n",
        "   - 9.3 **User Defined Functions (UDF) - b√°sico**\n",
        "\n",
        "10. **Tema 10: Visualizaci√≥n**\n",
        "    - 10.1 Visualizaciones nativas en notebooks\n",
        "    - 10.2 Dashboards b√°sicos\n",
        "    - 10.3 Integraci√≥n con herramientas externas (opcional)\n",
        "\n",
        "### **M√≥dulo 6: Workflows y Automatizaci√≥n**\n",
        "11. **Tema 11: Jobs y Workflows**\n",
        "    - 11.1 ¬øQu√© son los Jobs en Databricks?\n",
        "    - 11.2 Creaci√≥n de un Job b√°sico\n",
        "    - 11.3 Programaci√≥n y triggers\n",
        "    - 11.4 Monitoreo de ejecuciones\n",
        "\n",
        "### **M√≥dulo 7: Buenas Pr√°cticas y Gesti√≥n**\n",
        "12. **Tema 12: Control de Versiones**\n",
        "    - 12.1 Integraci√≥n con Git (Repos)\n",
        "    - 12.2 Workflows de desarrollo (dev/prod)\n",
        "    - 12.3 Organizaci√≥n del workspace\n",
        "\n",
        "13. **Tema 13: Optimizaci√≥n y Mejores Pr√°cticas**\n",
        "    - 13.1 Gesti√≥n eficiente de clusters y costos\n",
        "    - 13.2 Particionamiento de datos\n",
        "    - 13.3 Optimizaci√≥n de Delta (OPTIMIZE, Z-ORDER)\n",
        "    - 13.4 Debugging y Spark UI avanzado\n",
        "\n",
        "### **M√≥dulo 8: Proyecto Final**\n",
        "14. **Tema 14: Proyecto Integrador**\n",
        "    - 14.1 ETL completo: ingesta ‚Üí transformaci√≥n ‚Üí almacenamiento\n",
        "    - 14.2 An√°lisis y visualizaci√≥n\n",
        "    - 14.3 Automatizaci√≥n con Jobs\n",
        "    - 14.4 Documentaci√≥n y presentaci√≥n\n",
        "\n",
        "### **Recursos y Pr√≥ximos Pasos**\n",
        "15. **Tema 15: Continuando el Aprendizaje**\n",
        "    - 15.1 Certificaciones Databricks\n",
        "    - 15.2 Temas avanzados sugeridos\n",
        "    - 15.3 Comunidad y recursos\n",
        "\n",
        "---\n",
        "\n",
        "### Duraci√≥n estimada\n",
        "- 8 semanas\n",
        "\n",
        "### ¬øQu√© aprender√°s en este curso?\n",
        "\n",
        "Este curso te guiar√° desde cero en el mundo de **Databricks**, la plataforma l√≠der para anal√≠tica de datos a gran escala. Al finalizar, ser√°s capaz de:\n",
        "\n",
        "- Comprender qu√© es Databricks y c√≥mo se integra en el ecosistema moderno de datos\n",
        "- Trabajar con Apache Spark de forma pr√°ctica\n",
        "- Procesar y analizar grandes vol√∫menes de datos\n",
        "- Crear pipelines de datos automatizados\n",
        "- Aplicar buenas pr√°cticas en ingenier√≠a de datos\n",
        "\n",
        "### Metodolog√≠a del Curso\n",
        "\n",
        "Este curso est√° dise√±ado con un enfoque **100% pr√°ctico**:\n",
        "\n",
        "- **Teor√≠a m√≠nima necesaria**: Solo los conceptos esenciales\n",
        "- **Ejercicios hands-on**: Cada concepto se practica inmediatamente\n",
        "- **Ejemplos del mundo real**: Casos de uso aplicables a tu trabajo\n",
        "- **Proyecto integrador**: Construcci√≥n de un pipeline completo de datos\n",
        "\n",
        "### Requisitos Previos\n",
        "\n",
        "Para aprovechar al m√°ximo este curso, necesitas:\n",
        "\n",
        "1. **Conocimientos b√°sicos de Python** (variables, funciones, estructuras de control)\n",
        "2. **Familiaridad con SQL** (SELECT, WHERE, JOIN b√°sicos)\n",
        "3. **Conceptos b√°sicos de bases de datos** (qu√© es una tabla, registro, columna)\n",
        "4. **Acceso a internet** para usar Databricks Community Edition\n",
        "\n",
        "> üí° **Nota**: Si no tienes experiencia con Python o SQL, no te preocupes. Iremos paso a paso y los conceptos se explican de forma clara.\n",
        "\n",
        "### Estructura de los Apuntes\n",
        "\n",
        "Estos apuntes siguen una estructura consistente:\n",
        "- üìö **Teor√≠a**: Conceptos fundamentales explicados de forma clara\n",
        "- üíª **Pr√°ctica**: C√≥digo ejecutable que puedes probar\n",
        "- ‚ö†Ô∏è **Importante**: Puntos cr√≠ticos a recordar\n",
        "- üí° **Tip**: Consejos y mejores pr√°cticas\n",
        "- üéØ **Ejercicio**: Retos para practicar lo aprendido\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YYbcVWVpOEd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **M√≥dulo 1: Introducci√≥n y Conceptos Fundamentales**"
      ],
      "metadata": {
        "id": "zsFQZKEaSjVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tema 1: Introducci√≥n a Databricks**"
      ],
      "metadata": {
        "id": "h8DZ0QAPSkHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 ¬øQu√© es Databricks y por qu√© es importante?**\n",
        "\n",
        "#### **¬øQu√© es Databricks?**\n",
        "\n",
        "**Databricks** es una plataforma unificada de anal√≠tica de datos construida sobre **Apache Spark**. Fue fundada en 2013 por los creadores originales de Apache Spark (en la Universidad de Berkeley).\n",
        "\n",
        "Piensa en Databricks como un **entorno de trabajo completo** que te permite:\n",
        "\n",
        "- **Procesar** grandes cantidades de datos (desde megabytes hasta petabytes)\n",
        "- **Colaborar** con tu equipo en tiempo real (como Google Docs, pero para datos)\n",
        "- **Automatizar** pipelines de datos complejos\n",
        "- **Analizar** datos con SQL, Python, Scala o R\n",
        "- **Entrenar** modelos de Machine Learning a escala\n",
        "- **Orquestar** workflows de principio a fin\n",
        "\n",
        "#### **Analog√≠a para entenderlo mejor**\n",
        "\n",
        "Imagina que eres un chef:\n",
        "\n",
        "- **Excel/Python local** = Tu cocina casera (buenos para platos peque√±os)\n",
        "- **Databricks** = Una cocina industrial profesional (dise√±ada para vol√∫menes grandes, equipos, y eficiencia)\n",
        "\n",
        "As√≠ como una cocina industrial tiene equipos especializados, procesos optimizados y permite que varios chefs trabajen simult√°neamente, Databricks hace lo mismo con tus datos.\n",
        "\n",
        "#### **¬øPor qu√© es importante Databricks?**\n",
        "\n",
        "##### **1. Resuelve problemas del mundo real**\n",
        "\n",
        "Cuando trabajas con datos en 2025, te enfrentas a:\n",
        "\n",
        "```python\n",
        "# Problema 1: VOLUMEN - Datos demasiado grandes\n",
        "# Tu laptop tiene 16GB RAM, pero tus datos pesan 500GB\n",
        "# ‚ùå Pandas no puede cargar todo en memoria\n",
        "# ‚úÖ Databricks distribuye el procesamiento en m√∫ltiples m√°quinas\n",
        "\n",
        "# Problema 2: VELOCIDAD - Procesamiento lento\n",
        "# Un an√°lisis en tu laptop tarda 8 horas\n",
        "# ‚ùå No puedes esperar tanto para tomar decisiones\n",
        "# ‚úÖ Databricks lo procesa en minutos usando procesamiento paralelo\n",
        "\n",
        "# Problema 3: COLABORACI√ìN - Trabajo aislado\n",
        "# Tu c√≥digo funciona en tu m√°quina, pero no en la de tu colega\n",
        "# ‚ùå \"En mi m√°quina funciona\"\n",
        "# ‚úÖ Databricks provee un entorno compartido y consistente\n",
        "```\n",
        "\n",
        "##### **2. Unifica el ciclo completo de datos**\n",
        "\n",
        "Antes de Databricks, necesitabas m√∫ltiples herramientas:\n",
        "\n",
        "```\n",
        "Ingesta de datos     ‚Üí  Herramienta A\n",
        "Transformaci√≥n       ‚Üí  Herramienta B  \n",
        "An√°lisis             ‚Üí  Herramienta C\n",
        "Machine Learning     ‚Üí  Herramienta D\n",
        "Orquestaci√≥n         ‚Üí  Herramienta E\n",
        "Visualizaci√≥n        ‚Üí  Herramienta F\n",
        "```\n",
        "\n",
        "Con Databricks, todo est√° en un solo lugar:\n",
        "\n",
        "```\n",
        "Databricks Lakehouse Platform\n",
        "‚îú‚îÄ‚îÄ Ingesta de datos\n",
        "‚îú‚îÄ‚îÄ Transformaci√≥n (ETL/ELT)\n",
        "‚îú‚îÄ‚îÄ Almacenamiento (Delta Lake)\n",
        "‚îú‚îÄ‚îÄ An√°lisis SQL\n",
        "‚îú‚îÄ‚îÄ Machine Learning\n",
        "‚îú‚îÄ‚îÄ Orquestaci√≥n (Jobs/Workflows)\n",
        "‚îî‚îÄ‚îÄ Dashboards\n",
        "```\n",
        "\n",
        "##### **3. Construido sobre Apache Spark**\n",
        "\n",
        "Apache Spark es el motor de procesamiento distribuido m√°s usado en el mundo para Big Data. Databricks te da acceso a Spark sin la complejidad de configurarlo y mantenerlo.\n",
        "\n",
        "```python\n",
        "# Sin Databricks: Configurar Spark localmente\n",
        "# - Instalar Java\n",
        "# - Instalar Scala\n",
        "# - Configurar SPARK_HOME\n",
        "# - Gestionar dependencias\n",
        "# - Configurar cluster\n",
        "# - Mantener versiones...\n",
        "# ‚è±Ô∏è Puede tomar d√≠as\n",
        "\n",
        "# Con Databricks:\n",
        "# 1. Crear cuenta\n",
        "# 2. Crear cluster (3 clics)\n",
        "# 3. Empezar a trabajar\n",
        "# ‚è±Ô∏è 5 minutos\n",
        "```\n",
        "\n",
        "#### **Casos de uso reales**\n",
        "\n",
        "##### **Ejemplo 1: E-commerce**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "Problema: Una tienda online tiene millones de transacciones diarias\n",
        "y necesita an√°lisis en tiempo real.\n",
        "\n",
        "Con Databricks:\n",
        "1. Ingesta de logs de servidor (streaming)\n",
        "2. Procesamiento de eventos de compra en tiempo real\n",
        "3. Detecci√≥n de fraude usando ML\n",
        "4. Dashboards para el equipo de negocio\n",
        "5. Recomendaciones personalizadas\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "##### **Ejemplo 2: Salud**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "Problema: Un hospital necesita analizar historiales m√©dicos de\n",
        "millones de pacientes para investigaci√≥n.\n",
        "\n",
        "Con Databricks:\n",
        "1. Unificaci√≥n de datos de m√∫ltiples sistemas\n",
        "2. Anonimizaci√≥n de datos sensibles\n",
        "3. An√°lisis estad√≠stico a gran escala\n",
        "4. Modelos predictivos para diagn√≥sticos\n",
        "5. Cumplimiento normativo (HIPAA, GDPR)\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "##### **Ejemplo 3: Finanzas**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "Problema: Un banco procesa millones de transacciones y necesita\n",
        "detectar actividades sospechosas.\n",
        "\n",
        "Con Databricks:\n",
        "1. Ingesta de transacciones en tiempo real\n",
        "2. An√°lisis de patrones hist√≥ricos\n",
        "3. Modelos de detecci√≥n de anomal√≠as\n",
        "4. Reportes regulatorios automatizados\n",
        "5. Data governance y auditor√≠a\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "#### **¬øQui√©n usa Databricks?**\n",
        "\n",
        "Empresas de todos los tama√±os y sectores:\n",
        "\n",
        "- **Tecnolog√≠a**: Comcast, Shell, H&M\n",
        "- **Finanzas**: ING, HSBC, Capital One\n",
        "- **Retail**: Walgreens, Cond√© Nast\n",
        "- **Salud**: Regeneron Pharmaceuticals\n",
        "- **Media**: Fox, NBC Universal\n",
        "\n",
        "#### **Ventajas clave de Databricks**\n",
        "\n",
        "```python\n",
        "# 1. ESCALABILIDAD\n",
        "# Procesa desde KB hasta PB sin cambiar tu c√≥digo\n",
        "df = spark.read.parquet(\"data.parquet\")  # Funciona igual con 1GB o 1TB\n",
        "\n",
        "# 2. VELOCIDAD\n",
        "# Procesamiento distribuido y en memoria\n",
        "# Lo que tarda horas en Pandas, puede tomar minutos\n",
        "\n",
        "# 3. COLABORACI√ìN\n",
        "# Notebooks compartidos en tiempo real\n",
        "# Control de versiones integrado\n",
        "\n",
        "# 4. MULTI-LENGUAJE\n",
        "# Python, SQL, Scala, R en el mismo notebook\n",
        "\n",
        "# 5. GESTI√ìN AUTOMATIZADA\n",
        "# Clusters que se crean/destruyen autom√°ticamente\n",
        "# Optimizaci√≥n de costos\n",
        "\n",
        "# 6. SEGURIDAD Y GOVERNANCE\n",
        "# Control de acceso granular\n",
        "# Auditor√≠a completa\n",
        "# Cumplimiento normativo\n",
        "```\n",
        "\n",
        "#### **Ecosistema Databricks**\n",
        "\n",
        "Databricks no trabaja solo, se integra con:\n",
        "\n",
        "```\n",
        "‚òÅÔ∏è Clouds:\n",
        "   - AWS (Amazon Web Services)\n",
        "   - Azure (Microsoft)\n",
        "   - GCP (Google Cloud Platform)\n",
        "\n",
        "üìä Almacenamiento:\n",
        "   - S3, Azure Blob, Google Cloud Storage\n",
        "   - Delta Lake (formato optimizado)\n",
        "\n",
        "üîß Herramientas:\n",
        "   - Power BI, Tableau (visualizaci√≥n)\n",
        "   - dbt (transformaci√≥n)\n",
        "   - Airflow (orquestaci√≥n)\n",
        "   - MLflow (ML lifecycle)\n",
        "\n",
        "üîå Conectores:\n",
        "   - Bases de datos (MySQL, PostgreSQL, SQL Server)\n",
        "   - Data warehouses (Snowflake, Redshift)\n",
        "   - APIs y servicios web\n",
        "```\n",
        "\n",
        "#### **Databricks vs Otras Herramientas**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DATABRICKS vs PANDAS\n",
        "--------------------\n",
        "Pandas: Excelente para datasets peque√±os (<5GB) en una sola m√°quina\n",
        "Databricks: Dise√±ado para datasets grandes, procesamiento distribuido\n",
        "\n",
        "DATABRICKS vs SNOWFLAKE\n",
        "-----------------------\n",
        "Snowflake: Data warehouse optimizado para SQL\n",
        "Databricks: Plataforma completa (SQL + Python + ML + Streaming)\n",
        "\n",
        "DATABRICKS vs AWS EMR\n",
        "---------------------\n",
        "EMR: Infraestructura Spark en AWS (m√°s control, m√°s complejidad)\n",
        "Databricks: Plataforma gestionada (menos administraci√≥n, m√°s productividad)\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica Inicial - Comprendiendo el Concepto**\n",
        "\n",
        "Antes de instalar nada, vamos a simular el problema que Databricks resuelve:\n",
        "\n",
        "```python\n",
        "# Celda de Python en tu Jupyter Notebook\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Simulemos procesar un dataset \"grande\" con Pandas\n",
        "print(\"üêº Procesamiento con Pandas (simulaci√≥n)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Crear un dataset de ejemplo\n",
        "data = {\n",
        "    'cliente_id': range(1000000),\n",
        "    'compra': [100 + i % 500 for i in range(1000000)],\n",
        "    'categoria': ['A', 'B', 'C'] * 333334\n",
        "}\n",
        "\n",
        "# Medir tiempo\n",
        "inicio = time.time()\n",
        "df_pandas = pd.DataFrame(data)\n",
        "resultado = df_pandas.groupby('categoria')['compra'].sum()\n",
        "fin = time.time()\n",
        "\n",
        "print(f\"‚úÖ Tiempo de procesamiento: {fin - inicio:.4f} segundos\")\n",
        "print(f\"üìä Resultado:\\n{resultado}\")\n",
        "print(\"\\n‚ö†Ô∏è Limitaci√≥n: Pandas requiere que TODO quepa en memoria\")\n",
        "print(\"‚ö†Ô∏è Con 10M, 100M o 1B de filas... tu laptop colapsar√≠a\")\n",
        "```\n",
        "\n",
        "**Salida esperada:**\n",
        "```\n",
        "üêº Procesamiento con Pandas (simulaci√≥n)\n",
        "--------------------------------------------------\n",
        "‚úÖ Tiempo de procesamiento: 0.1234 segundos\n",
        "üìä Resultado:\n",
        "categoria\n",
        "A    166583350000\n",
        "B    166583350000\n",
        "C    166666650000\n",
        "Name: compra, dtype: int64\n",
        "\n",
        "‚ö†Ô∏è Limitaci√≥n: Pandas requiere que TODO quepa en memoria\n",
        "‚ö†Ô∏è Con 10M, 100M o 1B de filas... tu laptop colapsar√≠a\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 1.1 - Reflexi√≥n**\n",
        "\n",
        "Antes de continuar, responde en una celda Markdown:\n",
        "\n",
        "```markdown\n",
        "### Reflexi√≥n Personal\n",
        "\n",
        "1. **¬øQu√© tipo de datos manejo actualmente en mi trabajo/estudios?**\n",
        "   - Volumen aproximado:\n",
        "   - Formato (CSV, Excel, JSON, etc.):\n",
        "   - Frecuencia de actualizaci√≥n:\n",
        "\n",
        "2. **¬øQu√© problemas enfrento con mis herramientas actuales?**\n",
        "   - [ ] Lentitud en el procesamiento\n",
        "   - [ ] Datos demasiado grandes para mi m√°quina\n",
        "   - [ ] Dificultad para compartir an√°lisis\n",
        "   - [ ] Falta de automatizaci√≥n\n",
        "   - [ ] Otro: _______________\n",
        "\n",
        "3. **¬øC√≥mo podr√≠a Databricks ayudarme?**\n",
        "   (Tu respuesta aqu√≠)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "```markdown\n",
        "1. **Databricks = Plataforma unificada** para todo el ciclo de vida de datos\n",
        "2. **Construida sobre Apache Spark** (procesamiento distribuido)\n",
        "3. **Resuelve problemas de escala** que herramientas tradicionales no pueden\n",
        "4. **Colaborativa y cloud-native** desde el dise√±o\n",
        "5. **Multi-prop√≥sito**: Data Engineering, Data Science, Analytics, ML\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "```markdown\n",
        "‚úÖ Databricks no reemplaza Excel o Pandas para an√°lisis peque√±os\n",
        "   ‚Üí √ösalo cuando el volumen o la complejidad lo justifique\n",
        "\n",
        "‚úÖ La curva de aprendizaje vale la pena\n",
        "   ‚Üí Es una habilidad muy demandada en el mercado laboral\n",
        "\n",
        "‚úÖ Empieza con Community Edition (gratis)\n",
        "   ‚Üí Practica sin costos antes de usar versiones empresariales\n",
        "\n",
        "‚úÖ Databricks es el futuro del an√°lisis de datos\n",
        "   ‚Üí Grandes empresas est√°n migrando a arquitecturas Lakehouse\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìö **Recursos Adicionales**\n",
        "\n",
        "```markdown\n",
        "üìñ Documentaci√≥n oficial: https://docs.databricks.com\n",
        "üéì Databricks Academy (cursos gratuitos): https://www.databricks.com/learn\n",
        "üì∫ Canal de YouTube: Databricks\n",
        "üê¶ Twitter: @databricks\n",
        "üí¨ Community Forums: https://community.databricks.com\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Pr√≥ximos Pasos**\n",
        "\n",
        "En la siguiente secci√≥n (1.2), profundizaremos en la **Arquitectura Lakehouse** y entenderemos c√≥mo Databricks organiza y procesa los datos de forma eficiente.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "7CFtVf2_O6C4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Arquitectura Lakehouse y sus ventajas**\n",
        "\n",
        "#### **El Problema Hist√≥rico: Data Warehouse vs Data Lake**\n",
        "\n",
        "Antes de Databricks, las empresas ten√≠an que elegir entre dos arquitecturas incompatibles:\n",
        "\n",
        "```markdown\n",
        "üìä DATA WAREHOUSE (Almac√©n de Datos)\n",
        "‚îú‚îÄ Ejemplos: Snowflake, Redshift, BigQuery\n",
        "‚îú‚îÄ Fortalezas:\n",
        "‚îÇ  ‚úÖ Excelente para SQL y BI\n",
        "‚îÇ  ‚úÖ Rendimiento r√°pido en consultas estructuradas\n",
        "‚îÇ  ‚úÖ ACID transactions (confiabilidad)\n",
        "‚îÇ  ‚úÖ Esquema bien definido\n",
        "‚îÇ\n",
        "‚îî‚îÄ Debilidades:\n",
        "   ‚ùå Solo datos estructurados (tablas)\n",
        "   ‚ùå Costoso para grandes vol√∫menes\n",
        "   ‚ùå No apto para Machine Learning\n",
        "   ‚ùå Inflexible para datos no estructurados\n",
        "\n",
        "üèûÔ∏è DATA LAKE (Lago de Datos)\n",
        "‚îú‚îÄ Ejemplos: HDFS, S3, Azure Data Lake\n",
        "‚îú‚îÄ Fortalezas:\n",
        "‚îÇ  ‚úÖ Almacena TODO tipo de datos (estructurados, semi-estructurados, no estructurados)\n",
        "‚îÇ  ‚úÖ Econ√≥mico (almacenamiento barato)\n",
        "‚îÇ  ‚úÖ Flexible y escalable\n",
        "‚îÇ  ‚úÖ Bueno para ML y an√°lisis exploratorio\n",
        "‚îÇ\n",
        "‚îî‚îÄ Debilidades:\n",
        "   ‚ùå Sin ACID transactions (inconsistencias)\n",
        "   ‚ùå Rendimiento SQL pobre\n",
        "   ‚ùå \"Data Swamp\" (pantano de datos) sin governance\n",
        "   ‚ùå Complejidad en la gesti√≥n\n",
        "```\n",
        "\n",
        "#### **Resultado: Arquitectura Dual (El Problema)**\n",
        "\n",
        "Las empresas terminaban con **dos sistemas separados**:\n",
        "\n",
        "```python\n",
        "# Arquitectura tradicional (problem√°tica)\n",
        "\n",
        "\"\"\"\n",
        "1. INGESTA DE DATOS\n",
        "   ‚Üì\n",
        "2. DATA LAKE (almacenamiento barato)\n",
        "   - Logs, JSONs, im√°genes, videos\n",
        "   - Datos crudos sin procesar\n",
        "   ‚Üì\n",
        "3. ETL (Extract, Transform, Load)\n",
        "   - Procesar y limpiar datos\n",
        "   - ‚ö†Ô∏è Proceso costoso y lento\n",
        "   ‚Üì\n",
        "4. DATA WAREHOUSE (almacenamiento caro)\n",
        "   - Solo datos estructurados y limpios\n",
        "   - Para reportes y BI\n",
        "   ‚Üì\n",
        "5. PROBLEMAS:\n",
        "   ‚ùå Duplicaci√≥n de datos (2x costo)\n",
        "   ‚ùå Dos sistemas que mantener\n",
        "   ‚ùå Latencia alta (ETL batch nocturno)\n",
        "   ‚ùå Complejidad operacional\n",
        "   ‚ùå Datos desactualizados para decisiones\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "#### **La Soluci√≥n: Lakehouse Architecture**\n",
        "\n",
        "Databricks introdujo **Lakehouse**: una arquitectura que combina lo mejor de ambos mundos.\n",
        "\n",
        "```markdown\n",
        "üèõÔ∏è LAKEHOUSE = Data Lake + Data Warehouse\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ         DATABRICKS LAKEHOUSE                ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  ‚úÖ Almacenamiento econ√≥mico (como Data Lake)‚îÇ\n",
        "‚îÇ  ‚úÖ ACID transactions (como Data Warehouse) ‚îÇ\n",
        "‚îÇ  ‚úÖ Performance SQL (como Data Warehouse)   ‚îÇ\n",
        "‚îÇ  ‚úÖ ML y an√°lisis (como Data Lake)          ‚îÇ\n",
        "‚îÇ  ‚úÖ Todos los tipos de datos                ‚îÇ\n",
        "‚îÇ  ‚úÖ Una sola copia de los datos             ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "#### **Componentes de la Arquitectura Lakehouse**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CAPAS DE LA ARQUITECTURA LAKEHOUSE\n",
        "\"\"\"\n",
        "\n",
        "# 1. STORAGE LAYER (Capa de Almacenamiento)\n",
        "storage = {\n",
        "    'tecnolog√≠a': 'Cloud Object Storage (S3, ADLS, GCS)',\n",
        "    'formato': 'Delta Lake (Parquet optimizado)',\n",
        "    'caracter√≠sticas': [\n",
        "        'Almacenamiento econ√≥mico',\n",
        "        'Escalabilidad infinita',\n",
        "        'Durabilidad y disponibilidad'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 2. DELTA LAKE (Motor de Almacenamiento)\n",
        "delta_lake = {\n",
        "    'funci√≥n': 'A√±ade ACID y confiabilidad al Data Lake',\n",
        "    'caracter√≠sticas': [\n",
        "        'ACID transactions',\n",
        "        'Time Travel (versionado)',\n",
        "        'Schema enforcement',\n",
        "        'Unified batch & streaming'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 3. COMPUTE LAYER (Capa de Procesamiento)\n",
        "compute = {\n",
        "    'motor': 'Apache Spark optimizado',\n",
        "    'caracter√≠sticas': [\n",
        "        'Procesamiento distribuido',\n",
        "        'Auto-scaling',\n",
        "        'Multi-lenguaje (SQL, Python, Scala, R)'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 4. GOVERNANCE & SECURITY (Gobernanza)\n",
        "governance = {\n",
        "    'herramienta': 'Unity Catalog',\n",
        "    'caracter√≠sticas': [\n",
        "        'Control de acceso centralizado',\n",
        "        'Auditor√≠a',\n",
        "        'Lineage (trazabilidad)',\n",
        "        'Data discovery'\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Visualizaci√≥n de la Arquitectura**\n",
        "\n",
        "```markdown\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    USUARIOS Y APLICACIONES               ‚îÇ\n",
        "‚îÇ  [Data Analysts] [Data Scientists] [ML Engineers] [Apps] ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                         ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ               DATABRICKS WORKSPACE                       ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
        "‚îÇ  ‚îÇNotebooks ‚îÇ ‚îÇ SQL Editor‚îÇ ‚îÇJobs/     ‚îÇ ‚îÇDashboards‚îÇ    ‚îÇ\n",
        "‚îÇ  ‚îÇ          ‚îÇ ‚îÇ           ‚îÇ ‚îÇWorkflows ‚îÇ ‚îÇ          ‚îÇ    ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                         ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                  COMPUTE LAYER                           ‚îÇ\n",
        "‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ\n",
        "‚îÇ         ‚îÇ    Apache Spark Clusters       ‚îÇ               ‚îÇ\n",
        "‚îÇ         ‚îÇ  (Auto-scaling, Optimizado)    ‚îÇ               ‚îÇ\n",
        "‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                         ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                   DELTA LAKE                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ\n",
        "‚îÇ  ‚îÇ  ACID Transactions | Time Travel | Schema      ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îÇ  Enforcement | Optimization | Indexing         ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                         ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                 CLOUD STORAGE                            ‚îÇ\n",
        "‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ\n",
        "‚îÇ    ‚îÇ AWS S3   ‚îÇ  ‚îÇ Azure    ‚îÇ  ‚îÇ Google   ‚îÇ              ‚îÇ\n",
        "‚îÇ    ‚îÇ          ‚îÇ  ‚îÇ ADLS     ‚îÇ  ‚îÇ Cloud    ‚îÇ              ‚îÇ\n",
        "‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ\n",
        "‚îÇ                                                          ‚îÇ\n",
        "‚îÇ  [Delta Tables] [Parquet] [JSON] [CSV] [Images] [Logs]   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "#### **Ventajas del Lakehouse**\n",
        "\n",
        "##### **1. Costo Reducido**\n",
        "\n",
        "```python\n",
        "# ANTES: Arquitectura Dual\n",
        "costos_antes = {\n",
        "    'data_lake': 100,      # Almacenamiento en S3\n",
        "    'data_warehouse': 500, # Snowflake/Redshift\n",
        "    'etl_tools': 100,      # Herramientas ETL\n",
        "    'total': 700\n",
        "}\n",
        "\n",
        "# AHORA: Lakehouse\n",
        "costos_ahora = {\n",
        "    'storage': 100,        # Solo almacenamiento en cloud\n",
        "    'databricks': 250,     # Plataforma unificada\n",
        "    'total': 350\n",
        "}\n",
        "\n",
        "ahorro = (costos_antes['total'] - costos_ahora['total']) / costos_antes['total']\n",
        "print(f\"üí∞ Ahorro: {ahorro:.0%}\")  # 50% de ahorro aproximado\n",
        "```\n",
        "\n",
        "**Salida:**\n",
        "```\n",
        "üí∞ Ahorro: 50%\n",
        "```\n",
        "\n",
        "##### **2. Rendimiento Superior**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "OPTIMIZACIONES EN LAKEHOUSE\n",
        "\"\"\"\n",
        "\n",
        "# Delta Lake optimiza autom√°ticamente\n",
        "optimizaciones = {\n",
        "    'z_ordering': 'Organiza datos para consultas r√°pidas',\n",
        "    'data_skipping': 'Salta archivos irrelevantes',\n",
        "    'caching': 'Datos en memoria para acceso r√°pido',\n",
        "    'photon': 'Motor nativo C++ (hasta 12x m√°s r√°pido)',\n",
        "    'liquid_clustering': 'Clustering autom√°tico y adaptativo'\n",
        "}\n",
        "\n",
        "# Ejemplo pr√°ctico\n",
        "\"\"\"\n",
        "Consulta t√≠pica en Data Lake tradicional: 45 segundos\n",
        "Misma consulta en Lakehouse con Delta: 3 segundos\n",
        "‚Üí 15x m√°s r√°pido\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "##### **3. Confiabilidad (ACID Transactions)**\n",
        "\n",
        "```python\n",
        "# Problema en Data Lake tradicional\n",
        "\"\"\"\n",
        "‚ùå ESCENARIO SIN ACID:\n",
        "\n",
        "1. Inicias escritura de 1M registros\n",
        "2. A mitad del proceso, falla la conexi√≥n\n",
        "3. Resultado: 500K registros escritos, 500K perdidos\n",
        "4. Datos corruptos e inconsistentes\n",
        "5. No puedes hacer rollback\n",
        "\"\"\"\n",
        "\n",
        "# Soluci√≥n en Lakehouse con Delta\n",
        "\"\"\"\n",
        "‚úÖ ESCENARIO CON ACID (Delta Lake):\n",
        "\n",
        "1. Inicias escritura de 1M registros\n",
        "2. Si falla, Delta hace rollback autom√°tico\n",
        "3. Resultado: 0 registros (transacci√≥n completa o nada)\n",
        "4. Datos siempre consistentes\n",
        "5. Puedes ver versiones anteriores\n",
        "\"\"\"\n",
        "\n",
        "# C√≥digo ejemplo\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "# Escritura ACID garantizada\n",
        "df.write.format(\"delta\").mode(\"append\").save(\"/data/ventas\")\n",
        "\n",
        "# Si algo falla, los datos anteriores permanecen intactos\n",
        "# No hay corrupci√≥n ni inconsistencias\n",
        "```\n",
        "\n",
        "##### **4. Time Travel (Viaje en el Tiempo)**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CAPACIDAD √öNICA DE DELTA LAKE\n",
        "\"\"\"\n",
        "\n",
        "# Ver versiones hist√≥ricas de tus datos\n",
        "# √ötil para:\n",
        "# - Auditor√≠a\n",
        "# - Recuperaci√≥n de errores\n",
        "# - An√°lisis temporal\n",
        "# - Compliance\n",
        "\n",
        "# Ejemplo pr√°ctico\n",
        "```\n",
        "\n",
        "```sql\n",
        "-- Celda SQL en tu notebook\n",
        "\n",
        "-- Ver la tabla actual\n",
        "SELECT * FROM ventas;\n",
        "\n",
        "-- Ver c√≥mo estaba hace 7 d√≠as\n",
        "SELECT * FROM ventas VERSION AS OF 7;\n",
        "\n",
        "-- Ver estado de ayer a las 3pm\n",
        "SELECT * FROM ventas TIMESTAMP AS OF '2026-01-31 15:00:00';\n",
        "\n",
        "-- Restaurar versi√≥n anterior si cometiste un error\n",
        "RESTORE TABLE ventas TO VERSION AS OF 5;\n",
        "```\n",
        "\n",
        "##### **5. Unificaci√≥n de Batch y Streaming**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ANTES: Dos sistemas diferentes\n",
        "\"\"\"\n",
        "# Sistema 1: Batch (procesos nocturnos)\n",
        "batch_pipeline = \"ETL cada 24 horas ‚Üí datos desactualizados\"\n",
        "\n",
        "# Sistema 2: Streaming (tiempo real)\n",
        "streaming_pipeline = \"Infraestructura compleja ‚Üí costoso mantener\"\n",
        "\n",
        "\"\"\"\n",
        "LAKEHOUSE: Un solo sistema\n",
        "\"\"\"\n",
        "# Mismo c√≥digo para batch y streaming\n",
        "df = spark.readStream.format(\"delta\") \\\n",
        "    .option(\"readChangeFeed\", \"true\") \\\n",
        "    .table(\"eventos\")\n",
        "\n",
        "# Procesa datos en tiempo real con la misma API\n",
        "df.writeStream.format(\"delta\") \\\n",
        "    .option(\"checkpointLocation\", \"/checkpoints/\") \\\n",
        "    .table(\"eventos_procesados\")\n",
        "\n",
        "# ‚úÖ Delta Lake maneja ambos casos transparentemente\n",
        "```\n",
        "\n",
        "##### **6. Schema Evolution (Evoluci√≥n de Esquema)**\n",
        "\n",
        "```python\n",
        "# Escenario com√∫n: Tu estructura de datos cambia con el tiempo\n",
        "\n",
        "# ANTES: Data Warehouse r√≠gido\n",
        "\"\"\"\n",
        "‚ùå Problema:\n",
        "- A√±adir columna nueva requiere cambios en toda la pipeline\n",
        "- Downtime para migraciones\n",
        "- Procesos complejos y arriesgados\n",
        "\"\"\"\n",
        "\n",
        "# LAKEHOUSE: Flexible y autom√°tico\n",
        "\"\"\"\n",
        "‚úÖ Soluci√≥n con Delta Lake:\n",
        "\"\"\"\n",
        "\n",
        "# Los datos originales tienen 3 columnas\n",
        "df_original = spark.createDataFrame([\n",
        "    (1, \"Juan\", 1000),\n",
        "    (2, \"Mar√≠a\", 1500)\n",
        "], [\"id\", \"nombre\", \"salario\"])\n",
        "\n",
        "df_original.write.format(\"delta\").save(\"/data/empleados\")\n",
        "\n",
        "# M√°s tarde, necesitas a√±adir una columna\n",
        "df_nuevo = spark.createDataFrame([\n",
        "    (3, \"Pedro\", 2000, \"Ventas\"),  # Nueva columna: departamento\n",
        "    (4, \"Ana\", 1800, \"IT\")\n",
        "], [\"id\", \"nombre\", \"salario\", \"departamento\"])\n",
        "\n",
        "# Delta Lake maneja el cambio autom√°ticamente\n",
        "df_nuevo.write.format(\"delta\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .option(\"mergeSchema\", \"true\") \\\n",
        "    .save(\"/data/empleados\")\n",
        "\n",
        "# ‚úÖ Registros antiguos tendr√°n NULL en 'departamento'\n",
        "# ‚úÖ No hay downtime\n",
        "# ‚úÖ No hay migraci√≥n compleja\n",
        "```\n",
        "\n",
        "##### **7. Soporte Multi-Formato**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "LAKEHOUSE puede trabajar con CUALQUIER formato\n",
        "\"\"\"\n",
        "\n",
        "formatos_soportados = {\n",
        "    'estructurados': ['Delta', 'Parquet', 'ORC', 'Avro'],\n",
        "    'semi_estructurados': ['JSON', 'XML', 'CSV'],\n",
        "    'no_estructurados': ['Im√°genes', 'Videos', 'Audio', 'PDFs', 'Logs'],\n",
        "    'streaming': ['Kafka', 'Kinesis', 'Event Hubs']\n",
        "}\n",
        "\n",
        "# Ejemplo: Leer m√∫ltiples formatos en una misma sesi√≥n\n",
        "df_json = spark.read.json(\"/data/logs.json\")\n",
        "df_csv = spark.read.csv(\"/data/ventas.csv\", header=True)\n",
        "df_parquet = spark.read.parquet(\"/data/clientes.parquet\")\n",
        "df_delta = spark.read.format(\"delta\").load(\"/data/productos\")\n",
        "\n",
        "# Todos pueden trabajarse juntos y convertirse a Delta\n",
        "df_json.write.format(\"delta\").save(\"/delta/logs\")\n",
        "```\n",
        "\n",
        "#### **Comparaci√≥n Visual: Antes vs Lakehouse**\n",
        "\n",
        "```python\n",
        "# Celda Python para visualizar la diferencia\n",
        "\n",
        "comparacion = {\n",
        "    'Caracter√≠stica': [\n",
        "        'Tipos de datos',\n",
        "        'ACID transactions',\n",
        "        'Performance SQL',\n",
        "        'Machine Learning',\n",
        "        'Costo',\n",
        "        'Complejidad',\n",
        "        'Time Travel',\n",
        "        'Streaming',\n",
        "        'Escalabilidad'\n",
        "    ],\n",
        "    'Data Warehouse': [\n",
        "        'Solo estructurados',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ Excelente',\n",
        "        '‚ùå Limitado',\n",
        "        'üí∞üí∞üí∞ Alto',\n",
        "        'üîß Media',\n",
        "        '‚ùå No',\n",
        "        '‚ùå No',\n",
        "        '‚ö†Ô∏è Limitada'\n",
        "    ],\n",
        "    'Data Lake': [\n",
        "        'Todos',\n",
        "        '‚ùå No',\n",
        "        '‚ùå Pobre',\n",
        "        '‚úÖ Excelente',\n",
        "        'üí∞ Bajo',\n",
        "        'üîßüîßüîß Alta',\n",
        "        '‚ùå No',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ Infinita'\n",
        "    ],\n",
        "    'Lakehouse': [\n",
        "        'Todos',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ Excelente',\n",
        "        '‚úÖ Excelente',\n",
        "        'üí∞üí∞ Medio',\n",
        "        'üîß Baja',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ Infinita'\n",
        "    ]\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "df_comp = pd.DataFrame(comparacion)\n",
        "print(df_comp.to_string(index=False))\n",
        "```\n",
        "\n",
        "**Salida:**\n",
        "```\n",
        "       Caracter√≠stica    Data Warehouse          Data Lake           Lakehouse\n",
        "     Tipos de datos Solo estructurados              Todos               Todos\n",
        "  ACID transactions             ‚úÖ S√≠              ‚ùå No               ‚úÖ S√≠\n",
        "     Performance SQL       ‚úÖ Excelente           ‚ùå Pobre        ‚úÖ Excelente\n",
        "   Machine Learning         ‚ùå Limitado       ‚úÖ Excelente        ‚úÖ Excelente\n",
        "                Costo       üí∞üí∞üí∞ Alto            üí∞ Bajo          üí∞üí∞ Medio\n",
        "          Complejidad           üîß Media        üîßüîßüîß Alta            üîß Baja\n",
        "         Time Travel               ‚ùå No              ‚ùå No               ‚úÖ S√≠\n",
        "           Streaming               ‚ùå No              ‚úÖ S√≠               ‚úÖ S√≠\n",
        "       Escalabilidad      ‚ö†Ô∏è Limitada        ‚úÖ Infinita         ‚úÖ Infinita\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Entendiendo Delta Lake**\n",
        "\n",
        "Aunque a√∫n no tenemos Databricks configurado, podemos simular el concepto:\n",
        "\n",
        "```python\n",
        "# Celda Python - Simulaci√≥n conceptual\n",
        "\n",
        "\"\"\"\n",
        "SIMULACI√ìN: Por qu√© Delta Lake es superior\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Simulemos un escenario de Data Lake tradicional\n",
        "print(\"üìÅ Escenario 1: Data Lake Tradicional (Parquet)\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Escribimos datos\n",
        "datos_v1 = pd.DataFrame({\n",
        "    'id': [1, 2, 3],\n",
        "    'producto': ['Laptop', 'Mouse', 'Teclado'],\n",
        "    'precio': [1000, 20, 50]\n",
        "})\n",
        "\n",
        "print(\"Versi√≥n 1 de los datos:\")\n",
        "print(datos_v1)\n",
        "print(\"\\n‚ö†Ô∏è Problema: Si actualizamos, perdemos la versi√≥n anterior\")\n",
        "print(\"‚ö†Ô∏è No hay forma de recuperar datos si cometemos un error\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üî∑ Escenario 2: Lakehouse con Delta Lake\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Con Delta Lake (conceptualmente)\n",
        "versiones = {\n",
        "    'v1': datos_v1,\n",
        "    'v2': pd.DataFrame({\n",
        "        'id': [1, 2, 3, 4],\n",
        "        'producto': ['Laptop', 'Mouse', 'Teclado', 'Monitor'],\n",
        "        'precio': [900, 20, 50, 300]  # Laptop con descuento\n",
        "    }),\n",
        "    'v3': pd.DataFrame({\n",
        "        'id': [1, 2, 3, 4, 5],\n",
        "        'producto': ['Laptop', 'Mouse', 'Teclado', 'Monitor', 'Webcam'],\n",
        "        'precio': [900, 20, 50, 300, 80]\n",
        "    })\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Delta Lake mantiene TODAS las versiones:\")\n",
        "for version, df in versiones.items():\n",
        "    print(f\"\\n{version}: {len(df)} registros\")\n",
        "\n",
        "print(\"\\n‚úÖ Puedes volver a cualquier versi√≥n anterior\")\n",
        "print(\"‚úÖ Puedes ver qui√©n hizo cada cambio y cu√°ndo\")\n",
        "print(\"‚úÖ Auditor√≠a completa y compliance garantizado\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 1.2 - Casos de Uso**\n",
        "\n",
        "Identifica qu√© arquitectura es mejor para cada escenario:\n",
        "\n",
        "```markdown\n",
        "### Ejercicio: ¬øWarehouse, Lake, o Lakehouse?\n",
        "\n",
        "Para cada caso, indica la mejor opci√≥n y justifica:\n",
        "\n",
        "**Caso 1**: Startup con presupuesto limitado que necesita an√°lisis b√°sicos de ventas (10K registros/d√≠a)\n",
        "- Soluci√≥n: _______________\n",
        "- Justificaci√≥n: _______________\n",
        "\n",
        "**Caso 2**: Banco que procesa millones de transacciones diarias, necesita detecci√≥n de fraude en tiempo real y cumplimiento regulatorio estricto\n",
        "- Soluci√≥n: _______________\n",
        "- Justificaci√≥n: _______________\n",
        "\n",
        "**Caso 3**: Empresa de e-commerce que necesita recomendaciones personalizadas con ML, an√°lisis de clickstream, y dashboards ejecutivos\n",
        "- Soluci√≥n: _______________\n",
        "- Justificaci√≥n: _______________\n",
        "\n",
        "**Caso 4**: Peque√±a empresa con Excel files que quiere empezar a hacer an√°lisis m√°s avanzados\n",
        "- Soluci√≥n: _______________\n",
        "- Justificaci√≥n: _______________\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **Lakehouse = Data Lake + Data Warehouse** en una sola arquitectura\n",
        "2. **Delta Lake** es la tecnolog√≠a clave que hace posible Lakehouse\n",
        "3. **ACID transactions** garantizan confiabilidad en datos\n",
        "4. **Time Travel** permite auditor√≠a y recuperaci√≥n\n",
        "5. **Una sola copia de datos** reduce costos y complejidad\n",
        "6. **Mismo c√≥digo** para batch y streaming\n",
        "7. **Almacenamiento econ√≥mico** con rendimiento de warehouse\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "- El Lakehouse no es solo una moda, es el futuro de la arquitectura de datos\n",
        "- Empresas est√°n migrando activamente de arquitecturas duales a Lakehouse\n",
        "- Delta Lake es open source, no es exclusivo de Databricks\n",
        "- La inversi√≥n en aprender Lakehouse tiene alto ROI para tu carrera\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el siguiente punto **1.3 Casos de uso: Data Engineering, Data Science, Analytics, ML**?"
      ],
      "metadata": {
        "id": "CvmKHbNMSWST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3 Casos de uso: Data Engineering, Data Science, Analytics, ML**\n",
        "\n",
        "#### **Introducci√≥n: Un Workspace, M√∫ltiples Roles**\n",
        "\n",
        "Databricks es una plataforma **unificada** que sirve a diferentes roles profesionales. Aunque todos trabajan en el mismo entorno, cada rol tiene objetivos y flujos de trabajo distintos.\n",
        "\n",
        "```markdown\n",
        "üè¢ ANTES: Equipos aislados con herramientas diferentes\n",
        "\n",
        "Data Engineers  ‚Üí  Herramienta A (Airflow, Spark on EMR)\n",
        "Data Scientists ‚Üí  Herramienta B (Jupyter, R Studio)  \n",
        "Data Analysts   ‚Üí  Herramienta C (Tableau, Power BI)\n",
        "ML Engineers    ‚Üí  Herramienta D (Kubeflow, SageMaker)\n",
        "\n",
        "‚ùå Problemas:\n",
        "   - No hay colaboraci√≥n efectiva\n",
        "   - Duplicaci√≥n de trabajo\n",
        "   - Versiones inconsistentes de datos\n",
        "   - Handoffs lentos entre equipos\n",
        "\n",
        "üéØ DATABRICKS: Todos en la misma plataforma\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ      DATABRICKS WORKSPACE           ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  Data Engineers  ‚îê                  ‚îÇ\n",
        "‚îÇ  Data Scientists ‚îÇ‚Üí Mismo workspace ‚îÇ\n",
        "‚îÇ  Data Analysts   ‚îÇ‚Üí Mismos datos    ‚îÇ\n",
        "‚îÇ  ML Engineers    ‚îò‚Üí Colaboraci√≥n    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚úÖ Ventajas:\n",
        "   - Colaboraci√≥n en tiempo real\n",
        "   - Datos compartidos y consistentes\n",
        "   - Workflows integrados\n",
        "   - Menor time-to-market\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Caso de Uso 1: Data Engineering (Ingenier√≠a de Datos)**\n",
        "\n",
        "#### **¬øQu√© hace un Data Engineer?**\n",
        "\n",
        "Los Data Engineers son los **arquitectos de datos**. Construyen y mantienen la infraestructura que permite que los datos fluyan desde las fuentes hasta los consumidores.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "RESPONSABILIDADES DE DATA ENGINEERING\n",
        "\"\"\"\n",
        "\n",
        "responsabilidades = {\n",
        "    'ingesta': 'Traer datos desde m√∫ltiples fuentes',\n",
        "    'transformaci√≥n': 'Limpiar, normalizar y enriquecer datos',\n",
        "    'orquestaci√≥n': 'Automatizar pipelines de datos',\n",
        "    'calidad': 'Garantizar precisi√≥n y consistencia',\n",
        "    'optimizaci√≥n': 'Hacer que todo sea r√°pido y eficiente',\n",
        "    'governance': 'Seguridad, auditor√≠a y compliance'\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Ejemplo Pr√°ctico: Pipeline ETL para E-commerce**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CASO REAL: Tienda Online\n",
        "\n",
        "Problema:\n",
        "- Datos de ventas en base de datos transaccional (MySQL)\n",
        "- Logs de clickstream en archivos JSON\n",
        "- Inventario en archivos CSV actualizados cada hora\n",
        "- Necesitan dashboard unificado actualizado cada 15 minutos\n",
        "\n",
        "Soluci√≥n con Databricks:\n",
        "\"\"\"\n",
        "\n",
        "# ===== PASO 1: INGESTA DE DATOS =====\n",
        "\n",
        "# Leer desde MySQL\n",
        "df_ventas = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:mysql://db.empresa.com:3306/ventas\") \\\n",
        "    .option(\"dbtable\", \"transacciones\") \\\n",
        "    .option(\"user\", \"readonly_user\") \\\n",
        "    .option(\"password\", \"secret\") \\\n",
        "    .load()\n",
        "\n",
        "# Leer logs de clickstream (JSON)\n",
        "df_clickstream = spark.read \\\n",
        "    .format(\"json\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"/data/logs/clickstream/*.json\")\n",
        "\n",
        "# Leer inventario (CSV que se actualiza cada hora)\n",
        "df_inventario = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"/data/inventario/stock_*.csv\")\n",
        "\n",
        "print(\"‚úÖ Datos ingestados desde 3 fuentes diferentes\")\n",
        "\n",
        "# ===== PASO 2: TRANSFORMACI√ìN Y LIMPIEZA =====\n",
        "\n",
        "from pyspark.sql.functions import col, when, to_timestamp, current_timestamp\n",
        "\n",
        "# Limpiar ventas\n",
        "df_ventas_clean = df_ventas \\\n",
        "    .filter(col(\"monto\") > 0) \\\n",
        "    .filter(col(\"estado\") == \"completado\") \\\n",
        "    .withColumn(\"fecha\", to_timestamp(col(\"fecha\"))) \\\n",
        "    .dropDuplicates([\"id_transaccion\"])\n",
        "\n",
        "# Enriquecer clickstream con informaci√≥n de sesi√≥n\n",
        "df_clickstream_clean = df_clickstream \\\n",
        "    .withColumn(\"duracion_sesion\",\n",
        "                col(\"timestamp_fin\") - col(\"timestamp_inicio\")) \\\n",
        "    .filter(col(\"duracion_sesion\") > 0)\n",
        "\n",
        "# Normalizar inventario\n",
        "df_inventario_clean = df_inventario \\\n",
        "    .withColumn(\"stock\", col(\"stock\").cast(\"integer\")) \\\n",
        "    .withColumn(\"actualizado_en\", current_timestamp())\n",
        "\n",
        "print(\"‚úÖ Datos transformados y limpiados\")\n",
        "\n",
        "# ===== PASO 3: UNIFICAR DATOS =====\n",
        "\n",
        "# Combinar ventas con inventario\n",
        "df_ventas_inventario = df_ventas_clean.join(\n",
        "    df_inventario_clean,\n",
        "    df_ventas_clean.producto_id == df_inventario_clean.producto_id,\n",
        "    \"left\"\n",
        ")\n",
        "\n",
        "# A√±adir m√©tricas de comportamiento web\n",
        "df_completo = df_ventas_inventario.join(\n",
        "    df_clickstream_clean,\n",
        "    df_ventas_inventario.usuario_id == df_clickstream_clean.usuario_id,\n",
        "    \"left\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Datos unificados en una sola vista\")\n",
        "\n",
        "# ===== PASO 4: ESCRIBIR A DELTA LAKE =====\n",
        "\n",
        "# Guardar en formato Delta para consumo downstream\n",
        "df_completo.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"mergeSchema\", \"true\") \\\n",
        "    .partitionBy(\"fecha\") \\\n",
        "    .save(\"/delta/ventas_unificadas\")\n",
        "\n",
        "print(\"‚úÖ Datos persistidos en Delta Lake\")\n",
        "print(\"‚úÖ Listos para Analytics, ML y Dashboards\")\n",
        "```\n",
        "\n",
        "#### **Ventajas para Data Engineering en Databricks**\n",
        "\n",
        "```python\n",
        "ventajas_data_engineering = {\n",
        "    '1. Auto-scaling':\n",
        "        'Los clusters crecen/reducen autom√°ticamente seg√∫n carga',\n",
        "    \n",
        "    '2. Conectores nativos':\n",
        "        'Integraci√≥n con 100+ fuentes de datos sin configuraci√≥n compleja',\n",
        "    \n",
        "    '3. Delta Lake':\n",
        "        'ACID transactions garantizan calidad de datos',\n",
        "    \n",
        "    '4. Workflows':\n",
        "        'Orquestaci√≥n nativa sin necesidad de Airflow',\n",
        "    \n",
        "    '5. Monitoring':\n",
        "        'Observabilidad completa de pipelines en tiempo real',\n",
        "    \n",
        "    '6. Data Quality':\n",
        "        'Expectations para validar datos autom√°ticamente'\n",
        "}\n",
        "\n",
        "# Ejemplo: Data Quality Check\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def validar_calidad_ventas(df):\n",
        "    \"\"\"Valida que los datos cumplan reglas de negocio\"\"\"\n",
        "    \n",
        "    checks = {\n",
        "        'no_nulos': df.filter(col(\"monto\").isNull()).count() == 0,\n",
        "        'montos_positivos': df.filter(col(\"monto\") <= 0).count() == 0,\n",
        "        'fechas_validas': df.filter(col(\"fecha\") > current_timestamp()).count() == 0\n",
        "    }\n",
        "    \n",
        "    if all(checks.values()):\n",
        "        print(\"‚úÖ Todos los checks de calidad pasaron\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"‚ùå Fallos en calidad de datos:\")\n",
        "        for check, resultado in checks.items():\n",
        "            if not resultado:\n",
        "                print(f\"   - {check}\")\n",
        "        return False\n",
        "\n",
        "# Ejecutar validaci√≥n\n",
        "if validar_calidad_ventas(df_ventas_clean):\n",
        "    # Proceder con el pipeline\n",
        "    pass\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Caso de Uso 2: Data Science (Ciencia de Datos)**\n",
        "\n",
        "#### **¬øQu√© hace un Data Scientist?**\n",
        "\n",
        "Los Data Scientists son los **investigadores**. Exploran datos, descubren patrones, construyen modelos predictivos y extraen insights accionables.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "RESPONSABILIDADES DE DATA SCIENCE\n",
        "\"\"\"\n",
        "\n",
        "responsabilidades = {\n",
        "    'exploraci√≥n': 'An√°lisis exploratorio de datos (EDA)',\n",
        "    'hip√≥tesis': 'Formular y probar hip√≥tesis',\n",
        "    'feature_engineering': 'Crear variables predictivas',\n",
        "    'modelado': 'Construir modelos de ML/estad√≠sticos',\n",
        "    'experimentaci√≥n': 'A/B testing y validaci√≥n',\n",
        "    'comunicaci√≥n': 'Visualizar y presentar hallazgos'\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Ejemplo Pr√°ctico: Predicci√≥n de Churn (Abandono de Clientes)**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CASO REAL: Empresa de Telecomunicaciones\n",
        "\n",
        "Problema:\n",
        "- 20% de clientes cancelan su servicio cada a√±o\n",
        "- Cuesta 5x m√°s adquirir cliente nuevo que retener uno existente\n",
        "- Necesitan identificar clientes en riesgo para retenerlos\n",
        "\n",
        "Soluci√≥n con Databricks:\n",
        "\"\"\"\n",
        "\n",
        "# ===== PASO 1: CARGAR Y EXPLORAR DATOS =====\n",
        "\n",
        "# Leer datos hist√≥ricos de clientes\n",
        "df_clientes = spark.read.format(\"delta\").table(\"clientes_historico\")\n",
        "\n",
        "# Vista r√°pida de los datos\n",
        "display(df_clientes.limit(10))\n",
        "\n",
        "# Estad√≠sticas descriptivas\n",
        "df_clientes.describe().show()\n",
        "\n",
        "# Distribuci√≥n de churn\n",
        "df_clientes.groupBy(\"churn\").count().show()\n",
        "\n",
        "\"\"\"\n",
        "Resultado:\n",
        "+-----+------+\n",
        "|churn| count|\n",
        "+-----+------+\n",
        "|   No|160000|\n",
        "|  Yes| 40000|\n",
        "+-----+------+\n",
        "\n",
        "‚ö†Ô∏è Dataset desbalanceado: 80% No churn, 20% Churn\n",
        "\"\"\"\n",
        "\n",
        "# ===== PASO 2: FEATURE ENGINEERING =====\n",
        "\n",
        "from pyspark.sql.functions import col, datediff, current_date, when, avg\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "\n",
        "# Crear nuevas features\n",
        "df_features = df_clientes \\\n",
        "    .withColumn(\"antiguedad_dias\",\n",
        "                datediff(current_date(), col(\"fecha_alta\"))) \\\n",
        "    .withColumn(\"promedio_consumo_mes\",\n",
        "                col(\"total_consumo\") / col(\"meses_activo\")) \\\n",
        "    .withColumn(\"llamadas_servicio_cliente\",\n",
        "                col(\"tickets_soporte\").cast(\"integer\")) \\\n",
        "    .withColumn(\"tiene_plan_premium\",\n",
        "                when(col(\"plan_tipo\") == \"premium\", 1).otherwise(0))\n",
        "\n",
        "# Codificar variables categ√≥ricas\n",
        "indexer = StringIndexer(inputCol=\"region\", outputCol=\"region_idx\")\n",
        "df_encoded = indexer.fit(df_features).transform(df_features)\n",
        "\n",
        "# Seleccionar features para el modelo\n",
        "feature_cols = [\n",
        "    \"antiguedad_dias\",\n",
        "    \"promedio_consumo_mes\",\n",
        "    \"llamadas_servicio_cliente\",\n",
        "    \"tiene_plan_premium\",\n",
        "    \"region_idx\",\n",
        "    \"edad\",\n",
        "    \"num_servicios_contratados\"\n",
        "]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "df_model = assembler.transform(df_encoded)\n",
        "\n",
        "print(\"‚úÖ Features creadas y preparadas\")\n",
        "\n",
        "# ===== PASO 3: ENTRENAR MODELO =====\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Preparar label\n",
        "indexer_label = StringIndexer(inputCol=\"churn\", outputCol=\"label\")\n",
        "df_model = indexer_label.fit(df_model).transform(df_model)\n",
        "\n",
        "# Split train/test\n",
        "train_df, test_df = df_model.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Entrenar modelo\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    numTrees=100,\n",
        "    maxDepth=10\n",
        ")\n",
        "\n",
        "modelo = rf.fit(train_df)\n",
        "\n",
        "print(\"‚úÖ Modelo entrenado con Random Forest\")\n",
        "\n",
        "# ===== PASO 4: EVALUAR MODELO =====\n",
        "\n",
        "# Predicciones\n",
        "predictions = modelo.transform(test_df)\n",
        "\n",
        "# M√©tricas\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "print(f\"üìä AUC-ROC: {auc:.3f}\")\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "predictions.groupBy(\"label\", \"prediction\").count().show()\n",
        "\n",
        "\"\"\"\n",
        "Resultado:\n",
        "+-----+----------+-----+\n",
        "|label|prediction|count|\n",
        "+-----+----------+-----+\n",
        "|  0.0|       0.0|31500|\n",
        "|  0.0|       1.0|  500|\n",
        "|  1.0|       0.0| 1200|\n",
        "|  1.0|       1.0| 6800|\n",
        "+-----+----------+-----+\n",
        "\n",
        "‚úÖ 85% de clientes en riesgo identificados correctamente\n",
        "‚úÖ Falsos positivos: 500 (aceptable para negocio)\n",
        "\"\"\"\n",
        "\n",
        "# ===== PASO 5: FEATURE IMPORTANCE =====\n",
        "\n",
        "# ¬øQu√© variables son m√°s importantes?\n",
        "import pandas as pd\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': modelo.featureImportances.toArray()\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nüìä Variables m√°s importantes para predecir churn:\")\n",
        "print(feature_importance)\n",
        "\n",
        "\"\"\"\n",
        "Resultado:\n",
        "                    feature  importance\n",
        "1     promedio_consumo_mes      0.3200\n",
        "2  llamadas_servicio_cliente    0.2800\n",
        "0            antiguedad_dias      0.1500\n",
        "6 num_servicios_contratados      0.1200\n",
        "3         tiene_plan_premium      0.0900\n",
        "4                region_idx      0.0300\n",
        "5                      edad      0.0100\n",
        "\n",
        "üí° Insight: El consumo promedio y las llamadas a soporte\n",
        "   son los mejores predictores de churn\n",
        "\"\"\"\n",
        "\n",
        "# ===== PASO 6: GUARDAR MODELO =====\n",
        "\n",
        "# MLflow tracking autom√°tico en Databricks\n",
        "import mlflow\n",
        "\n",
        "with mlflow.start_run(run_name=\"churn_model_v1\"):\n",
        "    mlflow.log_param(\"num_trees\", 100)\n",
        "    mlflow.log_param(\"max_depth\", 10)\n",
        "    mlflow.log_metric(\"auc\", auc)\n",
        "    mlflow.spark.log_model(modelo, \"model\")\n",
        "    \n",
        "print(\"‚úÖ Modelo versionado y registrado en MLflow\")\n",
        "```\n",
        "\n",
        "#### **Ventajas para Data Science en Databricks**\n",
        "\n",
        "```python\n",
        "ventajas_data_science = {\n",
        "    '1. Escalabilidad':\n",
        "        'Entrena modelos con millones/billones de registros sin problemas',\n",
        "    \n",
        "    '2. MLflow integrado':\n",
        "        'Tracking, versionado y deployment de modelos autom√°tico',\n",
        "    \n",
        "    '3. Notebooks colaborativos':\n",
        "        'Varios data scientists trabajando en el mismo an√°lisis',\n",
        "    \n",
        "    '4. Feature Store':\n",
        "        'Reutilizar features entre proyectos y equipos',\n",
        "    \n",
        "    '5. AutoML':\n",
        "        'Genera modelos baseline autom√°ticamente',\n",
        "    \n",
        "    '6. GPU clusters':\n",
        "        'Entrenar deep learning models con GPUs'\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Caso de Uso 3: Analytics (Anal√≠tica de Negocio)**\n",
        "\n",
        "#### **¬øQu√© hace un Data Analyst?**\n",
        "\n",
        "Los Data Analysts son los **traductores de datos**. Convierten datos en insights accionables para decisiones de negocio.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "RESPONSABILIDADES DE DATA ANALYTICS\n",
        "\"\"\"\n",
        "\n",
        "responsabilidades = {\n",
        "    'reporting': 'Crear reportes regulares para stakeholders',\n",
        "    'dashboards': 'Visualizaciones interactivas',\n",
        "    'ad_hoc': 'Responder preguntas de negocio espec√≠ficas',\n",
        "    'kpis': 'Monitorear m√©tricas clave del negocio',\n",
        "    'sql': 'Consultas SQL para extraer insights',\n",
        "    'presentaciones': 'Comunicar hallazgos a audiencias no t√©cnicas'\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Ejemplo Pr√°ctico: Dashboard Ejecutivo de Ventas**\n",
        "\n",
        "```sql\n",
        "-- Celda SQL en Databricks\n",
        "\n",
        "-- ===== AN√ÅLISIS 1: TENDENCIA DE VENTAS =====\n",
        "\n",
        "SELECT\n",
        "    DATE_TRUNC('month', fecha) AS mes,\n",
        "    COUNT(DISTINCT id_transaccion) AS num_transacciones,\n",
        "    SUM(monto) AS ingresos_totales,\n",
        "    AVG(monto) AS ticket_promedio,\n",
        "    COUNT(DISTINCT cliente_id) AS clientes_unicos\n",
        "FROM ventas_unificadas\n",
        "WHERE fecha >= DATE_SUB(CURRENT_DATE(), 365)\n",
        "GROUP BY mes\n",
        "ORDER BY mes;\n",
        "\n",
        "/*\n",
        "Resultado:\n",
        "+----------+------------------+----------------+----------------+-----------------+\n",
        "|      mes |num_transacciones |ingresos_totales|ticket_promedio |clientes_unicos  |\n",
        "+----------+------------------+----------------+----------------+-----------------+\n",
        "|2025-02-01|           125,000|      $5,250,000|          $42.00|           45,000|\n",
        "|2025-03-01|           138,000|      $5,980,000|          $43.33|           48,500|\n",
        "|2025-04-01|           145,000|      $6,380,000|          $44.00|           51,200|\n",
        "+----------+------------------+----------------+----------------+-----------------+\n",
        "\n",
        "üìà Tendencia positiva: Crecimiento 16% en transacciones\n",
        "üìà Ticket promedio aument√≥ 5%\n",
        "*/\n",
        "```\n",
        "\n",
        "```sql\n",
        "-- ===== AN√ÅLISIS 2: TOP PRODUCTOS =====\n",
        "\n",
        "SELECT\n",
        "    producto_nombre,\n",
        "    categoria,\n",
        "    COUNT(*) AS unidades_vendidas,\n",
        "    SUM(monto) AS ingresos,\n",
        "    ROUND(SUM(monto) / SUM(SUM(monto)) OVER () * 100, 2) AS porcentaje_ingresos\n",
        "FROM ventas_unificadas\n",
        "WHERE fecha >= DATE_SUB(CURRENT_DATE(), 30)\n",
        "GROUP BY producto_nombre, categoria\n",
        "ORDER BY ingresos DESC\n",
        "LIMIT 10;\n",
        "\n",
        "/*\n",
        "Resultado:\n",
        "+------------------+-----------+------------------+-----------+--------------------+\n",
        "|producto_nombre   |categoria  |unidades_vendidas |ingresos   |porcentaje_ingresos |\n",
        "+------------------+-----------+------------------+-----------+--------------------+\n",
        "|iPhone 15 Pro     |Electr√≥nica|             2,500|$2,500,000 |              15.50%|\n",
        "|MacBook Air M3    |Electr√≥nica|             1,200|$1,440,000 |               8.93%|\n",
        "|AirPods Pro 2     |Accesorios |             8,500|  $850,000 |               5.27%|\n",
        "+------------------+-----------+------------------+-----------+--------------------+\n",
        "\n",
        "üí° Insight: Top 3 productos generan 30% de ingresos totales\n",
        "üí° Categor√≠a Electr√≥nica domina con 45% de ventas\n",
        "*/\n",
        "```\n",
        "\n",
        "```sql\n",
        "-- ===== AN√ÅLISIS 3: AN√ÅLISIS DE COHORTES =====\n",
        "\n",
        "WITH cohortes AS (\n",
        "    SELECT\n",
        "        cliente_id,\n",
        "        DATE_TRUNC('month', MIN(fecha)) AS mes_primera_compra\n",
        "    FROM ventas_unificadas\n",
        "    GROUP BY cliente_id\n",
        "),\n",
        "actividad AS (\n",
        "    SELECT\n",
        "        v.cliente_id,\n",
        "        c.mes_primera_compra,\n",
        "        DATE_TRUNC('month', v.fecha) AS mes_actividad,\n",
        "        DATEDIFF(MONTH, c.mes_primera_compra, v.fecha) AS meses_desde_primera_compra\n",
        "    FROM ventas_unificadas v\n",
        "    JOIN cohortes c ON v.cliente_id = c.cliente_id\n",
        ")\n",
        "SELECT\n",
        "    mes_primera_compra AS cohorte,\n",
        "    meses_desde_primera_compra,\n",
        "    COUNT(DISTINCT cliente_id) AS clientes_activos,\n",
        "    ROUND(COUNT(DISTINCT cliente_id) * 100.0 /\n",
        "          FIRST_VALUE(COUNT(DISTINCT cliente_id)) OVER (\n",
        "              PARTITION BY mes_primera_compra\n",
        "              ORDER BY meses_desde_primera_compra\n",
        "          ), 2) AS retencion_porcentaje\n",
        "FROM actividad\n",
        "WHERE meses_desde_primera_compra <= 12\n",
        "GROUP BY mes_primera_compra, meses_desde_primera_compra\n",
        "ORDER BY mes_primera_compra, meses_desde_primera_compra;\n",
        "\n",
        "/*\n",
        "Resultado (simplificado):\n",
        "+-----------------+-------------------------------+------------------+---------------------+\n",
        "|cohorte          |meses_desde_primera_compra     |clientes_activos  |retencion_porcentaje |\n",
        "+-----------------+-------------------------------+------------------+---------------------+\n",
        "|2025-01-01       |                             0 |           10,000 |              100.00%|\n",
        "|2025-01-01       |                             1 |            7,500 |               75.00%|\n",
        "|2025-01-01       |                             2 |            6,200 |               62.00%|\n",
        "|2025-01-01       |                             3 |            5,500 |               55.00%|\n",
        "+-----------------+-------------------------------+------------------+---------------------+\n",
        "\n",
        "‚ö†Ô∏è Problema identificado: 45% de clientes se pierden en 3 meses\n",
        "üí° Acci√≥n: Implementar programa de retenci√≥n temprana\n",
        "*/\n",
        "```\n",
        "\n",
        "#### **Crear Dashboard Interactivo**\n",
        "\n",
        "```python\n",
        "# Celda Python para visualizaci√≥n\n",
        "\n",
        "from pyspark.sql.functions import col, sum, avg, count, month, year\n",
        "\n",
        "# Cargar datos\n",
        "df_ventas = spark.read.format(\"delta\").table(\"ventas_unificadas\")\n",
        "\n",
        "# KPIs principales\n",
        "kpis = df_ventas.agg(\n",
        "    count(\"id_transaccion\").alias(\"total_transacciones\"),\n",
        "    sum(\"monto\").alias(\"ingresos_totales\"),\n",
        "    avg(\"monto\").alias(\"ticket_promedio\"),\n",
        "    count(col(\"cliente_id\").distinct()).alias(\"clientes_unicos\")\n",
        ").collect()[0]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üìä DASHBOARD EJECUTIVO - RESUMEN DEL MES\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üí∞ Ingresos Totales:      ${kpis['ingresos_totales']:,.2f}\")\n",
        "print(f\"üõí Transacciones:         {kpis['total_transacciones']:,}\")\n",
        "print(f\"üé´ Ticket Promedio:       ${kpis['ticket_promedio']:.2f}\")\n",
        "print(f\"üë• Clientes √önicos:       {kpis['clientes_unicos']:,}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Databricks tiene visualizaciones nativas\n",
        "# display() crea gr√°ficos autom√°ticamente\n",
        "display(df_ventas.groupBy(month(\"fecha\").alias(\"mes\"))\n",
        "        .agg(sum(\"monto\").alias(\"ingresos\"))\n",
        "        .orderBy(\"mes\"))\n",
        "```\n",
        "\n",
        "#### **Ventajas para Analytics en Databricks**\n",
        "\n",
        "```python\n",
        "ventajas_analytics = {\n",
        "    '1. SQL Warehouses':\n",
        "        'Consultas SQL optimizadas con cach√© inteligente',\n",
        "    \n",
        "    '2. Dashboards nativos':\n",
        "        'Crear visualizaciones sin herramientas externas',\n",
        "    \n",
        "    '3. Consultas sobre Delta':\n",
        "        'Acceso a datos frescos y consistentes (no ETL adicional)',\n",
        "    \n",
        "    '4. Par√°metros y widgets':\n",
        "        'Dashboards interactivos con filtros din√°micos',\n",
        "    \n",
        "    '5. Programaci√≥n autom√°tica':\n",
        "        'Reportes que se actualizan solos',\n",
        "    \n",
        "    '6. Compartir f√°cilmente':\n",
        "        'URLs p√∫blicas o embeds en otras apps'\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Caso de Uso 4: Machine Learning Engineering**\n",
        "\n",
        "#### **¬øQu√© hace un ML Engineer?**\n",
        "\n",
        "Los ML Engineers son los **operadores de modelos**. Llevan modelos desde notebooks experimentales a producci√≥n a escala.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "RESPONSABILIDADES DE ML ENGINEERING\n",
        "\"\"\"\n",
        "\n",
        "responsabilidades = {\n",
        "    'productionizaci√≥n': 'Desplegar modelos en producci√≥n',\n",
        "    'mlops': 'CI/CD para modelos de ML',\n",
        "    'monitoring': 'Detectar data drift y model decay',\n",
        "    'retraining': 'Automatizar reentrenamiento de modelos',\n",
        "    'serving': 'APIs de inferencia escalables',\n",
        "    'governance': 'Versionado y auditor√≠a de modelos'\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Ejemplo Pr√°ctico: Despliegue de Modelo de Recomendaciones**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CASO REAL: Sistema de Recomendaciones en Producci√≥n\n",
        "\n",
        "Problema:\n",
        "- Data Scientists crearon modelo de recomendaci√≥n de productos\n",
        "- Necesita servir predicciones a 10,000 requests/segundo\n",
        "- Debe actualizarse diariamente con nuevos datos\n",
        "- Requiere monitoreo de performance en tiempo real\n",
        "\n",
        "Soluci√≥n con Databricks ML:\n",
        "\"\"\"\n",
        "\n",
        "# ===== PASO 1: REGISTRAR MODELO EN MLFLOW =====\n",
        "\n",
        "import mlflow\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "# El modelo ya fue entrenado por Data Science\n",
        "# Ahora lo registramos en Model Registry\n",
        "\n",
        "model_name = \"recomendador_productos\"\n",
        "model_uri = \"runs:/<run_id>/model\"  # Del experimento anterior\n",
        "\n",
        "# Registrar modelo\n",
        "mlflow.register_model(model_uri, model_name)\n",
        "\n",
        "# Promover a producci√≥n\n",
        "client = MlflowClient()\n",
        "client.transition_model_version_stage(\n",
        "    name=model_name,\n",
        "    version=1,\n",
        "    stage=\"Production\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Modelo '{model_name}' v1 en Producci√≥n\")\n",
        "\n",
        "# ===== PASO 2: CREAR ENDPOINT DE INFERENCIA =====\n",
        "\n",
        "\"\"\"\n",
        "En Databricks UI:\n",
        "1. Ir a \"Machine Learning\" ‚Üí \"Serving\"\n",
        "2. Crear endpoint con el modelo registrado\n",
        "3. Configurar auto-scaling\n",
        "\n",
        "Resultado: API REST lista para uso\n",
        "\"\"\"\n",
        "\n",
        "# Ejemplo de uso del endpoint\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# URL del endpoint (generada autom√°ticamente)\n",
        "endpoint_url = \"https://workspace.cloud.databricks.com/serving-endpoints/recomendador/invocations\"\n",
        "\n",
        "# Token de autenticaci√≥n\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {databricks_token}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Request de inferencia\n",
        "payload = {\n",
        "    \"dataframe_records\": [\n",
        "        {\n",
        "            \"usuario_id\": 12345,\n",
        "            \"categoria_preferida\": \"electronica\",\n",
        "            \"historial_compras\": [101, 205, 389]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Llamar al endpoint\n",
        "response = requests.post(endpoint_url, headers=headers, json=payload)\n",
        "recomendaciones = response.json()\n",
        "\n",
        "print(\"üéØ Recomendaciones para usuario 12345:\")\n",
        "print(recomendaciones)\n",
        "\n",
        "\"\"\"\n",
        "Resultado:\n",
        "{\n",
        "  \"predictions\": [\n",
        "    {\"producto_id\": 450, \"score\": 0.92, \"nombre\": \"iPhone 15\"},\n",
        "    {\"producto_id\": 328, \"score\": 0.87, \"nombre\": \"AirPods Pro\"},\n",
        "    {\"producto_id\": 562, \"score\": 0.81, \"nombre\": \"Apple Watch\"}\n",
        "  ]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# ===== PASO 3: BATCH INFERENCE (ALTERNATIVA) =====\n",
        "\n",
        "# Para casos donde no necesitas real-time\n",
        "\n",
        "# Cargar modelo de producci√≥n\n",
        "modelo_prod = mlflow.pyfunc.load_model(f\"models:/{model_name}/Production\")\n",
        "\n",
        "# Leer usuarios que necesitan recomendaciones\n",
        "df_usuarios = spark.read.format(\"delta\").table(\"usuarios_activos\")\n",
        "\n",
        "# Generar recomendaciones en batch\n",
        "df_recomendaciones = modelo_prod.predict(df_usuarios)\n",
        "\n",
        "# Guardar resultados\n",
        "df_recomendaciones.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"/delta/recomendaciones_diarias\")\n",
        "\n",
        "print(\"‚úÖ Recomendaciones generadas para 500K usuarios en 5 minutos\")\n",
        "\n",
        "# ===== PASO 4: MONITOREO DEL MODELO =====\n",
        "\n",
        "from databricks.feature_engineering import FeatureEngineeringClient\n",
        "\n",
        "# Feature Store tracking\n",
        "fe = FeatureEngineeringClient()\n",
        "\n",
        "# Log inference data para monitoring\n",
        "inference_log = spark.read.format(\"delta\").table(\"inference_logs\")\n",
        "\n",
        "# Detectar data drift\n",
        "from scipy import stats\n",
        "\n",
        "# Comparar distribuci√≥n de features en training vs producci√≥n\n",
        "feature_col = \"edad_usuario\"\n",
        "\n",
        "train_dist = spark.read.format(\"delta\").table(\"training_data\") \\\n",
        "    .select(feature_col).toPandas()\n",
        "\n",
        "prod_dist = inference_log.select(feature_col) \\\n",
        "    .limit(10000).toPandas()\n",
        "\n",
        "# Test de Kolmogorov-Smirnov\n",
        "ks_statistic, p_value = stats.ks_2samp(\n",
        "    train_dist[feature_col],\n",
        "    prod_dist[feature_col]\n",
        ")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(f\"‚ö†Ô∏è DATA DRIFT detectado en {feature_col}\")\n",
        "    print(f\"   KS statistic: {ks_statistic:.3f}, p-value: {p_value:.4f}\")\n",
        "    print(\"   ‚Üí Considerar reentrenar modelo\")\n",
        "else:\n",
        "    print(f\"‚úÖ Distribuci√≥n de {feature_col} estable\")\n",
        "\n",
        "# ===== PASO 5: REENTRENAMIENTO AUTOMATIZADO =====\n",
        "\n",
        "# Job programado que corre diariamente\n",
        "def retrain_model():\n",
        "    \"\"\"\n",
        "    Este c√≥digo corre autom√°ticamente cada noche\n",
        "    \"\"\"\n",
        "    from datetime import datetime\n",
        "    \n",
        "    # Cargar datos frescos\n",
        "    df_train_new = spark.read.format(\"delta\") \\\n",
        "        .table(\"ventas_unificadas\") \\\n",
        "        .filter(col(\"fecha\") >= datetime.now() - timedelta(days=90))\n",
        "    \n",
        "    # Reentrenar modelo\n",
        "    nuevo_modelo = entrenar_modelo(df_train_new)\n",
        "    \n",
        "    # Evaluar contra modelo actual\n",
        "    metricas_actual = evaluar_modelo(modelo_prod, test_set)\n",
        "    metricas_nuevo = evaluar_modelo(nuevo_modelo, test_set)\n",
        "    \n",
        "    # Solo promover si mejora\n",
        "    if metricas_nuevo['auc'] > metricas_actual['auc']:\n",
        "        # Registrar nuevo modelo\n",
        "        mlflow.spark.log_model(nuevo_modelo, \"model\")\n",
        "        \n",
        "        # Promover a staging primero\n",
        "        client.transition_model_version_stage(\n",
        "            name=model_name,\n",
        "            version=2,\n",
        "            stage=\"Staging\"\n",
        "        )\n",
        "        \n",
        "        print(\"‚úÖ Nuevo modelo en Staging para testing\")\n",
        "        # Despu√©s de validaci√≥n manual ‚Üí Producci√≥n\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è Modelo actual sigue siendo mejor, no se actualiza\")\n",
        "\n",
        "# Programar con Databricks Jobs (en UI):\n",
        "# Frequency: Daily at 2 AM\n",
        "# Cluster: Job cluster (costo-eficiente)\n",
        "# Alerts: Email si falla\n",
        "```\n",
        "\n",
        "#### **Ventajas para ML Engineering en Databricks**\n",
        "\n",
        "```python\n",
        "ventajas_ml_engineering = {\n",
        "    '1. MLflow integrado':\n",
        "        'Versionado, registry, serving todo en uno',\n",
        "    \n",
        "    '2. Model Serving':\n",
        "        'Endpoints auto-escalables sin configurar infraestructura',\n",
        "    \n",
        "    '3. Feature Store':\n",
        "        'Features reutilizables y consistentes train/serve',\n",
        "    \n",
        "    '4. Monitoring':\n",
        "        'Detecci√≥n autom√°tica de drift y degradaci√≥n',\n",
        "    \n",
        "    '5. CI/CD':\n",
        "        'Integraci√≥n con Git, testing automatizado',\n",
        "    \n",
        "    '6. Governance':\n",
        "        'Auditor√≠a completa de modelos en producci√≥n'\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparaci√≥n de Roles: Flujo de Trabajo Completo**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "PROYECTO REAL: Sistema de Detecci√≥n de Fraude\n",
        "\n",
        "Veamos c√≥mo los 4 roles colaboran en Databricks:\n",
        "\"\"\"\n",
        "\n",
        "# ====== ROL 1: DATA ENGINEER ======\n",
        "print(\"üë∑ DATA ENGINEER:\")\n",
        "print(\"- Construye pipeline de ingesta desde sistemas transaccionales\")\n",
        "print(\"- Limpia y normaliza datos de transacciones\")\n",
        "print(\"- Crea tabla Delta 'transacciones_limpias'\")\n",
        "print(\"- Programa Job para actualizar cada 15 minutos\")\n",
        "print()\n",
        "\n",
        "# C√≥digo del Data Engineer\n",
        "df_transacciones = spark.read.jdbc(...)  # Leer desde DB\n",
        "df_clean = limpiar_datos(df_transacciones)\n",
        "df_clean.write.format(\"delta\").mode(\"append\").save(\"/delta/transacciones\")\n",
        "\n",
        "# ====== ROL 2: DATA SCIENTIST ======\n",
        "print(\"üî¨ DATA SCIENTIST:\")\n",
        "print(\"- Lee tabla 'transacciones_limpias'\")\n",
        "print(\"- An√°lisis exploratorio: patrones de fraude\")\n",
        "print(\"- Crea features: monto_promedio, frecuencia, ubicaci√≥n inusual\")\n",
        "print(\"- Entrena modelo XGBoost\")\n",
        "print(\"- Registra modelo en MLflow\")\n",
        "print()\n",
        "\n",
        "# C√≥digo del Data Scientist\n",
        "df_data = spark.read.format(\"delta\").table(\"transacciones_limpias\")\n",
        "df_features = feature_engineering(df_data)\n",
        "modelo = entrenar_xgboost(df_features)\n",
        "mlflow.sklearn.log_model(modelo, \"fraud_detector\")\n",
        "\n",
        "# ====== ROL 3: ML ENGINEER ======\n",
        "print(\"‚öôÔ∏è ML ENGINEER:\")\n",
        "print(\"- Toma modelo del Data Scientist\")\n",
        "print(\"- Crea endpoint de serving\")\n",
        "print(\"- Implementa monitoreo de performance\")\n",
        "print(\"- Configura reentrenamiento semanal\")\n",
        "print(\"- Establece alertas de degradaci√≥n\")\n",
        "print()\n",
        "\n",
        "# C√≥digo del ML Engineer\n",
        "mlflow.register_model(\"runs:/.../model\", \"fraud_detector\")\n",
        "# Crear endpoint via UI\n",
        "# Configurar monitoring y retraining job\n",
        "\n",
        "# ====== ROL 4: DATA ANALYST ======\n",
        "print(\"üìä DATA ANALYST:\")\n",
        "print(\"- Crea dashboard de fraude detectado\")\n",
        "print(\"- Reportes diarios para equipo de riesgos\")\n",
        "print(\"- KPIs: tasa de fraude, falsos positivos, dinero ahorrado\")\n",
        "print(\"- An√°lisis de tendencias de fraude por regi√≥n/tiempo\")\n",
        "print()\n",
        "\n",
        "# C√≥digo del Data Analyst (SQL)\n",
        "\"\"\"\n",
        "SELECT\n",
        "    fecha,\n",
        "    COUNT(*) AS transacciones_totales,\n",
        "    SUM(CASE WHEN fraude_detectado = 1 THEN 1 ELSE 0 END) AS fraudes,\n",
        "    SUM(CASE WHEN fraude_detectado = 1 THEN monto ELSE 0 END) AS dinero_protegido\n",
        "FROM predicciones_fraude\n",
        "GROUP BY fecha\n",
        "ORDER BY fecha DESC;\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÖ RESULTADO FINAL:\")\n",
        "print(\"   Sistema de detecci√≥n de fraude en producci√≥n\")\n",
        "print(\"   Protegiendo $500K diarios\")\n",
        "print(\"   Todos los roles colaborando en UN SOLO WORKSPACE\")\n",
        "print(\"=\"*60)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Simulando Roles**\n",
        "\n",
        "```python\n",
        "# Ejercicio conceptual: Identifica el rol apropiado\n",
        "\n",
        "casos = [\n",
        "    {\n",
        "        'tarea': 'Construir pipeline que ingesta datos de 5 APIs diferentes cada hora',\n",
        "        'rol': '___________',\n",
        "        'herramientas': '___________'\n",
        "    },\n",
        "    {\n",
        "        'tarea': 'Analizar por qu√© las ventas cayeron 15% en el √∫ltimo trimestre',\n",
        "        'rol': '___________',\n",
        "        'herramientas': '___________'\n",
        "    },\n",
        "    {\n",
        "        'tarea': 'Crear modelo que prediga demanda de productos para los pr√≥ximos 30 d√≠as',\n",
        "        'rol': '___________',\n",
        "        'herramientas': '___________'\n",
        "    },\n",
        "    {\n",
        "        'tarea': 'Desplegar modelo de recomendaciones que sirva 50K requests/min',\n",
        "        'rol': '___________',\n",
        "        'herramientas': '___________'\n",
        "    },\n",
        "    {\n",
        "        'tarea': 'Detectar que el modelo en producci√≥n est√° degrad√°ndose y necesita reentrenamiento',\n",
        "        'rol': '___________',\n",
        "        'herramientas': '___________'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Respuestas:\n",
        "# 1. Data Engineer | Workflows, Delta Lake, Autoloader\n",
        "# 2. Data Analyst | SQL, Dashboards, Visualizaciones\n",
        "# 3. Data Scientist | PySpark ML, MLflow, Feature Store\n",
        "# 4. ML Engineer | Model Serving, Endpoints, Auto-scaling\n",
        "# 5. ML Engineer | Monitoring, MLflow, Retraining Jobs\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 1.3 - Caso Pr√°ctico**\n",
        "\n",
        "```markdown\n",
        "### Proyecto: Sistema de An√°lisis de Sentimiento para Reviews\n",
        "\n",
        "Una empresa de e-commerce recibe 100K reviews diarias de productos.\n",
        "Necesitan automatizar el an√°lisis de sentimiento.\n",
        "\n",
        "**Tu tarea**: Describe c√≥mo cada rol contribuir√≠a al proyecto\n",
        "\n",
        "**Data Engineer**:\n",
        "- Tareas: ___________\n",
        "- Entregables: ___________\n",
        "\n",
        "**Data Scientist**:\n",
        "- Tareas: ___________\n",
        "- Entregables: ___________\n",
        "\n",
        "**ML Engineer**:\n",
        "- Tareas: ___________\n",
        "- Entregables: ___________\n",
        "\n",
        "**Data Analyst**:\n",
        "- Tareas: ___________\n",
        "- Entregables: ___________\n",
        "\n",
        "**Bonus**: ¬øEn qu√© orden trabajar√≠an? ¬øC√≥mo colaborar√≠an?\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **Databricks unifica** 4 roles en una plataforma\n",
        "2. **Data Engineers** construyen la infraestructura de datos\n",
        "3. **Data Scientists** crean modelos y extraen insights\n",
        "4. **ML Engineers** operan modelos en producci√≥n\n",
        "5. **Data Analysts** traducen datos en decisiones de negocio\n",
        "6. **Colaboraci√≥n fluida** sin handoffs entre herramientas\n",
        "7. **Todos usan Delta Lake** como fuente √∫nica de verdad\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "- En equipos peque√±os, una persona puede hacer varios roles\n",
        "- En Databricks, cambiar entre roles es natural (mismo workspace)\n",
        "- La l√≠nea entre roles se difumina - colaboraci√≥n > silos\n",
        "- Aprende los fundamentos de todos los roles para mejor colaboraci√≥n\n",
        "- Las empresas buscan perfiles \"T-shaped\": profundo en uno, amplio en varios\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el siguiente punto **1.4 Ecosistema: Relaci√≥n con Apache Spark y la nube**?"
      ],
      "metadata": {
        "id": "Cn3XPS3dTluD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.4 Ecosistema: Relaci√≥n con Apache Spark y la nube**\n",
        "\n",
        "#### **Introducci√≥n: Databricks no existe en el vac√≠o**\n",
        "\n",
        "Databricks es parte de un **ecosistema m√°s amplio** de tecnolog√≠as de Big Data y Cloud Computing. Entender c√≥mo se relaciona con Apache Spark y los proveedores cloud es fundamental para aprovechar todo su potencial.\n",
        "\n",
        "```markdown\n",
        "üåç EL ECOSISTEMA DE DATABRICKS\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ            CLOUD PROVIDERS                  ‚îÇ\n",
        "‚îÇ   [AWS]    [Azure]    [Google Cloud]        ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                   ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ         DATABRICKS PLATFORM                 ‚îÇ\n",
        "‚îÇ  (Capa de gesti√≥n y optimizaci√≥n)           ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                   ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ          APACHE SPARK                       ‚îÇ\n",
        "‚îÇ  (Motor de procesamiento distribuido)       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                   ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ      ALMACENAMIENTO (S3, ADLS, GCS)         ‚îÇ\n",
        "‚îÇ         + DELTA LAKE                        ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 1: Apache Spark - El Motor Detr√°s de Databricks**\n",
        "\n",
        "#### **¬øQu√© es Apache Spark?**\n",
        "\n",
        "**Apache Spark** es un motor de procesamiento de datos distribuido open-source creado en 2009 en UC Berkeley (por los mismos fundadores de Databricks).\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "APACHE SPARK EN POCAS PALABRAS\n",
        "\"\"\"\n",
        "\n",
        "caracteristicas_spark = {\n",
        "    'velocidad': 'Hasta 100x m√°s r√°pido que Hadoop MapReduce',\n",
        "    'procesamiento_en_memoria': 'Datos en RAM para operaciones r√°pidas',\n",
        "    'distribuido': 'Procesamiento paralelo en m√∫ltiples m√°quinas',\n",
        "    'unificado': 'Batch, Streaming, SQL, ML, Graphs en un solo framework',\n",
        "    'multi_lenguaje': 'APIs en Python, Scala, Java, R, SQL',\n",
        "    'open_source': 'Gratis y con comunidad activa'\n",
        "}\n",
        "\n",
        "# Spark es el \"motor\" que hace posible procesar\n",
        "# billones de registros en segundos\n",
        "```\n",
        "\n",
        "#### **¬øPor qu√© Spark es revolucionario?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ANTES DE SPARK: Hadoop MapReduce (2006-2014)\n",
        "\"\"\"\n",
        "\n",
        "# Problema: Lent√≠simo\n",
        "hadoop_ejemplo = \"\"\"\n",
        "1. Leer datos del disco ‚Üí Procesar ‚Üí Escribir al disco\n",
        "2. Leer del disco ‚Üí Procesar ‚Üí Escribir al disco\n",
        "3. Leer del disco ‚Üí Procesar ‚Üí Escribir al disco\n",
        "...\n",
        "‚è±Ô∏è Cada paso tarda minutos/horas\n",
        "‚ùå Total: Horas o d√≠as para an√°lisis complejo\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "CON SPARK: Procesamiento en Memoria (2014-presente)\n",
        "\"\"\"\n",
        "\n",
        "spark_ejemplo = \"\"\"\n",
        "1. Cargar datos en memoria (RAM)\n",
        "2. Procesar todo en memoria\n",
        "3. Solo escribir resultado final al disco\n",
        "\n",
        "‚è±Ô∏è Total: Minutos o segundos\n",
        "‚úÖ 10-100x m√°s r√°pido que Hadoop\n",
        "\"\"\"\n",
        "\n",
        "# Ejemplo visual de la diferencia\n",
        "print(\"üêå HADOOP MapReduce:\")\n",
        "print(\"Disco ‚Üí CPU ‚Üí Disco ‚Üí CPU ‚Üí Disco ‚Üí CPU ‚Üí Disco\")\n",
        "print(\"‚è±Ô∏è  Tiempo: 4 horas\")\n",
        "print()\n",
        "print(\"‚ö° APACHE SPARK:\")\n",
        "print(\"Disco ‚Üí RAM ‚Üí CPU ‚Üí CPU ‚Üí CPU ‚Üí Disco\")\n",
        "print(\"‚è±Ô∏è  Tiempo: 15 minutos\")\n",
        "```\n",
        "\n",
        "#### **Componentes de Apache Spark**\n",
        "\n",
        "```markdown\n",
        "üì¶ STACK DE APACHE SPARK\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ         APLICACIONES Y APIS                 ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  [Python]  [Scala]  [Java]  [R]  [SQL]      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                   ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ        BIBLIOTECAS ESPECIALIZADAS           ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Spark SQL ‚îÇ MLlib ‚îÇ Spark   ‚îÇ GraphX        ‚îÇ\n",
        "‚îÇ (SQL y    ‚îÇ (ML)  ‚îÇ Streaming‚îÇ (Grafos)     ‚îÇ\n",
        "‚îÇ  DataFrames)‚îÇ      ‚îÇ (Tiempo  ‚îÇ              ‚îÇ\n",
        "‚îÇ            ‚îÇ       ‚îÇ  real)   ‚îÇ              ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                   ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ           SPARK CORE ENGINE                 ‚îÇ\n",
        "‚îÇ  (Gesti√≥n de memoria, scheduling, fault     ‚îÇ\n",
        "‚îÇ   tolerance, distribuci√≥n de tareas)        ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                   ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ      CLUSTER MANAGER                        ‚îÇ\n",
        "‚îÇ  [Standalone] [YARN] [Mesos] [Kubernetes]   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "#### **Spark Core Concepts**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CONCEPTOS FUNDAMENTALES DE SPARK\n",
        "\"\"\"\n",
        "\n",
        "# 1. RDD (Resilient Distributed Dataset)\n",
        "rdd_concepto = \"\"\"\n",
        "RDD = Colecci√≥n inmutable y distribuida de objetos\n",
        "\n",
        "Caracter√≠sticas:\n",
        "- Distribuido: Particionado en m√∫ltiples nodos\n",
        "- Inmutable: No se puede modificar (solo crear nuevos RDDs)\n",
        "- Resiliente: Si un nodo falla, se recupera autom√°ticamente\n",
        "- Lazy: Las transformaciones no se ejecutan hasta una acci√≥n\n",
        "\"\"\"\n",
        "\n",
        "# Ejemplo conceptual (no ejecutar a√∫n)\n",
        "# rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
        "# rdd_doubled = rdd.map(lambda x: x * 2)  # Transformaci√≥n (lazy)\n",
        "# result = rdd_doubled.collect()  # Acci√≥n (ejecuta todo)\n",
        "\n",
        "# 2. DataFrame\n",
        "dataframe_concepto = \"\"\"\n",
        "DataFrame = Tabla distribuida con esquema (filas + columnas)\n",
        "\n",
        "Ventajas sobre RDD:\n",
        "- Optimizaci√≥n autom√°tica (Catalyst Optimizer)\n",
        "- API m√°s intuitiva (similar a Pandas/SQL)\n",
        "- Mejor rendimiento\n",
        "- Soporte para m√∫ltiples lenguajes\n",
        "\n",
        "‚Üí En Databricks, SIEMPRE usamos DataFrames, no RDDs\n",
        "\"\"\"\n",
        "\n",
        "# 3. Transformations vs Actions\n",
        "transformations_vs_actions = {\n",
        "    'Transformations': {\n",
        "        'definici√≥n': 'Operaciones que crean nuevo DataFrame (lazy)',\n",
        "        'ejemplos': ['select', 'filter', 'groupBy', 'join', 'withColumn'],\n",
        "        'ejecuci√≥n': 'NO se ejecutan inmediatamente',\n",
        "        'retorno': 'Nuevo DataFrame'\n",
        "    },\n",
        "    'Actions': {\n",
        "        'definici√≥n': 'Operaciones que retornan resultado (trigger)',\n",
        "        'ejemplos': ['show', 'count', 'collect', 'write', 'take'],\n",
        "        'ejecuci√≥n': 'Ejecutan todas las transformaciones pendientes',\n",
        "        'retorno': 'Datos al driver o almacenamiento'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Ejemplo pr√°ctico del concepto\n",
        "print(\"üîÑ TRANSFORMATIONS (Lazy):\")\n",
        "print(\"df.filter(...) ‚Üí No ejecuta nada a√∫n\")\n",
        "print(\"df.select(...) ‚Üí No ejecuta nada a√∫n\")\n",
        "print(\"df.groupBy(...) ‚Üí No ejecuta nada a√∫n\")\n",
        "print()\n",
        "print(\"‚ö° ACTION (Trigger):\")\n",
        "print(\"df.show() ‚Üí ¬°AHORA ejecuta todo!\")\n",
        "```\n",
        "\n",
        "#### **Ejemplo Pr√°ctico: Spark en Acci√≥n**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CASO: Procesar 1 Bill√≥n de registros de logs web\n",
        "\"\"\"\n",
        "\n",
        "# Imaginemos que tenemos archivos de logs distribuidos\n",
        "# Tama√±o total: 500 GB\n",
        "# Objetivo: Encontrar las 10 URLs m√°s visitadas\n",
        "\n",
        "# ===== SIN SPARK (Pandas tradicional) =====\n",
        "pandas_approach = \"\"\"\n",
        "‚ùå IMPOSIBLE CON PANDAS:\n",
        "- 500 GB no caben en memoria de una sola m√°quina\n",
        "- Incluso con 128 GB RAM, pandas colapsar√≠a\n",
        "- Proceso tomar√≠a d√≠as (si no falla)\n",
        "\"\"\"\n",
        "\n",
        "# ===== CON SPARK (Databricks) =====\n",
        "\n",
        "# PASO 1: Leer datos (lazy)\n",
        "df_logs = spark.read.json(\"/data/logs/*.json\")  # 500 GB, 1B registros\n",
        "\n",
        "print(\"‚úÖ Spark ley√≥ metadata, no carg√≥ los 500 GB a√∫n\")\n",
        "\n",
        "# PASO 2: Transformaciones (lazy)\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_urls = df_logs.select(\"url\", \"timestamp\")  # Solo columnas necesarias\n",
        "df_filtered = df_urls.filter(col(\"timestamp\") > \"2025-01-01\")  # Filtrar\n",
        "df_grouped = df_filtered.groupBy(\"url\").count()  # Agrupar\n",
        "df_sorted = df_grouped.orderBy(col(\"count\").desc())  # Ordenar\n",
        "df_top10 = df_sorted.limit(10)  # Top 10\n",
        "\n",
        "print(\"‚úÖ Transformaciones definidas, a√∫n NO ejecutadas\")\n",
        "\n",
        "# PASO 3: Acci√≥n (trigger - ejecuta todo)\n",
        "top_urls = df_top10.collect()  # AQU√ç se ejecuta todo\n",
        "\n",
        "print(\"\\nüéØ Top 10 URLs m√°s visitadas:\")\n",
        "for row in top_urls:\n",
        "    print(f\"   {row.url}: {row.count:,} visitas\")\n",
        "\n",
        "\"\"\"\n",
        "RESULTADO:\n",
        "‚úÖ Procesamiento completado en 5 minutos\n",
        "‚úÖ Spark distribuy√≥ el trabajo en 50 workers\n",
        "‚úÖ Cada worker proces√≥ ~10 GB en paralelo\n",
        "‚úÖ Total: 500 GB procesados eficientemente\n",
        "\n",
        "SIN SPARK:\n",
        "‚ùå Imposible o tomar√≠a d√≠as\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "#### **Lazy Evaluation - El Secreto de Spark**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "¬øPOR QU√â LAZY EVALUATION?\n",
        "\"\"\"\n",
        "\n",
        "# Sin lazy evaluation (ejecuci√≥n inmediata)\n",
        "sin_lazy = \"\"\"\n",
        "df = read_data()          ‚Üí Ejecuta: Lee 500 GB\n",
        "df = filter(df)           ‚Üí Ejecuta: Escanea 500 GB\n",
        "df = select(df)           ‚Üí Ejecuta: Escanea 500 GB otra vez\n",
        "df = group_by(df)         ‚Üí Ejecuta: Escanea 500 GB otra vez\n",
        "...\n",
        "‚ùå M√∫ltiples pasadas sobre los datos\n",
        "‚ùå Extremadamente ineficiente\n",
        "\"\"\"\n",
        "\n",
        "# Con lazy evaluation (Spark)\n",
        "con_lazy = \"\"\"\n",
        "df = read_data()          ‚Üí Solo registra: 'Voy a leer'\n",
        "df = filter(df)           ‚Üí Solo registra: 'Luego filtrar'\n",
        "df = select(df)           ‚Üí Solo registra: 'Luego seleccionar'\n",
        "df = group_by(df)         ‚Üí Solo registra: 'Luego agrupar'\n",
        "df.show()                 ‚Üí AQU√ç optimiza y ejecuta TODO en 1 pasada\n",
        "‚úÖ Una sola pasada sobre los datos\n",
        "‚úÖ Optimizaci√≥n autom√°tica (Catalyst)\n",
        "‚úÖ Extremadamente eficiente\n",
        "\"\"\"\n",
        "\n",
        "# Visualizaci√≥n del plan de ejecuci√≥n\n",
        "ejemplo_plan = \"\"\"\n",
        "Spark analiza TODAS las transformaciones antes de ejecutar\n",
        "y crea un \"plan de ejecuci√≥n optimizado\":\n",
        "\n",
        "Plan Original (naive):\n",
        "1. Leer 500 GB\n",
        "2. Filtrar ‚Üí 400 GB\n",
        "3. Seleccionar 2 columnas ‚Üí 100 GB\n",
        "4. Agrupar\n",
        "\n",
        "Plan Optimizado por Spark:\n",
        "1. Leer SOLO las 2 columnas necesarias ‚Üí 50 GB\n",
        "2. Mientras lee, aplicar filtro ‚Üí 40 GB\n",
        "3. Agrupar directamente\n",
        "\n",
        "Ahorro: 500 GB ‚Üí 40 GB procesados\n",
        "Velocidad: 10x m√°s r√°pido\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_plan)\n",
        "```\n",
        "\n",
        "#### **Databricks Optimiza Spark**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "SPARK VANILLA vs DATABRICKS RUNTIME\n",
        "\"\"\"\n",
        "\n",
        "comparacion = {\n",
        "    'Apache Spark Open Source': {\n",
        "        'instalaci√≥n': 'Manual, compleja',\n",
        "        'configuraci√≥n': 'Requiere expertise',\n",
        "        'optimizaciones': 'Manuales',\n",
        "        'monitoreo': 'Limitado',\n",
        "        'colaboraci√≥n': 'No incluida',\n",
        "        'costo': 'Gratis + infra + tiempo'\n",
        "    },\n",
        "    'Databricks Runtime': {\n",
        "        'instalaci√≥n': 'Clic en bot√≥n',\n",
        "        'configuraci√≥n': 'Autom√°tica y optimizada',\n",
        "        'optimizaciones': 'Photon Engine (3-8x m√°s r√°pido)',\n",
        "        'monitoreo': 'UI avanzada integrada',\n",
        "        'colaboraci√≥n': 'Notebooks compartidos',\n",
        "        'costo': 'Licencia + infra (pero m√°s productivo)'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Mejoras espec√≠ficas de Databricks sobre Spark\n",
        "mejoras_databricks = \"\"\"\n",
        "1. PHOTON ENGINE\n",
        "   - Motor nativo en C++ (vs JVM)\n",
        "   - 3-8x m√°s r√°pido en consultas SQL\n",
        "   - Gratuito en Databricks (no en Spark vanilla)\n",
        "\n",
        "2. AUTO-TUNING\n",
        "   - Databricks ajusta configuraci√≥n autom√°ticamente\n",
        "   - Spark vanilla: Debes configurar manualmente 50+ par√°metros\n",
        "\n",
        "3. DELTA LAKE\n",
        "   - ACID transactions (no en Parquet vanilla)\n",
        "   - Time Travel\n",
        "   - Optimizaciones autom√°ticas\n",
        "\n",
        "4. ADAPTIVE QUERY EXECUTION\n",
        "   - Re-optimiza queries durante ejecuci√≥n\n",
        "   - Ajusta joins din√°micamente\n",
        "   - Versi√≥n mejorada vs Spark OSS\n",
        "\n",
        "5. LIQUID CLUSTERING\n",
        "   - Clustering autom√°tico de datos\n",
        "   - No disponible en Spark vanilla\n",
        "\"\"\"\n",
        "\n",
        "print(mejoras_databricks)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 2: Relaci√≥n con Cloud Providers**\n",
        "\n",
        "#### **Databricks es Multi-Cloud**\n",
        "\n",
        "```markdown\n",
        "‚òÅÔ∏è DATABRICKS EN LOS 3 GRANDES CLOUDS\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ    DATABRICKS ON AWS                    ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Almacenamiento: Amazon S3               ‚îÇ\n",
        "‚îÇ Compute: EC2 instances                  ‚îÇ\n",
        "‚îÇ Networking: VPC, Security Groups        ‚îÇ\n",
        "‚îÇ Identity: IAM Roles                     ‚îÇ\n",
        "‚îÇ Integraci√≥n: Redshift, RDS, Kinesis     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ    DATABRICKS ON AZURE                  ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Almacenamiento: Azure Data Lake Storage ‚îÇ\n",
        "‚îÇ Compute: Azure VMs                      ‚îÇ\n",
        "‚îÇ Networking: VNet, NSG                   ‚îÇ\n",
        "‚îÇ Identity: Azure AD, Managed Identities  ‚îÇ\n",
        "‚îÇ Integraci√≥n: Synapse, SQL DB, Event Hub ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ    DATABRICKS ON GCP                    ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Almacenamiento: Google Cloud Storage    ‚îÇ\n",
        "‚îÇ Compute: Compute Engine VMs             ‚îÇ\n",
        "‚îÇ Networking: VPC, Firewall Rules         ‚îÇ\n",
        "‚îÇ Identity: Service Accounts              ‚îÇ\n",
        "‚îÇ Integraci√≥n: BigQuery, Cloud SQL, Pub/Sub‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "#### **Arquitectura en la Nube**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "C√ìMO FUNCIONA DATABRICKS EN LA NUBE\n",
        "\"\"\"\n",
        "\n",
        "# Databricks tiene 2 planos:\n",
        "\n",
        "plano_control = {\n",
        "    'qu√©_es': 'Gesti√≥n de Databricks (UI, Jobs, Notebooks)',\n",
        "    'd√≥nde': 'Managed by Databricks (AWS/Azure/GCP)',\n",
        "    'responsable': 'Databricks',\n",
        "    'contiene': [\n",
        "        'Web Application (UI)',\n",
        "        'Cluster Manager',\n",
        "        'Job Scheduler',\n",
        "        'Notebook Server',\n",
        "        'MLflow Tracking Server'\n",
        "    ]\n",
        "}\n",
        "\n",
        "plano_datos = {\n",
        "    'qu√©_es': 'Procesamiento y almacenamiento de datos',\n",
        "    'd√≥nde': 'Tu cuenta de cloud (AWS/Azure/GCP)',\n",
        "    'responsable': 'T√∫ (cliente)',\n",
        "    'contiene': [\n",
        "        'Spark Clusters (VMs)',\n",
        "        'Almacenamiento (S3/ADLS/GCS)',\n",
        "        'Networking (VPC)',\n",
        "        'Datos (NUNCA salen de tu cloud)'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"üîí SEGURIDAD CLAVE:\")\n",
        "print(\"   Tus datos NUNCA salen de tu cuenta cloud\")\n",
        "print(\"   Databricks solo gestiona la orquestaci√≥n\")\n",
        "print(\"   Cumplimiento: GDPR, HIPAA, SOC 2, etc.\")\n",
        "```\n",
        "\n",
        "#### **Arquitectura T√©cnica Detallada (AWS como ejemplo)**\n",
        "\n",
        "```markdown\n",
        "üèóÔ∏è ARQUITECTURA DATABRICKS ON AWS\n",
        "\n",
        "TU CUENTA AWS\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                                                ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
        "‚îÇ  ‚îÇ  VPC (Red Privada)                        ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  DATABRICKS WORKSPACE               ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ                                     ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ Driver   ‚îÇ  ‚îÇ Worker 1 ‚îÇ       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  Node    ‚îÇ  ‚îÇ          ‚îÇ       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ (EC2)    ‚îÇ  ‚îÇ  (EC2)   ‚îÇ       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ                                     ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ Worker 2 ‚îÇ  ‚îÇ Worker N ‚îÇ       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  (EC2)   ‚îÇ  ‚îÇ  (EC2)   ‚îÇ       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
        "‚îÇ                                                ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
        "‚îÇ  ‚îÇ  AMAZON S3 (ALMACENAMIENTO)              ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  /dbfs/                             ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  /delta/                            ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  /mnt/                              ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
        "‚îÇ                                                ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚ñ≤\n",
        "         ‚îÇ Secure Communication\n",
        "         ‚îÇ\n",
        "DATABRICKS CONTROL PLANE (Managed by Databricks)\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Web UI ‚îÇ Notebooks ‚îÇ Jobs ‚îÇ Cluster Manager  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "#### **Ventajas de la Arquitectura Cloud**\n",
        "\n",
        "```python\n",
        "ventajas_cloud = {\n",
        "    '1. Separaci√≥n de compute y storage': {\n",
        "        'beneficio': 'Escala independientemente',\n",
        "        'ejemplo': \"\"\"\n",
        "        - Puedes tener 1 PB de datos en S3 (barato)\n",
        "        - Solo pagas por compute cuando procesas\n",
        "        - Apagas clusters cuando no los usas ‚Üí $0\n",
        "        \"\"\"\n",
        "    },\n",
        "    \n",
        "    '2. Elasticidad': {\n",
        "        'beneficio': 'Recursos bajo demanda',\n",
        "        'ejemplo': \"\"\"\n",
        "        Lunes 9am: Cluster de 2 workers\n",
        "        Lunes 2pm: Pico de trabajo, auto-scale a 50 workers\n",
        "        Lunes 6pm: De vuelta a 2 workers\n",
        "        Noche: Cluster apagado ‚Üí $0\n",
        "        \"\"\"\n",
        "    },\n",
        "    \n",
        "    '3. Durabilidad': {\n",
        "        'beneficio': 'Datos ultra-seguros',\n",
        "        'ejemplo': \"\"\"\n",
        "        S3: 99.999999999% durabilidad (11 nueves)\n",
        "        ‚Üí Pr√°cticamente imposible perder datos\n",
        "        ‚Üí Replicaci√≥n autom√°tica\n",
        "        \"\"\"\n",
        "    },\n",
        "    \n",
        "    '4. Multi-regi√≥n': {\n",
        "        'beneficio': 'Baja latencia global',\n",
        "        'ejemplo': \"\"\"\n",
        "        Workspace en EU-West (Europa)\n",
        "        Workspace en US-East (Norte Am√©rica)\n",
        "        Workspace en AP-Southeast (Asia)\n",
        "        ‚Üí Usuarios acceden al m√°s cercano\n",
        "        \"\"\"\n",
        "    },\n",
        "    \n",
        "    '5. Pay-as-you-go': {\n",
        "        'beneficio': 'Solo pagas lo que usas',\n",
        "        'ejemplo': \"\"\"\n",
        "        NO necesitas:\n",
        "        - Comprar servidores ($100K+)\n",
        "        - Mantener data center\n",
        "        - Personal de ops 24/7\n",
        "        \n",
        "        Pagas:\n",
        "        - Compute por minuto\n",
        "        - Storage por GB/mes\n",
        "        - Networking (m√≠nimo)\n",
        "        \"\"\"\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Integraci√≥n con Servicios Cloud**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DATABRICKS SE INTEGRA NATIVAMENTE CON TODO EL ECOSISTEMA CLOUD\n",
        "\"\"\"\n",
        "\n",
        "# ===== AWS =====\n",
        "integraciones_aws = {\n",
        "    'data_sources': [\n",
        "        'S3 (almacenamiento)',\n",
        "        'RDS (bases de datos SQL)',\n",
        "        'DynamoDB (NoSQL)',\n",
        "        'Redshift (data warehouse)',\n",
        "        'Kinesis (streaming)',\n",
        "        'MSK (Kafka managed)'\n",
        "    ],\n",
        "    'security': [\n",
        "        'IAM Roles',\n",
        "        'KMS (encriptaci√≥n)',\n",
        "        'Secrets Manager',\n",
        "        'VPC Endpoints'\n",
        "    ],\n",
        "    'analytics': [\n",
        "        'Athena',\n",
        "        'QuickSight',\n",
        "        'SageMaker'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Ejemplo: Leer desde S3\n",
        "df_s3 = spark.read \\\n",
        "    .format(\"delta\") \\\n",
        "    .load(\"s3://mi-bucket/datos/\")\n",
        "\n",
        "# Ejemplo: Escribir a Redshift\n",
        "df.write \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:redshift://cluster.amazonaws.com:5439/db\") \\\n",
        "    .option(\"dbtable\", \"ventas\") \\\n",
        "    .option(\"tempdir\", \"s3://temp-bucket/\") \\\n",
        "    .save()\n",
        "\n",
        "# ===== AZURE =====\n",
        "integraciones_azure = {\n",
        "    'data_sources': [\n",
        "        'ADLS Gen2 (almacenamiento)',\n",
        "        'Azure SQL Database',\n",
        "        'Cosmos DB (NoSQL)',\n",
        "        'Synapse Analytics',\n",
        "        'Event Hubs (streaming)',\n",
        "        'Azure Data Factory'\n",
        "    ],\n",
        "    'security': [\n",
        "        'Azure AD',\n",
        "        'Managed Identities',\n",
        "        'Key Vault',\n",
        "        'Private Link'\n",
        "    ],\n",
        "    'ai': [\n",
        "        'Azure ML',\n",
        "        'Cognitive Services',\n",
        "        'Power BI'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Ejemplo: Leer desde ADLS\n",
        "df_adls = spark.read \\\n",
        "    .format(\"delta\") \\\n",
        "    .load(\"abfss://container@storage.dfs.core.windows.net/datos/\")\n",
        "\n",
        "# ===== GCP =====\n",
        "integraciones_gcp = {\n",
        "    'data_sources': [\n",
        "        'GCS (almacenamiento)',\n",
        "        'BigQuery (data warehouse)',\n",
        "        'Cloud SQL',\n",
        "        'Firestore (NoSQL)',\n",
        "        'Pub/Sub (streaming)',\n",
        "        'Dataflow'\n",
        "    ],\n",
        "    'security': [\n",
        "        'Service Accounts',\n",
        "        'Cloud KMS',\n",
        "        'Secret Manager',\n",
        "        'VPC Service Controls'\n",
        "    ],\n",
        "    'ai': [\n",
        "        'Vertex AI',\n",
        "        'AutoML',\n",
        "        'Looker'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Ejemplo: Leer desde BigQuery\n",
        "df_bq = spark.read \\\n",
        "    .format(\"bigquery\") \\\n",
        "    .option(\"table\", \"proyecto.dataset.tabla\") \\\n",
        "    .load()\n",
        "```\n",
        "\n",
        "#### **Comparaci√≥n: Cloud vs On-Premise**\n",
        "\n",
        "```python\n",
        "# Escenario: Empresa necesita procesar 10 TB diarios\n",
        "\n",
        "# ===== ON-PREMISE (Data Center Propio) =====\n",
        "on_premise = {\n",
        "    'capex_inicial': '$500,000',  # Servidores, storage, networking\n",
        "    'setup_tiempo': '6-12 meses',\n",
        "    'personal': '5-10 ingenieros DevOps/SRE',\n",
        "    'opex_anual': '$200,000',  # Electricidad, cooling, mantenimiento\n",
        "    'escalabilidad': 'Comprar m√°s hardware (meses)',\n",
        "    'disponibilidad': '99.9% (con trabajo)',\n",
        "    'disaster_recovery': 'Complejo y caro',\n",
        "    'actualizaciones': 'Manuales, arriesgadas'\n",
        "}\n",
        "\n",
        "# ===== CLOUD (Databricks on AWS/Azure/GCP) =====\n",
        "cloud = {\n",
        "    'capex_inicial': '$0',  # No hardware\n",
        "    'setup_tiempo': '1 d√≠a',\n",
        "    'personal': '1-2 Data Engineers',\n",
        "    'opex_mensual': '$15,000',  # Solo lo que usas\n",
        "    'escalabilidad': 'Instant√°nea (auto-scaling)',\n",
        "    'disponibilidad': '99.99% (SLA garantizado)',\n",
        "    'disaster_recovery': 'Autom√°tico, multi-regi√≥n',\n",
        "    'actualizaciones': 'Autom√°ticas, sin downtime'\n",
        "}\n",
        "\n",
        "# C√°lculo ROI (3 a√±os)\n",
        "roi_on_premise = 500_000 + (200_000 * 3)  # $1,100,000\n",
        "roi_cloud = 15_000 * 36  # $540,000\n",
        "\n",
        "ahorro = roi_on_premise - roi_cloud\n",
        "print(f\"üí∞ Ahorro en 3 a√±os: ${ahorro:,}\")\n",
        "print(f\"üí∞ Ahorro porcentual: {ahorro/roi_on_premise*100:.1f}%\")\n",
        "\n",
        "\"\"\"\n",
        "Resultado:\n",
        "üí∞ Ahorro en 3 a√±os: $560,000\n",
        "üí∞ Ahorro porcentual: 50.9%\n",
        "\n",
        "Y esto SIN contar:\n",
        "+ Flexibilidad\n",
        "+ Velocidad de innovaci√≥n\n",
        "+ Menor riesgo t√©cnico\n",
        "+ Mejor disaster recovery\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Entendiendo la Arquitectura**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "EJERCICIO CONCEPTUAL: Flujo de datos completo\n",
        "\"\"\"\n",
        "\n",
        "# Simulaci√≥n de arquitectura\n",
        "print(\"üèóÔ∏è ARQUITECTURA DATABRICKS EN ACCI√ìN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# PASO 1: Datos llegan a la nube\n",
        "print(\"\\n1Ô∏è‚É£ INGESTA DE DATOS\")\n",
        "print(\"   üì§ Aplicaci√≥n m√≥vil ‚Üí API Gateway\")\n",
        "print(\"   üì• API Gateway ‚Üí Kinesis Stream (AWS)\")\n",
        "print(\"   üíæ Kinesis ‚Üí S3 (raw data)\")\n",
        "print(\"   ‚úÖ Datos persistidos en tu cuenta AWS\")\n",
        "\n",
        "# PASO 2: Databricks procesa\n",
        "print(\"\\n2Ô∏è‚É£ PROCESAMIENTO CON DATABRICKS\")\n",
        "print(\"   üñ•Ô∏è  Control Plane (Databricks):\")\n",
        "print(\"      - Usuario abre notebook\")\n",
        "print(\"      - Databricks crea cluster en TU VPC\")\n",
        "print(\"   ‚öôÔ∏è  Data Plane (Tu AWS):\")\n",
        "print(\"      - Cluster lee de S3\")\n",
        "print(\"      - Spark procesa en memoria (EC2)\")\n",
        "print(\"      - Escribe a Delta Lake (S3)\")\n",
        "\n",
        "# PASO 3: Consumo\n",
        "print(\"\\n3Ô∏è‚É£ CONSUMO DE DATOS\")\n",
        "print(\"   üìä Analyst: SQL en Databricks ‚Üí Dashboard\")\n",
        "print(\"   ü§ñ ML Model: Lee Delta ‚Üí Predicciones ‚Üí S3\")\n",
        "print(\"   üìà BI Tool: Power BI/Tableau ‚Üí Databricks SQL Warehouse\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üîí SEGURIDAD:\")\n",
        "print(\"   ‚úÖ Datos NUNCA salen de tu cuenta AWS\")\n",
        "print(\"   ‚úÖ Encriptaci√≥n en tr√°nsito y en reposo\")\n",
        "print(\"   ‚úÖ Acceso controlado por IAM roles\")\n",
        "print(\"=\"*60)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 1.4 - Decisiones de Arquitectura**\n",
        "\n",
        "```markdown\n",
        "### Ejercicio: Elige la mejor arquitectura\n",
        "\n",
        "**Escenario 1**: Startup con presupuesto limitado, necesita analizar 100 GB de datos hist√≥ricos\n",
        "\n",
        "Opciones:\n",
        "A) Databricks on AWS con cluster auto-scaling\n",
        "B) Comprar servidor f√≠sico ($50K)\n",
        "C) Pandas en laptop\n",
        "\n",
        "Tu elecci√≥n: ___________\n",
        "Justificaci√≥n: ___________\n",
        "\n",
        "\n",
        "**Escenario 2**: Banco multinacional, regulaciones estrictas de que datos NO pueden salir del pa√≠s\n",
        "\n",
        "Opciones:\n",
        "A) Databricks SaaS (Control Plane en US)\n",
        "B) Databricks con Customer-Managed VPC (Data Plane en tu regi√≥n)\n",
        "C) On-premise Spark cluster\n",
        "\n",
        "Tu elecci√≥n: ___________\n",
        "Justificaci√≥n: ___________\n",
        "\n",
        "\n",
        "**Escenario 3**: Empresa retail con picos de Black Friday (100x tr√°fico normal)\n",
        "\n",
        "Opciones:\n",
        "A) Cluster fijo on-premise (dimensionado para pico)\n",
        "B) Databricks con auto-scaling (2-100 workers)\n",
        "C) Cluster fijo cloud (dimensionado para promedio)\n",
        "\n",
        "Tu elecci√≥n: ___________\n",
        "Justificaci√≥n: ___________\n",
        "\n",
        "\n",
        "**Escenario 4**: Proyecto de investigaci√≥n, presupuesto $0, dataset 50 GB\n",
        "\n",
        "Opciones:\n",
        "A) Databricks Community Edition (gratis)\n",
        "B) Comprar licencias enterprise\n",
        "C) Google Colab con Pandas\n",
        "\n",
        "Tu elecci√≥n: ___________\n",
        "Justificaci√≥n: ___________\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **Apache Spark** es el motor que hace posible el procesamiento distribuido\n",
        "2. **Databricks optimiza Spark** con Photon, auto-tuning, y Delta Lake\n",
        "3. **Lazy evaluation** permite optimizaciones autom√°ticas\n",
        "4. **Databricks es multi-cloud**: funciona igual en AWS, Azure y GCP\n",
        "5. **Separaci√≥n compute/storage** permite escalabilidad independiente\n",
        "6. **Tus datos NUNCA salen** de tu cuenta cloud (seguridad)\n",
        "7. **Pay-as-you-go** elimina CAPEX y reduce OPEX vs on-premise\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **No necesitas ser experto en Spark** para usar Databricks - la plataforma abstrae la complejidad\n",
        "\n",
        "‚úÖ **Elige tu cloud** bas√°ndote en:\n",
        "   - D√≥nde est√°n tus datos actuales\n",
        "   - Qu√© servicios usas (AWS ‚Üí Databricks on AWS)\n",
        "   - Regulaciones (GDPR puede requerir EU region)\n",
        "\n",
        "‚úÖ **Empieza peque√±o, escala cuando necesites**\n",
        "   - Community Edition para aprender\n",
        "   - Trial para POCs\n",
        "   - Production cuando est√©s listo\n",
        "\n",
        "‚úÖ **Databricks no es solo para Big Data**\n",
        "   - √ötil incluso con datasets medianos (GB)\n",
        "   - La colaboraci√≥n y governance valen la pena\n",
        "\n",
        "‚úÖ **Cloud es el presente y futuro**\n",
        "   - 90%+ de nuevos proyectos de datos est√°n en cloud\n",
        "   - Habilidad muy demandada en el mercado laboral\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el siguiente tema **Tema 2: Fundamentos de Apache Spark**?"
      ],
      "metadata": {
        "id": "WEJEcOz8T8h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tema 2: Fundamentos de Apache Spark**\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1 ¬øQu√© es Apache Spark y procesamiento distribuido?**\n",
        "\n",
        "#### **Introducci√≥n: El Problema que Spark Resuelve**\n",
        "\n",
        "Imagina que tienes que contar todas las palabras en un libro de 100 p√°ginas:\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ESCENARIO 1: Una persona (Procesamiento Secuencial)\n",
        "\"\"\"\n",
        "# T√∫ solo, leyendo p√°gina por p√°gina\n",
        "tiempo_una_persona = \"100 p√°ginas √ó 2 minutos/p√°gina = 200 minutos (3.3 horas)\"\n",
        "\n",
        "\"\"\"\n",
        "ESCENARIO 2: 10 personas (Procesamiento Distribuido)\n",
        "\"\"\"\n",
        "# Cada persona lee 10 p√°ginas en paralelo\n",
        "tiempo_diez_personas = \"10 p√°ginas √ó 2 minutos/p√°gina = 20 minutos\"\n",
        "\n",
        "# Ventaja: 10x m√°s r√°pido\n",
        "speedup = 200 / 20\n",
        "print(f\"‚ö° Speedup: {speedup}x m√°s r√°pido\")\n",
        "\n",
        "\"\"\"\n",
        "Este es el concepto fundamental del procesamiento distribuido:\n",
        "DIVIDIR el trabajo entre m√∫ltiples trabajadores que operan en PARALELO\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "**Apache Spark** es el framework que hace esto posible con datos a gran escala.\n",
        "\n",
        "---\n",
        "\n",
        "### **¬øQu√© es Apache Spark?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DEFINICI√ìN FORMAL\n",
        "\"\"\"\n",
        "\n",
        "apache_spark = {\n",
        "    'qu√©_es': 'Motor de procesamiento de datos distribuido y de prop√≥sito general',\n",
        "    'creado': '2009 en UC Berkeley (AMPLab)',\n",
        "    'open_source': 'S√≠, Apache License 2.0',\n",
        "    'lenguajes': ['Scala (nativo)', 'Python (PySpark)', 'Java', 'R', 'SQL'],\n",
        "    'velocidad': 'Hasta 100x m√°s r√°pido que Hadoop MapReduce',\n",
        "    'caracter√≠stica_clave': 'Procesamiento en memoria (in-memory computing)',\n",
        "    'casos_de_uso': [\n",
        "        'Procesamiento batch',\n",
        "        'Streaming en tiempo real',\n",
        "        'Machine Learning',\n",
        "        'An√°lisis SQL',\n",
        "        'Procesamiento de grafos'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"üî• APACHE SPARK\")\n",
        "print(\"=\"*60)\n",
        "for key, value in apache_spark.items():\n",
        "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "```\n",
        "\n",
        "#### **¬øPor qu√© \"Spark\" (Chispa)?**\n",
        "\n",
        "El nombre refleja la **velocidad** del procesamiento:\n",
        "\n",
        "```python\n",
        "# Comparaci√≥n hist√≥rica\n",
        "\n",
        "hadoop_mapreduce = {\n",
        "    'a√±o': 2006,\n",
        "    'enfoque': 'Disco ‚Üí Procesar ‚Üí Disco (repetir)',\n",
        "    'velocidad': 'üêå Lento (horas/d√≠as)',\n",
        "    'memoria': 'Escribe a disco en cada paso'\n",
        "}\n",
        "\n",
        "apache_spark = {\n",
        "    'a√±o': 2009,\n",
        "    'enfoque': 'Cargar en memoria ‚Üí Procesar todo ‚Üí Escribir una vez',\n",
        "    'velocidad': '‚ö° R√°pido (minutos/segundos)',\n",
        "    'memoria': 'Todo en RAM cuando es posible'\n",
        "}\n",
        "\n",
        "print(\"EVOLUCI√ìN DEL PROCESAMIENTO DISTRIBUIDO:\")\n",
        "print(\"\\nüìÖ 2006 - HADOOP MAPREDUCE:\")\n",
        "print(\"   Paso 1: Leer 100GB del disco ‚Üí Procesar ‚Üí Escribir 100GB\")\n",
        "print(\"   Paso 2: Leer 100GB del disco ‚Üí Procesar ‚Üí Escribir 100GB\")\n",
        "print(\"   Paso 3: Leer 100GB del disco ‚Üí Procesar ‚Üí Escribir 100GB\")\n",
        "print(\"   ‚è±Ô∏è  Total: 12 horas\")\n",
        "\n",
        "print(\"\\nüìÖ 2014 - APACHE SPARK:\")\n",
        "print(\"   Paso 1: Leer 100GB del disco ‚Üí Cargar en RAM\")\n",
        "print(\"   Pasos 2-N: Procesar en RAM (sin disco)\")\n",
        "print(\"   √öltimo paso: Escribir resultado final al disco\")\n",
        "print(\"   ‚è±Ô∏è  Total: 30 minutos\")\n",
        "\n",
        "print(\"\\n‚ö° Mejora: 24x m√°s r√°pido\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Procesamiento Distribuido: Conceptos Fundamentales**\n",
        "\n",
        "#### **1. ¬øQu√© es el Procesamiento Distribuido?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "PROCESAMIENTO DISTRIBUIDO =\n",
        "Dividir una tarea grande en sub-tareas peque√±as que se ejecutan\n",
        "en paralelo en m√∫ltiples computadoras (nodos)\n",
        "\"\"\"\n",
        "\n",
        "# Analog√≠a: Restaurante\n",
        "restaurante_analogia = \"\"\"\n",
        "üçΩÔ∏è RESTAURANTE SIN DISTRIBUCI√ìN (1 chef):\n",
        "- 1 chef prepara todos los platos\n",
        "- Atiende 1 mesa a la vez\n",
        "- Capacidad: 10 clientes/hora\n",
        "- Si llegan 100 clientes ‚Üí 10 horas de espera\n",
        "\n",
        "üçΩÔ∏è RESTAURANTE CON DISTRIBUCI√ìN (10 chefs):\n",
        "- 10 chefs trabajan en paralelo\n",
        "- Cada chef prepara platos simult√°neamente\n",
        "- Coordinador (ma√Ætre) asigna tareas\n",
        "- Capacidad: 100 clientes/hora\n",
        "- Si llegan 100 clientes ‚Üí 1 hora de espera\n",
        "\n",
        "CONCEPTOS SPARK:\n",
        "- Chefs = Workers (nodos de trabajo)\n",
        "- Ma√Ætre = Driver (coordinador)\n",
        "- Platos = Particiones de datos\n",
        "- Cocinar = Transformaciones\n",
        "- Servir = Acciones\n",
        "\"\"\"\n",
        "\n",
        "print(restaurante_analogia)\n",
        "```\n",
        "\n",
        "#### **2. Componentes de un Sistema Distribuido Spark**\n",
        "\n",
        "```markdown\n",
        "üèóÔ∏è ARQUITECTURA SPARK\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                   DRIVER                        ‚îÇ\n",
        "‚îÇ  (El Cerebro - Coordina todo)                   ‚îÇ\n",
        "‚îÇ                                                 ‚îÇ\n",
        "‚îÇ  ‚Ä¢ SparkContext / SparkSession                  ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Analiza el c√≥digo                            ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Crea plan de ejecuci√≥n                       ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Distribuye tareas a Workers                  ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Recolecta resultados                         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "               ‚îÇ\n",
        "               ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "               ‚îÇ             ‚îÇ              ‚îÇ              ‚îÇ\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ  WORKER 1   ‚îÇ ‚îÇ  WORKER 2  ‚îÇ ‚îÇ  WORKER 3  ‚îÇ ‚îÇ  WORKER N  ‚îÇ\n",
        "        ‚îÇ             ‚îÇ ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ\n",
        "        ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
        "        ‚îÇ ‚îÇExecutor ‚îÇ ‚îÇ ‚îÇ ‚îÇExecutor‚îÇ ‚îÇ ‚îÇ ‚îÇExecutor‚îÇ ‚îÇ ‚îÇ ‚îÇExecutor‚îÇ ‚îÇ\n",
        "        ‚îÇ ‚îÇ         ‚îÇ ‚îÇ ‚îÇ ‚îÇ        ‚îÇ ‚îÇ ‚îÇ ‚îÇ        ‚îÇ ‚îÇ ‚îÇ ‚îÇ        ‚îÇ ‚îÇ\n",
        "        ‚îÇ ‚îÇ Task 1  ‚îÇ ‚îÇ ‚îÇ ‚îÇ Task 2 ‚îÇ ‚îÇ ‚îÇ ‚îÇ Task 3 ‚îÇ ‚îÇ ‚îÇ ‚îÇ Task N ‚îÇ ‚îÇ\n",
        "        ‚îÇ ‚îÇ Task 2  ‚îÇ ‚îÇ ‚îÇ ‚îÇ Task 3 ‚îÇ ‚îÇ ‚îÇ ‚îÇ Task 4 ‚îÇ ‚îÇ ‚îÇ ‚îÇ Task M ‚îÇ ‚îÇ\n",
        "        ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
        "        ‚îÇ             ‚îÇ ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ\n",
        "        ‚îÇ  Cache/RAM  ‚îÇ ‚îÇ Cache/RAM  ‚îÇ ‚îÇ Cache/RAM  ‚îÇ ‚îÇ Cache/RAM  ‚îÇ\n",
        "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "               ‚îÇ             ‚îÇ              ‚îÇ              ‚îÇ\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ           ALMACENAMIENTO DISTRIBUIDO                     ‚îÇ\n",
        "        ‚îÇ     (HDFS, S3, ADLS, Delta Lake)                        ‚îÇ\n",
        "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "#### **Glosario de Componentes**\n",
        "\n",
        "```python\n",
        "componentes_spark = {\n",
        "    'Driver': {\n",
        "        'funci√≥n': 'Nodo maestro que coordina la ejecuci√≥n',\n",
        "        'responsabilidades': [\n",
        "            'Ejecutar el c√≥digo del usuario (main)',\n",
        "            'Crear SparkContext',\n",
        "            'Construir el DAG (Directed Acyclic Graph)',\n",
        "            'Programar tareas en workers',\n",
        "            'Recolectar resultados'\n",
        "        ],\n",
        "        'analog√≠a': 'Director de orquesta'\n",
        "    },\n",
        "    \n",
        "    'Worker Node': {\n",
        "        'funci√≥n': 'Nodo que ejecuta el trabajo real',\n",
        "        'responsabilidades': [\n",
        "            'Ejecutar tareas asignadas por el Driver',\n",
        "            'Almacenar datos en memoria/disco',\n",
        "            'Reportar resultados al Driver'\n",
        "        ],\n",
        "        'analog√≠a': 'M√∫sico en la orquesta'\n",
        "    },\n",
        "    \n",
        "    'Executor': {\n",
        "        'funci√≥n': 'Proceso JVM en el Worker que ejecuta tareas',\n",
        "        'responsabilidades': [\n",
        "            'Ejecutar c√≥digo (transformaciones/acciones)',\n",
        "            'Almacenar datos en cache/memoria',\n",
        "            'Comunicarse con el Driver'\n",
        "        ],\n",
        "        'analog√≠a': 'Trabajador individual dentro del m√∫sico',\n",
        "        'nota': 'Cada Worker puede tener 1 o m√°s Executors'\n",
        "    },\n",
        "    \n",
        "    'Task': {\n",
        "        'funci√≥n': 'Unidad m√°s peque√±a de trabajo',\n",
        "        'responsabilidades': [\n",
        "            'Operar sobre una partici√≥n de datos',\n",
        "            'Ejecutar transformaciones'\n",
        "        ],\n",
        "        'analog√≠a': 'Una nota musical espec√≠fica',\n",
        "        'nota': 'Una Task = Una operaci√≥n sobre una partici√≥n'\n",
        "    },\n",
        "    \n",
        "    'Partition': {\n",
        "        'funci√≥n': 'Fragmento de datos distribuido',\n",
        "        'responsabilidades': [\n",
        "            'Almacenar porci√≥n de los datos totales',\n",
        "            'Ser procesada por una Task'\n",
        "        ],\n",
        "        'analog√≠a': 'Cap√≠tulo de un libro',\n",
        "        'nota': '# Particiones determina paralelismo'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Visualizaci√≥n\n",
        "print(\"üìö COMPONENTES DE SPARK\")\n",
        "print(\"=\"*60)\n",
        "for componente, info in componentes_spark.items():\n",
        "    print(f\"\\nüîπ {componente.upper()}\")\n",
        "    print(f\"   Funci√≥n: {info['funci√≥n']}\")\n",
        "    print(f\"   Analog√≠a: {info['analog√≠a']}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Ejemplo Pr√°ctico: Procesamiento Distribuido en Acci√≥n**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CASO PR√ÅCTICO: Contar palabras en 1 Bill√≥n de tweets\n",
        "\"\"\"\n",
        "\n",
        "# ===== DATOS =====\n",
        "datos = {\n",
        "    'total_tweets': 1_000_000_000,  # 1 bill√≥n\n",
        "    'tama√±o_promedio': 140,  # bytes por tweet\n",
        "    'tama√±o_total_gb': (1_000_000_000 * 140) / (1024**3),  # ~130 GB\n",
        "    'palabras_promedio': 20\n",
        "}\n",
        "\n",
        "print(\"üìä DATASET:\")\n",
        "print(f\"   Total tweets: {datos['total_tweets']:,}\")\n",
        "print(f\"   Tama√±o total: ~{datos['tama√±o_total_gb']:.0f} GB\")\n",
        "\n",
        "# ===== SIN SPARK (Una sola m√°quina) =====\n",
        "print(\"\\n‚ùå SIN SPARK (Procesamiento Secuencial):\")\n",
        "print(\"   M√°quina: 1 core, 16 GB RAM\")\n",
        "print(\"   Problema: Dataset no cabe en memoria\")\n",
        "print(\"   Tiempo estimado: IMPOSIBLE o d√≠as\")\n",
        "\n",
        "# ===== CON SPARK (Distribuido) =====\n",
        "print(\"\\n‚úÖ CON SPARK (Procesamiento Distribuido):\")\n",
        "print(\"   Cluster: 1 Driver + 10 Workers\")\n",
        "print(\"   Cada Worker: 4 cores, 32 GB RAM\")\n",
        "\n",
        "# Configuraci√≥n del cluster\n",
        "cluster = {\n",
        "    'workers': 10,\n",
        "    'cores_por_worker': 4,\n",
        "    'total_cores': 10 * 4,  # 40 cores\n",
        "    'ram_por_worker_gb': 32,\n",
        "    'total_ram_gb': 10 * 32  # 320 GB\n",
        "}\n",
        "\n",
        "# Particiones autom√°ticas\n",
        "particiones = cluster['total_cores'] * 2  # Regla general: 2-3x cores\n",
        "datos_por_particion_gb = datos['tama√±o_total_gb'] / particiones\n",
        "\n",
        "print(f\"   Total cores: {cluster['total_cores']}\")\n",
        "print(f\"   Total RAM: {cluster['total_ram_gb']} GB\")\n",
        "print(f\"   Particiones: {particiones}\")\n",
        "print(f\"   Datos por partici√≥n: ~{datos_por_particion_gb:.2f} GB\")\n",
        "\n",
        "# ===== EJECUCI√ìN PASO A PASO =====\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚öôÔ∏è  EJECUCI√ìN DISTRIBUIDA:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Paso 1: Cargar datos\n",
        "print(\"\\n1Ô∏è‚É£ CARGAR DATOS (Lectura paralela):\")\n",
        "print(\"   ‚Ä¢ Driver lee metadata de archivos en S3\")\n",
        "print(\"   ‚Ä¢ Driver divide datos en 80 particiones\")\n",
        "print(\"   ‚Ä¢ Asigna particiones a Workers disponibles\")\n",
        "print(\"   ‚Ä¢ Cada Worker lee sus particiones (~1.6 GB cada una)\")\n",
        "print(\"   ‚è±Ô∏è  Tiempo: 2 minutos (paralelo)\")\n",
        "\n",
        "# C√≥digo conceptual\n",
        "\"\"\"\n",
        "# Este c√≥digo se ejecuta en el Driver\n",
        "df_tweets = spark.read.text(\"s3://tweets/*.txt\")\n",
        "# Spark autom√°ticamente:\n",
        "# - Detecta 130 GB de datos\n",
        "# - Crea 80 particiones (~1.6 GB cada una)\n",
        "# - Distribuye a Workers\n",
        "\"\"\"\n",
        "\n",
        "# Paso 2: Transformaciones (Lazy)\n",
        "print(\"\\n2Ô∏è‚É£ TRANSFORMACIONES (En memoria, paralelo):\")\n",
        "print(\"   ‚Ä¢ Cada Worker procesa sus particiones en RAM\")\n",
        "print(\"   ‚Ä¢ Split de texto en palabras\")\n",
        "print(\"   ‚Ä¢ Filtrar palabras vac√≠as\")\n",
        "print(\"   ‚Ä¢ Map: palabra ‚Üí (palabra, 1)\")\n",
        "\n",
        "\"\"\"\n",
        "# Transformaciones (lazy - no ejecutan a√∫n)\n",
        "words = df_tweets.flatMap(lambda line: line.split(\" \"))\n",
        "word_counts = words.map(lambda word: (word, 1))\n",
        "\"\"\"\n",
        "\n",
        "print(\"   ‚úÖ Sin comunicaci√≥n entre Workers (eficiente)\")\n",
        "print(\"   ‚è±Ô∏è  Tiempo: 0 segundos (lazy, no ejecuta a√∫n)\")\n",
        "\n",
        "# Paso 3: Shuffle (Redistribuci√≥n)\n",
        "print(\"\\n3Ô∏è‚É£ SHUFFLE (Redistribuci√≥n de datos):\")\n",
        "print(\"   ‚Ä¢ Agrupar por palabra requiere reagrupar datos\")\n",
        "print(\"   ‚Ä¢ Palabras iguales deben ir al mismo Worker\")\n",
        "print(\"   ‚Ä¢ Workers intercambian datos por la red\")\n",
        "\n",
        "\"\"\"\n",
        "# Esta operaci√≥n causa shuffle\n",
        "result = word_counts.reduceByKey(lambda a, b: a + b)\n",
        "\"\"\"\n",
        "\n",
        "print(\"   ‚ö†Ô∏è  Operaci√≥n costosa (red)\")\n",
        "print(\"   ‚è±Ô∏è  Tiempo: 5 minutos\")\n",
        "\n",
        "# Paso 4: Agregaci√≥n\n",
        "print(\"\\n4Ô∏è‚É£ AGREGACI√ìN (Reducci√≥n paralela):\")\n",
        "print(\"   ‚Ä¢ Cada Worker suma conteos de sus palabras\")\n",
        "print(\"   ‚Ä¢ Resultado parcial por Worker\")\n",
        "\n",
        "print(\"   ‚è±Ô∏è  Tiempo: 1 minuto\")\n",
        "\n",
        "# Paso 5: Colectar resultados\n",
        "print(\"\\n5Ô∏è‚É£ ACCI√ìN (Trigger y recolecci√≥n):\")\n",
        "print(\"   ‚Ä¢ Driver solicita top 10 palabras\")\n",
        "print(\"   ‚Ä¢ Workers env√≠an sus top 10 al Driver\")\n",
        "print(\"   ‚Ä¢ Driver combina y ordena resultados finales\")\n",
        "\n",
        "\"\"\"\n",
        "# Acci√≥n - ejecuta toda la pipeline\n",
        "top_words = result.takeOrdered(10, key=lambda x: -x[1])\n",
        "\"\"\"\n",
        "\n",
        "print(\"   ‚è±Ô∏è  Tiempo: 30 segundos\")\n",
        "\n",
        "# Resumen\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä RESUMEN:\")\n",
        "print(\"=\"*60)\n",
        "print(\"‚è±Ô∏è  Tiempo total: ~9 minutos\")\n",
        "print(\"‚ö° Speedup vs 1 m√°quina: ~40x m√°s r√°pido\")\n",
        "print(\"üí∞ Costo: Solo pagas por 9 minutos de compute\")\n",
        "print(\"‚úÖ Escalable: Con 100 workers ‚Üí 1 minuto\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Conceptos Clave del Procesamiento Distribuido**\n",
        "\n",
        "#### **1. Particionamiento (Partitioning)**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "PARTICIONAMIENTO = Dividir datos en fragmentos\n",
        "\"\"\"\n",
        "\n",
        "# Ejemplo conceptual\n",
        "print(\"üì¶ PARTICIONAMIENTO DE DATOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Dataset original\n",
        "dataset_completo = {\n",
        "    'registros_totales': 1_000_000,\n",
        "    'tama√±o_gb': 10\n",
        "}\n",
        "\n",
        "# Sin particiones (imposible procesar distribuido)\n",
        "sin_particiones = \"\"\"\n",
        "‚ùå SIN PARTICIONES:\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  1,000,000 registros (10 GB)       ‚îÇ\n",
        "‚îÇ  [Todo en un solo bloque]          ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "‚Üí Solo 1 Worker puede procesarlo\n",
        "‚Üí No hay paralelismo\n",
        "‚Üí Lento\n",
        "\"\"\"\n",
        "\n",
        "# Con particiones\n",
        "con_particiones = \"\"\"\n",
        "‚úÖ CON 10 PARTICIONES:\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Part 1   ‚îÇ ‚îÇ Part 2   ‚îÇ ‚îÇ Part 3   ‚îÇ\n",
        "‚îÇ 100K     ‚îÇ ‚îÇ 100K     ‚îÇ ‚îÇ 100K     ‚îÇ\n",
        "‚îÇ (1 GB)   ‚îÇ ‚îÇ (1 GB)   ‚îÇ ‚îÇ (1 GB)   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ...\n",
        "‚îÇ Part 4   ‚îÇ ‚îÇ Part 5   ‚îÇ\n",
        "‚îÇ 100K     ‚îÇ ‚îÇ 100K     ‚îÇ\n",
        "‚îÇ (1 GB)   ‚îÇ ‚îÇ (1 GB)   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚Üí 10 Workers procesan en paralelo\n",
        "‚Üí M√°ximo paralelismo\n",
        "‚Üí 10x m√°s r√°pido\n",
        "\"\"\"\n",
        "\n",
        "print(sin_particiones)\n",
        "print(con_particiones)\n",
        "\n",
        "# Reglas de particionamiento\n",
        "reglas = \"\"\"\n",
        "üìè REGLAS DE PARTICIONAMIENTO:\n",
        "\n",
        "1. N√∫mero √≥ptimo de particiones:\n",
        "   ‚Ä¢ M√≠nimo: 2-3 particiones por core\n",
        "   ‚Ä¢ M√°ximo: Evitar particiones <128 MB\n",
        "   \n",
        "   Ejemplo: 40 cores ‚Üí 80-120 particiones\n",
        "\n",
        "2. Tama√±o de partici√≥n:\n",
        "   ‚Ä¢ Ideal: 128 MB - 1 GB por partici√≥n\n",
        "   ‚Ä¢ Demasiado peque√±as ‚Üí Overhead\n",
        "   ‚Ä¢ Demasiado grandes ‚Üí Problemas de memoria\n",
        "\n",
        "3. Balance:\n",
        "   ‚Ä¢ Particiones del mismo tama√±o (evitar skew)\n",
        "   ‚Ä¢ Distribuci√≥n uniforme entre Workers\n",
        "\"\"\"\n",
        "\n",
        "print(reglas)\n",
        "```\n",
        "\n",
        "#### **2. Paralelismo**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "PARALELISMO = M√∫ltiples operaciones simult√°neas\n",
        "\"\"\"\n",
        "\n",
        "# Niveles de paralelismo en Spark\n",
        "\n",
        "print(\"‚ö° NIVELES DE PARALELISMO EN SPARK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Nivel 1: Paralelismo de tareas\n",
        "paralelismo_tareas = \"\"\"\n",
        "1Ô∏è‚É£ PARALELISMO DE TAREAS:\n",
        "   \n",
        "   Cluster con 10 Workers, 4 cores cada uno = 40 cores\n",
        "   80 particiones de datos\n",
        "   \n",
        "   Ejecuci√≥n:\n",
        "   ‚Ä¢ Ronda 1: 40 tareas ejecutan simult√°neamente (1 por core)\n",
        "   ‚Ä¢ Ronda 2: Otras 40 tareas ejecutan simult√°neamente\n",
        "   ‚Ä¢ Total: 2 rondas para procesar 80 particiones\n",
        "   \n",
        "   Tiempo = Tiempo_por_tarea √ó (Particiones / Cores)\n",
        "\"\"\"\n",
        "\n",
        "# Nivel 2: Paralelismo de stages\n",
        "paralelismo_stages = \"\"\"\n",
        "2Ô∏è‚É£ PARALELISMO DE STAGES:\n",
        "\n",
        "   Pipeline: Read ‚Üí Filter ‚Üí GroupBy ‚Üí Aggregate\n",
        "   \n",
        "   ‚ö†Ô∏è  SECUENCIAL (dependencias):\n",
        "   Stage 1: Read + Filter    ‚Üí Completa primero\n",
        "   Stage 2: GroupBy (shuffle) ‚Üí Luego ejecuta\n",
        "   Stage 3: Aggregate        ‚Üí Finalmente ejecuta\n",
        "   \n",
        "   Dentro de cada stage ‚Üí Paralelismo de tareas\n",
        "\"\"\"\n",
        "\n",
        "# Nivel 3: Paralelismo de jobs\n",
        "paralelismo_jobs = \"\"\"\n",
        "3Ô∏è‚É£ PARALELISMO DE JOBS (Avanzado):\n",
        "\n",
        "   Si tienes 2 acciones independientes:\n",
        "   \n",
        "   Job 1: df1.filter(...).count()\n",
        "   Job 2: df2.select(...).write(...)\n",
        "   \n",
        "   ‚Üí Pueden ejecutarse en paralelo si hay recursos\n",
        "\"\"\"\n",
        "\n",
        "print(paralelismo_tareas)\n",
        "print(paralelismo_stages)\n",
        "print(paralelismo_jobs)\n",
        "```\n",
        "\n",
        "#### **3. Shuffle - El Villano del Procesamiento Distribuido**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "SHUFFLE = Redistribuci√≥n de datos entre Workers\n",
        "\"\"\"\n",
        "\n",
        "print(\"üîÄ SHUFFLE - OPERACI√ìN COSTOSA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ¬øQu√© es shuffle?\n",
        "shuffle_explicacion = \"\"\"\n",
        "ANTES DEL SHUFFLE:\n",
        "Worker 1: [(\"casa\", 1), (\"perro\", 1), (\"casa\", 1)]\n",
        "Worker 2: [(\"gato\", 1), (\"casa\", 1), (\"perro\", 1)]\n",
        "Worker 3: [(\"perro\", 1), (\"casa\", 1), (\"gato\", 1)]\n",
        "\n",
        "DESPU√âS DEL SHUFFLE (agrupado por palabra):\n",
        "Worker 1: [(\"casa\", 1), (\"casa\", 1), (\"casa\", 1), (\"casa\", 1)]\n",
        "Worker 2: [(\"perro\", 1), (\"perro\", 1), (\"perro\", 1)]\n",
        "Worker 3: [(\"gato\", 1), (\"gato\", 1)]\n",
        "\n",
        "üì° Requiere comunicaci√≥n por red entre Workers\n",
        "üí∞ Costoso en tiempo y recursos\n",
        "\"\"\"\n",
        "\n",
        "print(shuffle_explicacion)\n",
        "\n",
        "# Operaciones que causan shuffle\n",
        "operaciones_shuffle = \"\"\"\n",
        "‚ö†Ô∏è  OPERACIONES QUE CAUSAN SHUFFLE:\n",
        "\n",
        "‚úÖ Necesarias (inevitables):\n",
        "   ‚Ä¢ groupBy()\n",
        "   ‚Ä¢ reduceByKey()\n",
        "   ‚Ä¢ join()\n",
        "   ‚Ä¢ repartition()\n",
        "   ‚Ä¢ distinct()\n",
        "   ‚Ä¢ sortBy()\n",
        "\n",
        "‚ùå Evitables (optimiza):\n",
        "   ‚Ä¢ repartition() innecesarios\n",
        "   ‚Ä¢ joins sin broadcast\n",
        "   ‚Ä¢ groupBy cuando podr√≠as usar aggregates\n",
        "\n",
        "üí° ESTRATEGIAS PARA MINIMIZAR SHUFFLE:\n",
        "   1. Usar operaciones que no requieren shuffle cuando sea posible\n",
        "   2. Broadcast de datasets peque√±os en joins\n",
        "   3. Particionar datos estrat√©gicamente\n",
        "   4. Reusar particionamiento\n",
        "\"\"\"\n",
        "\n",
        "print(operaciones_shuffle)\n",
        "\n",
        "# Ejemplo de costo de shuffle\n",
        "ejemplo_shuffle = \"\"\"\n",
        "EJEMPLO REAL:\n",
        "\n",
        "Dataset: 100 GB en 10 Workers\n",
        "\n",
        "SIN SHUFFLE (filter, map, select):\n",
        "‚Üí Cada Worker procesa sus datos localmente\n",
        "‚Üí No hay comunicaci√≥n de red\n",
        "‚Üí Tiempo: 2 minutos\n",
        "\n",
        "CON SHUFFLE (groupBy):\n",
        "‚Üí Workers intercambian ~80 GB de datos\n",
        "‚Üí Red: 10 Gbps ‚Üí 64 segundos solo de transferencia\n",
        "‚Üí Overhead serializaci√≥n/deserializaci√≥n\n",
        "‚Üí Tiempo total: 5 minutos\n",
        "\n",
        "Diferencia: 2.5x m√°s lento\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_shuffle)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Simulaci√≥n Conceptual**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "EJERCICIO: Simular procesamiento distribuido\n",
        "\"\"\"\n",
        "\n",
        "# Simulaci√≥n simple de word count distribuido\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"üß™ SIMULACI√ìN: WORD COUNT DISTRIBUIDO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Dataset simulado\n",
        "documentos = [\n",
        "    \"apache spark es r√°pido\",\n",
        "    \"spark procesa datos grandes\",\n",
        "    \"python y spark trabajan juntos\",\n",
        "    \"spark es distribuido\"\n",
        "] * 25  # 100 documentos\n",
        "\n",
        "print(f\"üìÑ Total documentos: {len(documentos)}\")\n",
        "\n",
        "# Configuraci√≥n del cluster\n",
        "num_workers = 4\n",
        "particiones = 4\n",
        "\n",
        "# PASO 1: Particionar datos\n",
        "print(f\"\\n1Ô∏è‚É£ PARTICIONAR en {particiones} particiones:\")\n",
        "particiones_datos = [[] for _ in range(particiones)]\n",
        "\n",
        "for i, doc in enumerate(documentos):\n",
        "    partition_id = i % particiones\n",
        "    particiones_datos[partition_id].append(doc)\n",
        "\n",
        "for i, partition in enumerate(particiones_datos):\n",
        "    print(f\"   Partici√≥n {i}: {len(partition)} documentos\")\n",
        "\n",
        "# PASO 2: Map fase (cada worker procesa su partici√≥n)\n",
        "print(f\"\\n2Ô∏è‚É£ MAP (Paralelamente en {num_workers} workers):\")\n",
        "\n",
        "def map_function(docs):\n",
        "    \"\"\"Simula el trabajo de un worker en la fase map\"\"\"\n",
        "    word_counts = defaultdict(int)\n",
        "    for doc in docs:\n",
        "        for word in doc.split():\n",
        "            word_counts[word] += 1\n",
        "    return word_counts\n",
        "\n",
        "# Simular procesamiento paralelo\n",
        "resultados_parciales = []\n",
        "for i, partition in enumerate(particiones_datos):\n",
        "    resultado = map_function(partition)\n",
        "    resultados_parciales.append(resultado)\n",
        "    print(f\"   Worker {i}: {len(resultado)} palabras √∫nicas\")\n",
        "\n",
        "# PASO 3: Shuffle (redistribuir por palabra)\n",
        "print(\"\\n3Ô∏è‚É£ SHUFFLE (Redistribuir por palabra):\")\n",
        "print(\"   ‚ö†Ô∏è  Datos se mueven entre workers...\")\n",
        "\n",
        "# Combinar resultados (simula shuffle)\n",
        "all_words = defaultdict(list)\n",
        "for worker_result in resultados_parciales:\n",
        "    for word, count in worker_result.items():\n",
        "        all_words[word].append(count)\n",
        "\n",
        "print(f\"   ‚úÖ {len(all_words)} palabras √∫nicas despu√©s de shuffle\")\n",
        "\n",
        "# PASO 4: Reduce (agregar conteos finales)\n",
        "print(\"\\n4Ô∏è‚É£ REDUCE (Agregar conteos):\")\n",
        "\n",
        "resultado_final = {}\n",
        "for word, counts in all_words.items():\n",
        "    resultado_final[word] = sum(counts)\n",
        "\n",
        "# Ordenar por frecuencia\n",
        "top_words = sorted(resultado_final.items(),\n",
        "                   key=lambda x: x[1],\n",
        "                   reverse=True)[:5]\n",
        "\n",
        "print(\"\\nüìä TOP 5 PALABRAS:\")\n",
        "for word, count in top_words:\n",
        "    print(f\"   '{word}': {count} veces\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Procesamiento distribuido completado!\")\n",
        "print(\"=\"*60)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 2.1 - Comprensi√≥n**\n",
        "\n",
        "```markdown\n",
        "### Ejercicio: Dise√±a un sistema distribuido\n",
        "\n",
        "**Escenario**: Tienes 1 TB de datos (1,000 GB) de logs de servidor que necesitas procesar.\n",
        "\n",
        "**Tu cluster**:\n",
        "- 1 Driver (16 GB RAM)\n",
        "- 20 Workers (32 GB RAM cada uno, 4 cores)\n",
        "\n",
        "**Preguntas**:\n",
        "\n",
        "1. **¬øCu√°ntas particiones crear√≠as?**\n",
        "   F√≥rmula: ___________\n",
        "   Respuesta: ___________\n",
        "   \n",
        "2. **¬øCu√°l ser√≠a el tama√±o ideal de cada partici√≥n?**\n",
        "   C√°lculo: ___________\n",
        "   Respuesta: ___________\n",
        "\n",
        "3. **¬øCu√°ntas tareas podr√≠an ejecutarse simult√°neamente?**\n",
        "   C√°lculo: ___________\n",
        "   Respuesta: ___________\n",
        "\n",
        "4. **Si cada tarea tarda 2 minutos, ¬øcu√°nto tardar√≠a el proceso completo?**\n",
        "   C√°lculo: ___________\n",
        "   Respuesta: ___________\n",
        "\n",
        "5. **¬øQu√© pasar√≠a si usaras solo 10 particiones en lugar de las √≥ptimas?**\n",
        "   Problema: ___________\n",
        "   Impacto: ___________\n",
        "\n",
        "6. **Identifica qu√© operaciones causan shuffle**:\n",
        "   - [ ] df.filter(col(\"edad\") > 18)\n",
        "   - [ ] df.select(\"nombre\", \"edad\")\n",
        "   - [ ] df.groupBy(\"ciudad\").count()\n",
        "   - [ ] df.withColumn(\"edad_doble\", col(\"edad\") * 2)\n",
        "   - [ ] df.join(df2, \"id\")\n",
        "   - [ ] df.orderBy(\"edad\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **Spark = Procesamiento distribuido en memoria** (hasta 100x m√°s r√°pido que Hadoop)\n",
        "2. **Driver coordina**, **Workers ejecutan** el trabajo real\n",
        "3. **Particiones** determinan el nivel de paralelismo\n",
        "4. **Shuffle** es costoso - minim√≠zalo cuando sea posible\n",
        "5. **Paralelismo** permite procesar datos masivos en minutos vs d√≠as\n",
        "6. **Tama√±o √≥ptimo de partici√≥n**: 128 MB - 1 GB\n",
        "7. **Regla particiones**: 2-3x n√∫mero de cores en el cluster\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **M√°s Workers ‚â† Siempre mejor**\n",
        "   - Hay overhead de coordinaci√≥n\n",
        "   - Encuentra el balance para tu caso de uso\n",
        "\n",
        "‚úÖ **Monitorea particiones**\n",
        "   - Particiones desbalanceadas (skew) ‚Üí Workers ociosos\n",
        "   - Usa Spark UI para detectar problemas\n",
        "\n",
        "‚úÖ **Shuffle es inevitable a veces**\n",
        "   - No evites groupBy/join si los necesitas\n",
        "   - Optimiza, pero no sacrifiques l√≥gica de negocio\n",
        "\n",
        "‚úÖ **In-memory ‚â† Todo cabe en RAM**\n",
        "   - Spark puede procesar datasets > RAM total\n",
        "   - Spill to disk cuando es necesario (m√°s lento pero funciona)\n",
        "\n",
        "‚úÖ **Prueba en peque√±o primero**\n",
        "   - Antes de procesar 1 TB, prueba con 1 GB\n",
        "   - Valida l√≥gica y estima tiempos\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el siguiente punto **2.2 RDDs, DataFrames y Datasets**?"
      ],
      "metadata": {
        "id": "IYCTuNc1UNtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 RDDs, DataFrames y Datasets**\n",
        "\n",
        "#### **Introducci√≥n: Las Tres Abstracciones de Spark**\n",
        "\n",
        "Apache Spark ofrece **tres abstracciones** para trabajar con datos distribuidos, cada una con diferentes niveles de optimizaci√≥n y facilidad de uso:\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "EVOLUCI√ìN DE LAS ABSTRACCIONES EN SPARK\n",
        "\"\"\"\n",
        "\n",
        "evolucion = \"\"\"\n",
        "üìÖ 2011 - RDD (Resilient Distributed Dataset)\n",
        "   ‚îú‚îÄ Primera abstracci√≥n de Spark\n",
        "   ‚îú‚îÄ Bajo nivel, m√°ximo control\n",
        "   ‚îî‚îÄ Sin optimizaciones autom√°ticas\n",
        "\n",
        "üìÖ 2013 - DataFrame\n",
        "   ‚îú‚îÄ Inspirado en Pandas y R\n",
        "   ‚îú‚îÄ API de alto nivel (SQL-like)\n",
        "   ‚îú‚îÄ Optimizaci√≥n autom√°tica (Catalyst)\n",
        "   ‚îî‚îÄ ‚≠ê M√ÅS USADO EN DATABRICKS\n",
        "\n",
        "üìÖ 2015 - Dataset\n",
        "   ‚îú‚îÄ Combina RDD + DataFrame\n",
        "   ‚îú‚îÄ Type-safe (solo Scala/Java)\n",
        "   ‚îî‚îÄ Usado principalmente en Scala\n",
        "\n",
        "HOY (2025):\n",
        "‚Üí 95% usamos DataFrames (PySpark/SQL)\n",
        "‚Üí 4% usan Datasets (Scala)\n",
        "‚Üí 1% usan RDDs (casos muy espec√≠ficos)\n",
        "\"\"\"\n",
        "\n",
        "print(evolucion)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 1: RDD (Resilient Distributed Dataset)**\n",
        "\n",
        "#### **¬øQu√© es un RDD?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "RDD = Resilient Distributed Dataset\n",
        "\"\"\"\n",
        "\n",
        "rdd_definicion = {\n",
        "    'R - Resilient': 'Tolerante a fallos (se recupera autom√°ticamente)',\n",
        "    'D - Distributed': 'Datos distribuidos en m√∫ltiples nodos',\n",
        "    'D - Dataset': 'Colecci√≥n de objetos',\n",
        "    'caracteristicas': [\n",
        "        'Inmutable (no se puede modificar)',\n",
        "        'Particionado (dividido en chunks)',\n",
        "        'Lazy evaluation',\n",
        "        'Bajo nivel (m√°s control, menos optimizaci√≥n)'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"üî∑ RDD - LA ABSTRACCI√ìN ORIGINAL\")\n",
        "print(\"=\"*60)\n",
        "for key, value in rdd_definicion.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "```\n",
        "\n",
        "#### **Anatom√≠a de un RDD**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ESTRUCTURA INTERNA DE UN RDD\n",
        "\"\"\"\n",
        "\n",
        "ejemplo_rdd = \"\"\"\n",
        "RDD con 12 elementos distribuidos en 3 particiones:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ           SPARK CLUSTER                 ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  Worker 1         Worker 2    Worker 3  ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
        "‚îÇ  ‚îÇPartition0‚îÇ   ‚îÇPartition1‚îÇ ‚îÇPart. 2 ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ\n",
        "‚îÇ  ‚îÇ   1      ‚îÇ   ‚îÇ    5     ‚îÇ ‚îÇ   9    ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ   2      ‚îÇ   ‚îÇ    6     ‚îÇ ‚îÇ  10    ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ   3      ‚îÇ   ‚îÇ    7     ‚îÇ ‚îÇ  11    ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ   4      ‚îÇ   ‚îÇ    8     ‚îÇ ‚îÇ  12    ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "Cada Worker procesa su partici√≥n independientemente\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_rdd)\n",
        "```\n",
        "\n",
        "#### **Operaciones con RDDs**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "OPERACIONES B√ÅSICAS CON RDDS\n",
        "\"\"\"\n",
        "\n",
        "# NOTA: Este c√≥digo es conceptual para entender RDDs\n",
        "# En Databricks, normalmente NO usar√≠as RDDs directamente\n",
        "\n",
        "# ===== CREAR UN RDD =====\n",
        "print(\"1Ô∏è‚É£ CREAR RDD:\")\n",
        "print()\n",
        "\n",
        "# Desde una lista (parallelize)\n",
        "# rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "# print(f\"RDD creado con {rdd.count()} elementos\")\n",
        "\n",
        "# Desde un archivo\n",
        "# rdd_texto = spark.sparkContext.textFile(\"/data/archivo.txt\")\n",
        "\n",
        "# ===== TRANSFORMACIONES (Lazy) =====\n",
        "print(\"\\n2Ô∏è‚É£ TRANSFORMACIONES RDD:\")\n",
        "print()\n",
        "\n",
        "transformaciones_rdd = \"\"\"\n",
        "# map() - Aplicar funci√≥n a cada elemento\n",
        "rdd_doubled = rdd.map(lambda x: x * 2)\n",
        "# [1,2,3] ‚Üí [2,4,6]\n",
        "\n",
        "# filter() - Filtrar elementos\n",
        "rdd_pares = rdd.filter(lambda x: x % 2 == 0)\n",
        "# [1,2,3,4,5] ‚Üí [2,4]\n",
        "\n",
        "# flatMap() - Map que retorna m√∫ltiples valores\n",
        "rdd_texto = sc.parallelize([\"hola mundo\", \"apache spark\"])\n",
        "rdd_palabras = rdd_texto.flatMap(lambda linea: linea.split(\" \"))\n",
        "# [\"hola mundo\", \"apache spark\"] ‚Üí [\"hola\", \"mundo\", \"apache\", \"spark\"]\n",
        "\n",
        "# reduceByKey() - Agregar por clave (causa shuffle)\n",
        "rdd_pares = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
        "rdd_agregado = rdd_pares.reduceByKey(lambda a, b: a + b)\n",
        "# [(\"a\",1), (\"b\",2), (\"a\",3)] ‚Üí [(\"a\",4), (\"b\",2)]\n",
        "\n",
        "‚ö†Ô∏è IMPORTANTE: Todas estas son LAZY (no ejecutan hasta una acci√≥n)\n",
        "\"\"\"\n",
        "\n",
        "print(transformaciones_rdd)\n",
        "\n",
        "# ===== ACCIONES (Trigger) =====\n",
        "print(\"\\n3Ô∏è‚É£ ACCIONES RDD (Ejecutan las transformaciones):\")\n",
        "print()\n",
        "\n",
        "acciones_rdd = \"\"\"\n",
        "# collect() - Traer todos los datos al Driver\n",
        "resultado = rdd.collect()\n",
        "# ‚ö†Ô∏è Peligroso con datasets grandes (OOM)\n",
        "\n",
        "# count() - Contar elementos\n",
        "total = rdd.count()\n",
        "\n",
        "# take(n) - Traer primeros n elementos\n",
        "primeros_5 = rdd.take(5)\n",
        "\n",
        "# reduce() - Reducir a un valor √∫nico\n",
        "suma = rdd.reduce(lambda a, b: a + b)\n",
        "\n",
        "# saveAsTextFile() - Guardar a archivo\n",
        "rdd.saveAsTextFile(\"/output/resultado\")\n",
        "\n",
        "# foreach() - Ejecutar funci√≥n en cada elemento (para side effects)\n",
        "rdd.foreach(lambda x: print(x))\n",
        "\"\"\"\n",
        "\n",
        "print(acciones_rdd)\n",
        "```\n",
        "\n",
        "#### **Ejemplo Completo: Word Count con RDD**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "EJEMPLO CL√ÅSICO: WORD COUNT CON RDD\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìù WORD COUNT CON RDD - PASO A PASO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Simulaci√≥n conceptual (el c√≥digo real ser√≠a en Databricks)\n",
        "\n",
        "word_count_rdd = \"\"\"\n",
        "# Paso 1: Leer archivo de texto\n",
        "texto_rdd = sc.textFile(\"/data/libros/*.txt\")\n",
        "# Lazy - no ejecuta a√∫n\n",
        "\n",
        "# Paso 2: Dividir l√≠neas en palabras (flatMap)\n",
        "palabras_rdd = texto_rdd.flatMap(lambda linea: linea.split(\" \"))\n",
        "# Cada l√≠nea se convierte en m√∫ltiples palabras\n",
        "# Lazy\n",
        "\n",
        "# Paso 3: Crear pares (palabra, 1)\n",
        "pares_rdd = palabras_rdd.map(lambda palabra: (palabra, 1))\n",
        "# \"hola\" ‚Üí (\"hola\", 1)\n",
        "# Lazy\n",
        "\n",
        "# Paso 4: Reducir por palabra (suma conteos)\n",
        "conteo_rdd = pares_rdd.reduceByKey(lambda a, b: a + b)\n",
        "# (\"hola\", 1) + (\"hola\", 1) + (\"hola\", 1) ‚Üí (\"hola\", 3)\n",
        "# Lazy, pero causa SHUFFLE\n",
        "\n",
        "# Paso 5: Ordenar por frecuencia (descendente)\n",
        "ordenado_rdd = conteo_rdd.sortBy(lambda x: x[1], ascending=False)\n",
        "# Lazy, causa SHUFFLE\n",
        "\n",
        "# Paso 6: Tomar top 10 (ACCI√ìN - ejecuta todo)\n",
        "top_10 = ordenado_rdd.take(10)\n",
        "# ‚ö° AQU√ç se ejecuta toda la pipeline\n",
        "\n",
        "# Resultado\n",
        "for palabra, conteo in top_10:\n",
        "    print(f\"{palabra}: {conteo}\")\n",
        "\"\"\"\n",
        "\n",
        "print(word_count_rdd)\n",
        "\n",
        "# Visualizaci√≥n del flujo\n",
        "flujo_visual = \"\"\"\n",
        "VISUALIZACI√ìN DEL FLUJO:\n",
        "\n",
        "DATOS ORIGINALES:\n",
        "[l√≠nea1, l√≠nea2, l√≠nea3, ...]\n",
        "\n",
        "‚Üì flatMap(split)\n",
        "\n",
        "[\"palabra1\", \"palabra2\", \"palabra3\", ...]\n",
        "\n",
        "‚Üì map(x => (x, 1))\n",
        "\n",
        "[(\"palabra1\", 1), (\"palabra2\", 1), (\"palabra1\", 1), ...]\n",
        "\n",
        "‚Üì reduceByKey(+)  [SHUFFLE]\n",
        "\n",
        "[(\"palabra1\", 5), (\"palabra2\", 3), ...]\n",
        "\n",
        "‚Üì sortBy(conteo)  [SHUFFLE]\n",
        "\n",
        "[(\"palabra1\", 5), (\"palabra2\", 3), (\"palabra3\", 2), ...]\n",
        "\n",
        "‚Üì take(10)  [ACCI√ìN]\n",
        "\n",
        "Top 10 palabras m√°s frecuentes\n",
        "\"\"\"\n",
        "\n",
        "print(flujo_visual)\n",
        "```\n",
        "\n",
        "#### **Problemas con RDDs**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "¬øPOR QU√â YA NO USAMOS RDDS?\n",
        "\"\"\"\n",
        "\n",
        "problemas_rdd = {\n",
        "    '1. Sin optimizaci√≥n autom√°tica': \"\"\"\n",
        "        # RDD\n",
        "        rdd.filter(...).map(...).filter(...)\n",
        "        ‚Üí Spark ejecuta exactamente como escribiste\n",
        "        ‚Üí No optimiza el orden de operaciones\n",
        "        \n",
        "        # DataFrame\n",
        "        df.filter(...).select(...).filter(...)\n",
        "        ‚Üí Catalyst Optimizer reordena autom√°ticamente\n",
        "        ‚Üí Ejecuta el plan m√°s eficiente\n",
        "    \"\"\",\n",
        "    \n",
        "    '2. Sin esquema (schema)': \"\"\"\n",
        "        # RDD: Spark no sabe qu√© contiene\n",
        "        rdd = sc.parallelize([{\"nombre\": \"Juan\", \"edad\": 25}])\n",
        "        ‚Üí Spark trata esto como \"objeto gen√©rico\"\n",
        "        ‚Üí No puede optimizar\n",
        "        \n",
        "        # DataFrame: Spark conoce la estructura\n",
        "        df = spark.createDataFrame([{\"nombre\": \"Juan\", \"edad\": 25}])\n",
        "        ‚Üí Spark sabe: columna \"nombre\" (string), \"edad\" (int)\n",
        "        ‚Üí Puede optimizar consultas\n",
        "    \"\"\",\n",
        "    \n",
        "    '3. API compleja': \"\"\"\n",
        "        # RDD: Funciones lambda complejas\n",
        "        rdd.map(lambda x: x[0]).filter(lambda x: x > 10)\n",
        "        \n",
        "        # DataFrame: SQL intuitivo\n",
        "        df.select(\"nombre\").filter(col(\"edad\") > 10)\n",
        "        df.select(\"nombre\").where(\"edad > 10\")  # SQL string\n",
        "    \"\"\",\n",
        "    \n",
        "    '4. Rendimiento': \"\"\"\n",
        "        BENCHMARK (mismo dataset, misma operaci√≥n):\n",
        "        \n",
        "        RDD:        100 segundos\n",
        "        DataFrame:   15 segundos  (6.6x m√°s r√°pido)\n",
        "        \n",
        "        ¬øPor qu√©? Optimizaci√≥n Catalyst + Tungsten execution\n",
        "    \"\"\",\n",
        "    \n",
        "    '5. Solo para Scala/Python avanzado': \"\"\"\n",
        "        RDD requiere pensar en bajo nivel\n",
        "        DataFrames/SQL son universales (analistas, ingenieros, cient√≠ficos)\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "print(\"\\n‚ùå PROBLEMAS CON RDDS:\")\n",
        "print(\"=\"*60)\n",
        "for problema, explicacion in problemas_rdd.items():\n",
        "    print(f\"\\n{problema}\")\n",
        "    print(explicacion)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 2: DataFrame - La Abstracci√≥n Moderna**\n",
        "\n",
        "#### **¬øQu√© es un DataFrame?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DATAFRAME = Tabla distribuida con esquema\n",
        "\"\"\"\n",
        "\n",
        "dataframe_definicion = {\n",
        "    'concepto': 'Tabla con filas y columnas (como Pandas o SQL)',\n",
        "    'esquema': 'Spark conoce el tipo de cada columna',\n",
        "    'distribuido': 'Datos particionados en el cluster',\n",
        "    'inmutable': 'Como RDD, no se modifica (se crean nuevos DFs)',\n",
        "    'optimizado': 'Catalyst Optimizer mejora queries autom√°ticamente',\n",
        "    'api': 'Python, Scala, Java, R, SQL'\n",
        "}\n",
        "\n",
        "print(\"üìä DATAFRAME - LA ABSTRACCI√ìN PREFERIDA\")\n",
        "print(\"=\"*60)\n",
        "for key, value in dataframe_definicion.items():\n",
        "    print(f\"{key.capitalize()}: {value}\")\n",
        "```\n",
        "\n",
        "#### **Anatom√≠a de un DataFrame**\n",
        "\n",
        "```markdown\n",
        "üìä ESTRUCTURA DE UN DATAFRAME\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                DATAFRAME                            ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  SCHEMA (Esquema conocido por Spark)                ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ\n",
        "‚îÇ  ‚îÇ  nombre   ‚îÇ   edad   ‚îÇ  ciudad   ‚îÇ               ‚îÇ\n",
        "‚îÇ  ‚îÇ (String)  ‚îÇ  (Int)   ‚îÇ (String)  ‚îÇ               ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  PARTICIONES (Distribuidas)                         ‚îÇ\n",
        "‚îÇ                                                     ‚îÇ\n",
        "‚îÇ  Worker 1         Worker 2         Worker 3         ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
        "‚îÇ  ‚îÇ Partition 0 ‚îÇ ‚îÇ Partition 1 ‚îÇ ‚îÇ Partition 2 ‚îÇ    ‚îÇ\n",
        "‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ\n",
        "‚îÇ  ‚îÇJuan‚îÇ25‚îÇMad. ‚îÇ ‚îÇAna‚îÇ30‚îÇBarc. ‚îÇ ‚îÇLuis‚îÇ28‚îÇVale‚îÇ‚îÇ    ‚îÇ\n",
        "‚îÇ  ‚îÇMar√≠a‚îÇ32‚îÇSev.‚îÇ ‚îÇPedro‚îÇ27‚îÇBil.‚îÇ ‚îÇLaura‚îÇ35‚îÇMal‚îÇ‚îÇ    ‚îÇ\n",
        "‚îÇ  ‚îÇCarlos‚îÇ29‚îÇM. ‚îÇ ‚îÇSof√≠a‚îÇ26‚îÇMad‚îÇ‚îÇ ‚îÇMiguel‚îÇ31‚îÇB.‚îÇ‚îÇ    ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚úÖ Spark sabe: 3 columnas, tipos de datos, 3 particiones\n",
        "‚úÖ Puede optimizar operaciones bas√°ndose en esta info\n",
        "```\n",
        "\n",
        "#### **Crear DataFrames**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "FORMAS DE CREAR DATAFRAMES EN DATABRICKS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìù CREAR DATAFRAMES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== M√âTODO 1: Desde una lista de tuplas =====\n",
        "print(\"\\n1Ô∏è‚É£ Desde lista de tuplas:\")\n",
        "\n",
        "codigo_1 = \"\"\"\n",
        "# Datos\n",
        "datos = [\n",
        "    (\"Juan\", 25, \"Madrid\"),\n",
        "    (\"Mar√≠a\", 32, \"Sevilla\"),\n",
        "    (\"Carlos\", 29, \"Madrid\")\n",
        "]\n",
        "\n",
        "# Crear DataFrame con esquema inferido\n",
        "df = spark.createDataFrame(datos, [\"nombre\", \"edad\", \"ciudad\"])\n",
        "\n",
        "# Ver resultado\n",
        "df.show()\n",
        "\n",
        "# Output:\n",
        "# +------+----+-------+\n",
        "# |nombre|edad| ciudad|\n",
        "# +------+----+-------+\n",
        "# |  Juan|  25| Madrid|\n",
        "# | Mar√≠a|  32|Sevilla|\n",
        "# |Carlos|  29| Madrid|\n",
        "# +------+----+-------+\n",
        "\"\"\"\n",
        "print(codigo_1)\n",
        "\n",
        "# ===== M√âTODO 2: Desde lista de diccionarios =====\n",
        "print(\"\\n2Ô∏è‚É£ Desde lista de diccionarios:\")\n",
        "\n",
        "codigo_2 = \"\"\"\n",
        "# M√°s expl√≠cito y legible\n",
        "datos = [\n",
        "    {\"nombre\": \"Juan\", \"edad\": 25, \"ciudad\": \"Madrid\"},\n",
        "    {\"nombre\": \"Mar√≠a\", \"edad\": 32, \"ciudad\": \"Sevilla\"},\n",
        "    {\"nombre\": \"Carlos\", \"edad\": 29, \"ciudad\": \"Madrid\"}\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(datos)\n",
        "df.show()\n",
        "\"\"\"\n",
        "print(codigo_2)\n",
        "\n",
        "# ===== M√âTODO 3: Desde archivo CSV =====\n",
        "print(\"\\n3Ô∏è‚É£ Desde archivo CSV:\")\n",
        "\n",
        "codigo_3 = \"\"\"\n",
        "# Leer CSV con opciones\n",
        "df = spark.read \\\\\n",
        "    .format(\"csv\") \\\\\n",
        "    .option(\"header\", \"true\") \\\\\n",
        "    .option(\"inferSchema\", \"true\") \\\\\n",
        "    .load(\"/data/clientes.csv\")\n",
        "\n",
        "# O m√°s corto\n",
        "df = spark.read.csv(\"/data/clientes.csv\", header=True, inferSchema=True)\n",
        "\"\"\"\n",
        "print(codigo_3)\n",
        "\n",
        "# ===== M√âTODO 4: Desde archivo Parquet =====\n",
        "print(\"\\n4Ô∏è‚É£ Desde Parquet (formato columnar optimizado):\")\n",
        "\n",
        "codigo_4 = \"\"\"\n",
        "df = spark.read.parquet(\"/data/ventas.parquet\")\n",
        "\n",
        "# Parquet ya incluye el esquema, no necesitas inferirlo\n",
        "\"\"\"\n",
        "print(codigo_4)\n",
        "\n",
        "# ===== M√âTODO 5: Desde Delta Lake =====\n",
        "print(\"\\n5Ô∏è‚É£ Desde Delta Lake (RECOMENDADO en Databricks):\")\n",
        "\n",
        "codigo_5 = \"\"\"\n",
        "# Leer tabla Delta\n",
        "df = spark.read.format(\"delta\").load(\"/delta/clientes\")\n",
        "\n",
        "# O m√°s directo\n",
        "df = spark.read.table(\"clientes\")  # Si est√° en el cat√°logo\n",
        "\n",
        "# O con SQL\n",
        "df = spark.sql(\"SELECT * FROM clientes\")\n",
        "\"\"\"\n",
        "print(codigo_5)\n",
        "\n",
        "# ===== M√âTODO 6: Desde JSON =====\n",
        "print(\"\\n6Ô∏è‚É£ Desde JSON:\")\n",
        "\n",
        "codigo_6 = \"\"\"\n",
        "df = spark.read.json(\"/data/logs/*.json\")\n",
        "\n",
        "# Spark infiere esquema autom√°ticamente de JSON\n",
        "\"\"\"\n",
        "print(codigo_6)\n",
        "\n",
        "# ===== M√âTODO 7: Con esquema expl√≠cito =====\n",
        "print(\"\\n7Ô∏è‚É£ Con esquema expl√≠cito (mejor pr√°ctica producci√≥n):\")\n",
        "\n",
        "codigo_7 = \"\"\"\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Definir esquema\n",
        "esquema = StructType([\n",
        "    StructField(\"nombre\", StringType(), nullable=False),\n",
        "    StructField(\"edad\", IntegerType(), nullable=False),\n",
        "    StructField(\"ciudad\", StringType(), nullable=True)\n",
        "])\n",
        "\n",
        "# Crear DataFrame con esquema\n",
        "df = spark.createDataFrame(datos, schema=esquema)\n",
        "\n",
        "# Ventaja: Control total, sin inferencia (m√°s r√°pido en producci√≥n)\n",
        "\"\"\"\n",
        "print(codigo_7)\n",
        "```\n",
        "\n",
        "#### **Operaciones con DataFrames**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "OPERACIONES COMUNES CON DATAFRAMES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîß OPERACIONES CON DATAFRAMES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== SELECCI√ìN =====\n",
        "print(\"\\n1Ô∏è‚É£ SELECCI√ìN DE COLUMNAS:\")\n",
        "\n",
        "seleccion = \"\"\"\n",
        "# Seleccionar columnas\n",
        "df.select(\"nombre\", \"edad\")\n",
        "\n",
        "# Con expresiones\n",
        "from pyspark.sql.functions import col, upper\n",
        "\n",
        "df.select(\n",
        "    col(\"nombre\"),\n",
        "    upper(col(\"ciudad\")).alias(\"ciudad_upper\")\n",
        ")\n",
        "\n",
        "# Equivalente SQL\n",
        "df.selectExpr(\"nombre\", \"UPPER(ciudad) as ciudad_upper\")\n",
        "\"\"\"\n",
        "print(seleccion)\n",
        "\n",
        "# ===== FILTRADO =====\n",
        "print(\"\\n2Ô∏è‚É£ FILTRADO:\")\n",
        "\n",
        "filtrado = \"\"\"\n",
        "# M√©todo 1: Con objetos Column\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.filter(col(\"edad\") > 25)\n",
        "df.filter((col(\"edad\") > 25) & (col(\"ciudad\") == \"Madrid\"))\n",
        "\n",
        "# M√©todo 2: Con strings SQL (m√°s legible)\n",
        "df.filter(\"edad > 25\")\n",
        "df.filter(\"edad > 25 AND ciudad = 'Madrid'\")\n",
        "\n",
        "# where() es sin√≥nimo de filter()\n",
        "df.where(\"edad > 25\")\n",
        "\"\"\"\n",
        "print(filtrado)\n",
        "\n",
        "# ===== AGREGAR COLUMNAS =====\n",
        "print(\"\\n3Ô∏è‚É£ AGREGAR/MODIFICAR COLUMNAS:\")\n",
        "\n",
        "agregar = \"\"\"\n",
        "from pyspark.sql.functions import col, lit, when\n",
        "\n",
        "# Agregar columna nueva\n",
        "df.withColumn(\"edad_en_meses\", col(\"edad\") * 12)\n",
        "\n",
        "# Modificar columna existente\n",
        "df.withColumn(\"ciudad\", upper(col(\"ciudad\")))\n",
        "\n",
        "# Columna con l√≥gica condicional\n",
        "df.withColumn(\"categoria\",\n",
        "    when(col(\"edad\") < 18, \"Menor\")\n",
        "    .when(col(\"edad\") < 65, \"Adulto\")\n",
        "    .otherwise(\"Senior\")\n",
        ")\n",
        "\n",
        "# Agregar columna literal\n",
        "df.withColumn(\"pais\", lit(\"Espa√±a\"))\n",
        "\"\"\"\n",
        "print(agregar)\n",
        "\n",
        "# ===== AGREGACIONES =====\n",
        "print(\"\\n4Ô∏è‚É£ AGREGACIONES:\")\n",
        "\n",
        "agregaciones = \"\"\"\n",
        "from pyspark.sql.functions import avg, max, min, count, sum\n",
        "\n",
        "# Agregaci√≥n simple\n",
        "df.agg(\n",
        "    avg(\"edad\").alias(\"edad_promedio\"),\n",
        "    max(\"edad\").alias(\"edad_maxima\"),\n",
        "    count(\"*\").alias(\"total\")\n",
        ")\n",
        "\n",
        "# GroupBy\n",
        "df.groupBy(\"ciudad\").agg(\n",
        "    count(\"*\").alias(\"num_personas\"),\n",
        "    avg(\"edad\").alias(\"edad_promedio\")\n",
        ")\n",
        "\n",
        "# Equivalente SQL\n",
        "df.groupBy(\"ciudad\").count()\n",
        "\"\"\"\n",
        "print(agregaciones)\n",
        "\n",
        "# ===== JOINS =====\n",
        "print(\"\\n5Ô∏è‚É£ JOINS:\")\n",
        "\n",
        "joins = \"\"\"\n",
        "# Inner join (por defecto)\n",
        "df1.join(df2, df1.id == df2.id)\n",
        "df1.join(df2, \"id\")  # Si la columna tiene el mismo nombre\n",
        "\n",
        "# Left join\n",
        "df1.join(df2, \"id\", \"left\")\n",
        "\n",
        "# Tipos de join: inner, left, right, outer, cross, semi, anti\n",
        "\"\"\"\n",
        "print(joins)\n",
        "\n",
        "# ===== ORDENAR =====\n",
        "print(\"\\n6Ô∏è‚É£ ORDENAR:\")\n",
        "\n",
        "ordenar = \"\"\"\n",
        "from pyspark.sql.functions import col, desc\n",
        "\n",
        "# Ascendente\n",
        "df.orderBy(\"edad\")\n",
        "df.orderBy(col(\"edad\").asc())\n",
        "\n",
        "# Descendente\n",
        "df.orderBy(col(\"edad\").desc())\n",
        "\n",
        "# M√∫ltiples columnas\n",
        "df.orderBy(\"ciudad\", col(\"edad\").desc())\n",
        "\"\"\"\n",
        "print(ordenar)\n",
        "```\n",
        "\n",
        "#### **Ejemplo Completo: Word Count con DataFrame**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "WORD COUNT CON DATAFRAME (vs RDD anterior)\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìù WORD COUNT CON DATAFRAME\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "word_count_df = \"\"\"\n",
        "from pyspark.sql.functions import explode, split, col\n",
        "\n",
        "# Paso 1: Leer archivo\n",
        "df_texto = spark.read.text(\"/data/libros/*.txt\")\n",
        "# Schema: [value: string] - una columna llamada \"value\"\n",
        "\n",
        "# Paso 2: Dividir en palabras\n",
        "df_palabras = df_texto.select(\n",
        "    explode(split(col(\"value\"), \" \")).alias(\"palabra\")\n",
        ")\n",
        "# explode() convierte array en filas\n",
        "# split() divide string en array\n",
        "\n",
        "# Paso 3: Agrupar y contar\n",
        "df_conteo = df_palabras.groupBy(\"palabra\").count()\n",
        "\n",
        "# Paso 4: Ordenar\n",
        "df_ordenado = df_conteo.orderBy(col(\"count\").desc())\n",
        "\n",
        "# Paso 5: Mostrar top 10\n",
        "df_ordenado.show(10)\n",
        "\n",
        "# Output:\n",
        "# +--------+-----+\n",
        "# | palabra|count|\n",
        "# +--------+-----+\n",
        "# |     the| 5420|\n",
        "# |      of| 3890|\n",
        "# |     and| 3120|\n",
        "# |      to| 2890|\n",
        "# |       a| 2560|\n",
        "# |      in| 2340|\n",
        "# |      is| 1980|\n",
        "# |    that| 1870|\n",
        "# |      it| 1690|\n",
        "# |     for| 1560|\n",
        "# +--------+-----+\n",
        "\n",
        "‚úÖ Mucho m√°s legible que RDD\n",
        "‚úÖ Optimizado autom√°ticamente por Catalyst\n",
        "‚úÖ M√°s r√°pido en ejecuci√≥n\n",
        "\"\"\"\n",
        "\n",
        "print(word_count_df)\n",
        "```\n",
        "\n",
        "#### **DataFrame vs RDD: Comparaci√≥n Directa**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "COMPARACI√ìN LADO A LADO\n",
        "\"\"\"\n",
        "\n",
        "comparacion = \"\"\"\n",
        "TAREA: Filtrar personas mayores de 25 a√±os en Madrid y contar\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "RDD (C√≥digo complejo, sin optimizaci√≥n)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "rdd = sc.parallelize([\n",
        "    {\"nombre\": \"Juan\", \"edad\": 30, \"ciudad\": \"Madrid\"},\n",
        "    {\"nombre\": \"Mar√≠a\", \"edad\": 22, \"ciudad\": \"Madrid\"},\n",
        "    {\"nombre\": \"Carlos\", \"edad\": 28, \"ciudad\": \"Barcelona\"}\n",
        "])\n",
        "\n",
        "resultado = rdd \\\\\n",
        "    .filter(lambda x: x[\"edad\"] > 25) \\\\\n",
        "    .filter(lambda x: x[\"ciudad\"] == \"Madrid\") \\\\\n",
        "    .count()\n",
        "\n",
        "print(resultado)  # 1\n",
        "\n",
        "‚ùå Problemas:\n",
        "   - Lambdas complejas\n",
        "   - Sin optimizaci√≥n autom√°tica\n",
        "   - Ejecuta 2 filter por separado\n",
        "   - Sin pushdown de predicados\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "DATAFRAME (C√≥digo simple, optimizado)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    {\"nombre\": \"Juan\", \"edad\": 30, \"ciudad\": \"Madrid\"},\n",
        "    {\"nombre\": \"Mar√≠a\", \"edad\": 22, \"ciudad\": \"Madrid\"},\n",
        "    {\"nombre\": \"Carlos\", \"edad\": 28, \"ciudad\": \"Barcelona\"}\n",
        "])\n",
        "\n",
        "# Opci√≥n 1: API DataFrame\n",
        "resultado = df \\\\\n",
        "    .filter((col(\"edad\") > 25) & (col(\"ciudad\") == \"Madrid\")) \\\\\n",
        "    .count()\n",
        "\n",
        "# Opci√≥n 2: SQL puro (a√∫n m√°s simple)\n",
        "df.createOrReplaceTempView(\"personas\")\n",
        "resultado = spark.sql('''\n",
        "    SELECT COUNT(*)\n",
        "    FROM personas\n",
        "    WHERE edad > 25 AND ciudad = 'Madrid'\n",
        "''').collect()[0][0]\n",
        "\n",
        "print(resultado)  # 1\n",
        "\n",
        "‚úÖ Ventajas:\n",
        "   - C√≥digo m√°s legible\n",
        "   - Catalyst combina ambos filtros en uno\n",
        "   - Predicate pushdown (filtra al leer)\n",
        "   - 3-10x m√°s r√°pido en ejecuci√≥n\n",
        "\"\"\"\n",
        "\n",
        "print(comparacion)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 3: Dataset - La Abstracci√≥n Type-Safe (Solo Scala/Java)**\n",
        "\n",
        "#### **¬øQu√© es un Dataset?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DATASET = DataFrame + Type Safety\n",
        "\"\"\"\n",
        "\n",
        "dataset_explicacion = \"\"\"\n",
        "üì¶ DATASET (Solo Scala/Java)\n",
        "\n",
        "Dataset combina:\n",
        "‚úÖ Optimizaci√≥n de DataFrame (Catalyst)\n",
        "‚úÖ Type-safety de RDD (errores en compilaci√≥n, no runtime)\n",
        "\n",
        "Ejemplo en Scala:\n",
        "\n",
        "// Definir case class (tipo)\n",
        "case class Persona(nombre: String, edad: Int, ciudad: String)\n",
        "\n",
        "// Crear Dataset tipado\n",
        "val ds: Dataset[Persona] = spark.read.json(\"/data\").as[Persona]\n",
        "\n",
        "// Operations type-safe\n",
        "ds.filter(_.edad > 25)  // Compilador sabe que 'edad' existe\n",
        "ds.map(_.nombre)        // Compilador sabe que retorna String\n",
        "\n",
        "// Error en compilaci√≥n (no en runtime)\n",
        "ds.filter(_.edad_invalida > 25)  // ‚ùå Error: 'edad_invalida' no existe\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "‚ö†Ô∏è EN PYTHON NO HAY DATASETS\n",
        "   Python es din√°micamente tipado, no hay type-safety en compilaci√≥n\n",
        "   En PySpark, siempre trabajas con DataFrames\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "CUANDO USAR DATASET (Scala):\n",
        "‚úÖ Aplicaciones cr√≠ticas que necesitan type-safety\n",
        "‚úÖ Grandes equipos (catches errores temprano)\n",
        "‚úÖ APIs complejas (el tipo documenta el c√≥digo)\n",
        "\n",
        "CUANDO USAR DATAFRAME:\n",
        "‚úÖ Python (√∫nica opci√≥n)\n",
        "‚úÖ SQL (m√°s simple)\n",
        "‚úÖ An√°lisis exploratorio\n",
        "‚úÖ BI y reporting\n",
        "‚úÖ 95% de los casos\n",
        "\"\"\"\n",
        "\n",
        "print(dataset_explicacion)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparaci√≥n Completa: RDD vs DataFrame vs Dataset**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "COMPARACI√ìN EXHAUSTIVA\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "comparacion_tabla = pd.DataFrame({\n",
        "    'Caracter√≠stica': [\n",
        "        'Lenguajes',\n",
        "        'Type-safe',\n",
        "        'Optimizaci√≥n',\n",
        "        'API',\n",
        "        'Performance',\n",
        "        'Uso en 2025',\n",
        "        'Curva aprendizaje',\n",
        "        'Errores detectados',\n",
        "        'Spark UI'\n",
        "    ],\n",
        "    'RDD': [\n",
        "        'Python, Scala, Java',\n",
        "        '‚ùå No',\n",
        "        '‚ùå Manual',\n",
        "        'Bajo nivel (lambdas)',\n",
        "        '‚≠ê‚≠ê Lento',\n",
        "        '1% (legacy)',\n",
        "        'üî¥ Alta',\n",
        "        'Runtime',\n",
        "        'Limitada'\n",
        "    ],\n",
        "    'DataFrame': [\n",
        "        'Python, Scala, Java, R, SQL',\n",
        "        '‚ùå No',\n",
        "        '‚úÖ Catalyst + Tungsten',\n",
        "        'Alto nivel (SQL-like)',\n",
        "        '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê R√°pido',\n",
        "        '95% (est√°ndar)',\n",
        "        'üü¢ Baja',\n",
        "        'Runtime',\n",
        "        'Excelente'\n",
        "    ],\n",
        "    'Dataset': [\n",
        "        'Solo Scala, Java',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ Catalyst + Tungsten',\n",
        "        'Alto nivel + tipos',\n",
        "        '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê R√°pido',\n",
        "        '4% (Scala apps)',\n",
        "        'üü° Media',\n",
        "        'Compile-time',\n",
        "        'Excelente'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nüìä COMPARACI√ìN COMPLETA\")\n",
        "print(\"=\"*60)\n",
        "print(comparacion_tabla.to_string(index=False))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Trabajando con DataFrames**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "EJERCICIO PR√ÅCTICO: CREAR Y MANIPULAR DATAFRAMES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüß™ PR√ÅCTICA: OPERACIONES CON DATAFRAMES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== CREAR DATASET DE EJEMPLO =====\n",
        "print(\"\\n1Ô∏è‚É£ Crear DataFrame de ejemplo:\")\n",
        "\n",
        "# Simular datos de una tienda online\n",
        "datos_ventas = [\n",
        "    {\"producto\": \"Laptop\", \"precio\": 1000, \"cantidad\": 2, \"ciudad\": \"Madrid\"},\n",
        "    {\"producto\": \"Mouse\", \"precio\": 20, \"cantidad\": 5, \"ciudad\": \"Barcelona\"},\n",
        "    {\"producto\": \"Teclado\", \"precio\": 50, \"cantidad\": 3, \"ciudad\": \"Madrid\"},\n",
        "    {\"producto\": \"Monitor\", \"precio\": 300, \"cantidad\": 1, \"ciudad\": \"Valencia\"},\n",
        "    {\"producto\": \"Laptop\", \"precio\": 1000, \"cantidad\": 1, \"ciudad\": \"Barcelona\"},\n",
        "    {\"producto\": \"Mouse\", \"precio\": 20, \"cantidad\": 10, \"ciudad\": \"Madrid\"},\n",
        "]\n",
        "\n",
        "# En Databricks, har√≠as:\n",
        "# df_ventas = spark.createDataFrame(datos_ventas)\n",
        "# df_ventas.show()\n",
        "\n",
        "print(\"DataFrame creado con ventas de productos\")\n",
        "\n",
        "# ===== OPERACIONES COMUNES =====\n",
        "print(\"\\n2Ô∏è‚É£ Operaciones que practicar√≠as:\")\n",
        "\n",
        "ejercicios = \"\"\"\n",
        "# A. Calcular total de venta (precio * cantidad)\n",
        "df_ventas = df_ventas.withColumn(\"total\", col(\"precio\") * col(\"cantidad\"))\n",
        "\n",
        "# B. Ventas por ciudad\n",
        "df_por_ciudad = df_ventas.groupBy(\"ciudad\").agg(\n",
        "    sum(\"total\").alias(\"total_ventas\"),\n",
        "    count(\"*\").alias(\"num_transacciones\")\n",
        ")\n",
        "\n",
        "# C. Top 3 productos por ingresos\n",
        "df_por_producto = df_ventas.groupBy(\"producto\").agg(\n",
        "    sum(\"total\").alias(\"ingresos_totales\")\n",
        ").orderBy(col(\"ingresos_totales\").desc()).limit(3)\n",
        "\n",
        "# D. Filtrar ventas > 100‚Ç¨\n",
        "df_ventas_grandes = df_ventas.filter(col(\"total\") > 100)\n",
        "\n",
        "# E. Agregar categor√≠a de venta\n",
        "df_ventas = df_ventas.withColumn(\"categoria\",\n",
        "    when(col(\"total\") < 50, \"Peque√±a\")\n",
        "    .when(col(\"total\") < 500, \"Mediana\")\n",
        "    .otherwise(\"Grande\")\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "print(ejercicios)\n",
        "\n",
        "# ===== RESULTADOS ESPERADOS =====\n",
        "print(\"\\n3Ô∏è‚É£ Resultados esperados:\")\n",
        "\n",
        "resultados = \"\"\"\n",
        "A. Con columna 'total':\n",
        "+-------+------+--------+---------+-----+\n",
        "|producto|precio|cantidad|   ciudad|total|\n",
        "+--------+------+--------+---------+-----+\n",
        "|  Laptop| 1000|       2|   Madrid| 2000|\n",
        "|   Mouse|   20|       5|Barcelona|  100|\n",
        "|Teclado |   50|       3|   Madrid|  150|\n",
        "| Monitor|  300|       1| Valencia|  300|\n",
        "|  Laptop| 1000|       1|Barcelona| 1000|\n",
        "|   Mouse|   20|      10|   Madrid|  200|\n",
        "+--------+------+--------+---------+-----+\n",
        "\n",
        "B. Ventas por ciudad:\n",
        "+---------+------------+-----------------+\n",
        "|   ciudad|total_ventas|num_transacciones|\n",
        "+---------+------------+-----------------+\n",
        "|   Madrid|        2350|                3|\n",
        "|Barcelona|        1100|                2|\n",
        "| Valencia|         300|                1|\n",
        "+---------+------------+-----------------+\n",
        "\n",
        "C. Top 3 productos:\n",
        "+--------+----------------+\n",
        "|producto|ingresos_totales|\n",
        "+--------+----------------+\n",
        "|  Laptop|            3000|\n",
        "|   Mouse|             300|\n",
        "| Monitor|             300|\n",
        "+--------+----------------+\n",
        "\"\"\"\n",
        "\n",
        "print(resultados)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 2.2 - Conversi√≥n RDD ‚Üî DataFrame**\n",
        "\n",
        "```markdown\n",
        "### Ejercicio: Entiende las diferencias\n",
        "\n",
        "**Tarea**: Dado el siguiente c√≥digo RDD, reescr√≠belo usando DataFrame\n",
        "\n",
        "```python\n",
        "# C√ìDIGO RDD (proporcionado)\n",
        "rdd = sc.parallelize([\n",
        "    (\"Juan\", 1000),\n",
        "    (\"Mar√≠a\", 1500),\n",
        "    (\"Juan\", 500),\n",
        "    (\"Pedro\", 2000),\n",
        "    (\"Mar√≠a\", 800)\n",
        "])\n",
        "\n",
        "# Calcular total por persona\n",
        "resultado_rdd = rdd.reduceByKey(lambda a, b: a + b)\n",
        "top_3 = resultado_rdd.takeOrdered(3, key=lambda x: -x[1])\n",
        "```\n",
        "\n",
        "**Tu soluci√≥n con DataFrame**:\n",
        "```python\n",
        "# Completar aqu√≠\n",
        "datos = [\n",
        "    # ...\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(...)\n",
        "\n",
        "# ...\n",
        "```\n",
        "\n",
        "**Preguntas**:\n",
        "1. ¬øCu√°l es m√°s legible? ¬øPor qu√©?\n",
        "2. ¬øCu√°l ser√° m√°s r√°pido? ¬øPor qu√©?\n",
        "3. ¬øCu√°ndo usar√≠as RDD sobre DataFrame?\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **RDD** = Bajo nivel, control total, sin optimizaci√≥n (legacy)\n",
        "2. **DataFrame** = Alto nivel, optimizado, 95% de casos de uso\n",
        "3. **Dataset** = Type-safe, solo Scala/Java, para apps empresariales\n",
        "4. **En Databricks/PySpark ‚Üí SIEMPRE usa DataFrames**\n",
        "5. **DataFrame tiene esquema** ‚Üí Spark optimiza autom√°ticamente\n",
        "6. **Catalyst Optimizer** hace DataFrames 3-10x m√°s r√°pidos que RDDs\n",
        "7. **API DataFrame** es intuitiva (similar a SQL y Pandas)\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **Nunca uses RDD en c√≥digo nuevo**\n",
        "   - Solo si mantienes c√≥digo legacy\n",
        "   - DataFrames son superiores en todo\n",
        "\n",
        "‚úÖ **Aprende SQL si usas DataFrames**\n",
        "   - Muchas operaciones son m√°s claras en SQL\n",
        "   - `df.filter(\"edad > 25\")` vs `df.filter(col(\"edad\") > 25)`\n",
        "\n",
        "‚úÖ **Usa .explain() para entender planes**\n",
        "   - `df.explain()` muestra c√≥mo Spark ejecutar√° tu query\n",
        "   - √ötil para debugging y optimizaci√≥n\n",
        "\n",
        "‚úÖ **Convierte RDD ‚Üí DataFrame si es necesario**\n",
        "   - `df = rdd.toDF([\"col1\", \"col2\"])`\n",
        "   - Ganas optimizaci√≥n inmediata\n",
        "\n",
        "‚úÖ **DataFrames = SQL distribuido**\n",
        "   - Si sabes SQL, ya sabes DataFrames\n",
        "   - Todo lo de SQL funciona en DataFrames\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el siguiente punto **2.3 Transformaciones y acciones en Spark (lazy evaluation)**?"
      ],
      "metadata": {
        "id": "NNKepX1PYBPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3 Transformaciones y acciones en Spark (lazy evaluation)**\n",
        "\n",
        "#### **Introducci√≥n: El Secreto de la Velocidad de Spark**\n",
        "\n",
        "Uno de los conceptos m√°s importantes (y a veces confuso) de Spark es la **lazy evaluation** (evaluaci√≥n perezosa). Entender esto es crucial para escribir c√≥digo Spark eficiente.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "LA PARADOJA DE SPARK\n",
        "\"\"\"\n",
        "\n",
        "paradoja = \"\"\"\n",
        "ü§î PARADOJA:\n",
        "\n",
        "# Escribes este c√≥digo:\n",
        "df = spark.read.parquet(\"/data/100GB.parquet\")  # ¬øSe leen 100 GB?\n",
        "df_filtrado = df.filter(\"edad > 25\")             # ¬øSe filtra ahora?\n",
        "df_seleccionado = df_filtrado.select(\"nombre\")   # ¬øSe selecciona ahora?\n",
        "\n",
        "print(\"¬øCu√°nto tiempo tard√≥?\")\n",
        "# Respuesta: ¬°Milisegundos! üò≤\n",
        "\n",
        "# Pero... ¬øc√≥mo puede procesar 100 GB en milisegundos?\n",
        "\n",
        "üéØ RESPUESTA: ¬°NO LO HIZO!\n",
        "\n",
        "Spark solo \"registr√≥\" qu√© quieres hacer, pero NO ejecut√≥ nada.\n",
        "Solo cuando ejecutas una ACCI√ìN, Spark realmente trabaja.\n",
        "\n",
        "Este concepto se llama LAZY EVALUATION\n",
        "\"\"\"\n",
        "\n",
        "print(paradoja)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 1: Transformaciones (Lazy)**\n",
        "\n",
        "#### **¬øQu√© son las Transformaciones?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "TRANSFORMACIONES = Operaciones que crean un nuevo DataFrame\n",
        "\"\"\"\n",
        "\n",
        "transformaciones_definicion = {\n",
        "    'concepto': 'Operaciones que definen C√ìMO transformar los datos',\n",
        "    'ejecuci√≥n': '‚ùå NO se ejecutan inmediatamente (lazy)',\n",
        "    'retorno': 'Nuevo DataFrame (inmutable)',\n",
        "    'prop√≥sito': 'Construir un plan de ejecuci√≥n',\n",
        "    'ejemplos': ['select', 'filter', 'groupBy', 'join', 'orderBy', 'withColumn']\n",
        "}\n",
        "\n",
        "print(\"üîÑ TRANSFORMACIONES (LAZY)\")\n",
        "print(\"=\"*60)\n",
        "for key, value in transformaciones_definicion.items():\n",
        "    print(f\"{key.capitalize()}: {value}\")\n",
        "```\n",
        "\n",
        "#### **Categor√≠as de Transformaciones**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "TIPOS DE TRANSFORMACIONES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìö CATEGOR√çAS DE TRANSFORMACIONES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== 1. NARROW TRANSFORMATIONS (Sin Shuffle) =====\n",
        "print(\"\\n1Ô∏è‚É£ NARROW TRANSFORMATIONS (R√°pidas, sin shuffle):\")\n",
        "\n",
        "narrow = \"\"\"\n",
        "NARROW = Cada partici√≥n de entrada genera UNA partici√≥n de salida\n",
        "         No requiere movimiento de datos entre workers\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Partition 0 ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ Partition 0'‚îÇ\n",
        "‚îÇ  [1,2,3]    ‚îÇ         ‚îÇ  [2,4,6]    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Partition 1 ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ Partition 1'‚îÇ\n",
        "‚îÇ  [4,5,6]    ‚îÇ         ‚îÇ  [8,10,12]  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚úÖ Cada worker procesa sus datos independientemente\n",
        "‚úÖ No hay comunicaci√≥n entre workers\n",
        "‚úÖ Muy eficiente\n",
        "\n",
        "EJEMPLOS:\n",
        "\"\"\"\n",
        "\n",
        "ejemplos_narrow = \"\"\"\n",
        "# select() - Seleccionar columnas\n",
        "df.select(\"nombre\", \"edad\")\n",
        "\n",
        "# filter() / where() - Filtrar filas\n",
        "df.filter(col(\"edad\") > 25)\n",
        "df.where(\"ciudad = 'Madrid'\")\n",
        "\n",
        "# withColumn() - Agregar/modificar columna\n",
        "df.withColumn(\"edad_doble\", col(\"edad\") * 2)\n",
        "\n",
        "# drop() - Eliminar columnas\n",
        "df.drop(\"columna_innecesaria\")\n",
        "\n",
        "# withColumnRenamed() - Renombrar\n",
        "df.withColumnRenamed(\"edad\", \"age\")\n",
        "\n",
        "# map() - Transformaci√≥n fila por fila (RDD-style)\n",
        "df.rdd.map(lambda row: row.edad * 2)\n",
        "\n",
        "# flatMap() - Similar a map pero retorna m√∫ltiples valores\n",
        "df.select(explode(split(\"texto\", \" \")))\n",
        "\n",
        "# union() - Unir DataFrames (mismo esquema)\n",
        "df1.union(df2)\n",
        "\n",
        "# sample() - Muestreo\n",
        "df.sample(fraction=0.1)\n",
        "\"\"\"\n",
        "\n",
        "print(narrow)\n",
        "print(ejemplos_narrow)\n",
        "\n",
        "# ===== 2. WIDE TRANSFORMATIONS (Con Shuffle) =====\n",
        "print(\"\\n2Ô∏è‚É£ WIDE TRANSFORMATIONS (Lentas, requieren shuffle):\")\n",
        "\n",
        "wide = \"\"\"\n",
        "WIDE = M√∫ltiples particiones de entrada pueden contribuir a\n",
        "       UNA partici√≥n de salida. Requiere SHUFFLE.\n",
        "\n",
        "ANTES DEL SHUFFLE:\n",
        "Worker 1: [(\"Madrid\", 1), (\"Barcelona\", 2)]\n",
        "Worker 2: [(\"Madrid\", 3), (\"Valencia\", 4)]\n",
        "\n",
        "SHUFFLE (reagrupar por ciudad):\n",
        "\n",
        "DESPU√âS DEL SHUFFLE:\n",
        "Worker 1: [(\"Madrid\", 1), (\"Madrid\", 3)]      ‚Üê De workers 1 y 2\n",
        "Worker 2: [(\"Barcelona\", 2), (\"Valencia\", 4)]\n",
        "\n",
        "‚ö†Ô∏è Datos se mueven por la red entre workers\n",
        "‚ö†Ô∏è Operaci√≥n costosa pero a veces necesaria\n",
        "\n",
        "EJEMPLOS:\n",
        "\"\"\"\n",
        "\n",
        "ejemplos_wide = \"\"\"\n",
        "# groupBy() - Agrupar\n",
        "df.groupBy(\"ciudad\").count()\n",
        "df.groupBy(\"ciudad\").agg(avg(\"edad\"))\n",
        "\n",
        "# join() - Unir DataFrames\n",
        "df1.join(df2, \"id\")\n",
        "df1.join(df2, df1.id == df2.customer_id, \"left\")\n",
        "\n",
        "# distinct() - Eliminar duplicados\n",
        "df.distinct()\n",
        "df.dropDuplicates([\"nombre\", \"edad\"])\n",
        "\n",
        "# orderBy() / sort() - Ordenar globalmente\n",
        "df.orderBy(\"edad\")\n",
        "df.sort(col(\"salario\").desc())\n",
        "\n",
        "# repartition() - Cambiar n√∫mero de particiones\n",
        "df.repartition(100)\n",
        "df.repartition(\"ciudad\")  # Particionar por columna\n",
        "\n",
        "# coalesce() - Reducir particiones (sin shuffle completo)\n",
        "df.coalesce(10)  # M√°s eficiente que repartition para reducir\n",
        "\n",
        "# intersect() - Intersecci√≥n de DataFrames\n",
        "df1.intersect(df2)\n",
        "\n",
        "# subtract() - Diferencia\n",
        "df1.subtract(df2)\n",
        "\"\"\"\n",
        "\n",
        "print(wide)\n",
        "print(ejemplos_wide)\n",
        "```\n",
        "\n",
        "#### **Transformaciones m√°s Usadas**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "TOP 10 TRANSFORMACIONES M√ÅS UTILIZADAS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n‚≠ê TOP 10 TRANSFORMACIONES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "top_transformaciones = \"\"\"\n",
        "1. select() - Seleccionar columnas\n",
        "   df.select(\"nombre\", \"edad\")\n",
        "   df.select(col(\"nombre\"), (col(\"edad\") * 2).alias(\"edad_doble\"))\n",
        "\n",
        "2. filter() / where() - Filtrar filas\n",
        "   df.filter(col(\"edad\") > 25)\n",
        "   df.where(\"edad > 25 AND ciudad = 'Madrid'\")\n",
        "\n",
        "3. withColumn() - Agregar/modificar columna\n",
        "   df.withColumn(\"categoria\",\n",
        "       when(col(\"edad\") < 18, \"Menor\")\n",
        "       .when(col(\"edad\") < 65, \"Adulto\")\n",
        "       .otherwise(\"Senior\"))\n",
        "\n",
        "4. groupBy() + agg() - Agregaciones\n",
        "   df.groupBy(\"ciudad\").agg(\n",
        "       count(\"*\").alias(\"total\"),\n",
        "       avg(\"edad\").alias(\"edad_promedio\"),\n",
        "       max(\"salario\").alias(\"salario_max\")\n",
        "   )\n",
        "\n",
        "5. join() - Combinar DataFrames\n",
        "   df_empleados.join(df_departamentos, \"dept_id\", \"left\")\n",
        "\n",
        "6. orderBy() / sort() - Ordenar\n",
        "   df.orderBy(col(\"salario\").desc(), \"nombre\")\n",
        "\n",
        "7. drop() - Eliminar columnas\n",
        "   df.drop(\"columna_temporal\", \"otra_columna\")\n",
        "\n",
        "8. dropDuplicates() - Eliminar duplicados\n",
        "   df.dropDuplicates()  # Todas las columnas\n",
        "   df.dropDuplicates([\"email\"])  # Por columnas espec√≠ficas\n",
        "\n",
        "9. union() / unionByName() - Combinar DataFrames verticalmente\n",
        "   df1.union(df2)  # Mismo orden de columnas\n",
        "   df1.unionByName(df2)  # Por nombre de columna (m√°s seguro)\n",
        "\n",
        "10. explode() - Convertir arrays/maps en filas\n",
        "    df.select(\"id\", explode(\"lista_items\").alias(\"item\"))\n",
        "\"\"\"\n",
        "\n",
        "print(top_transformaciones)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 2: Acciones (Eager)**\n",
        "\n",
        "#### **¬øQu√© son las Acciones?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ACCIONES = Operaciones que EJECUTAN el plan y retornan resultados\n",
        "\"\"\"\n",
        "\n",
        "acciones_definicion = {\n",
        "    'concepto': 'Operaciones que EJECUTAN las transformaciones pendientes',\n",
        "    'ejecuci√≥n': '‚úÖ Se ejecutan INMEDIATAMENTE (eager)',\n",
        "    'retorno': 'Datos al Driver o almacenamiento (NO DataFrame)',\n",
        "    'prop√≥sito': 'Materializar resultados',\n",
        "    'trigger': 'Disparan toda la pipeline de transformaciones',\n",
        "    'ejemplos': ['show', 'count', 'collect', 'write', 'take', 'first']\n",
        "}\n",
        "\n",
        "print(\"\\n‚ö° ACCIONES (EAGER)\")\n",
        "print(\"=\"*60)\n",
        "for key, value in acciones_definicion.items():\n",
        "    print(f\"{key.capitalize()}: {value}\")\n",
        "```\n",
        "\n",
        "#### **Acciones m√°s Comunes**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ACCIONES PRINCIPALES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìã ACCIONES M√ÅS COMUNES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "acciones_comunes = \"\"\"\n",
        "1. show(n) - Mostrar primeras n filas (default: 20)\n",
        "   df.show()\n",
        "   df.show(5)\n",
        "   df.show(10, truncate=False)  # Sin truncar strings largos\n",
        "   \n",
        "   ‚ö†Ô∏è Trae datos al Driver (cuidado con datasets grandes)\n",
        "\n",
        "2. count() - Contar filas\n",
        "   total = df.count()\n",
        "   print(f\"Total filas: {total}\")\n",
        "   \n",
        "   ‚úÖ No trae datos al Driver, solo el n√∫mero\n",
        "\n",
        "3. collect() - Traer TODOS los datos al Driver\n",
        "   filas = df.collect()  # Lista de Row objects\n",
        "   for fila in filas:\n",
        "       print(fila.nombre, fila.edad)\n",
        "   \n",
        "   ‚ö†Ô∏è PELIGROSO: Puede causar OutOfMemory si dataset es grande\n",
        "   ‚ö†Ô∏è NUNCA uses collect() en producci√≥n con datos grandes\n",
        "\n",
        "4. take(n) - Traer primeras n filas al Driver\n",
        "   primeras_10 = df.take(10)\n",
        "   \n",
        "   ‚úÖ M√°s seguro que collect() (l√≠mite expl√≠cito)\n",
        "\n",
        "5. first() / head() - Primera fila\n",
        "   primera_fila = df.first()\n",
        "   print(primera_fila.nombre)\n",
        "   \n",
        "   ‚úÖ Equivalente a take(1)[0]\n",
        "\n",
        "6. write - Escribir a almacenamiento\n",
        "   df.write.parquet(\"/output/datos.parquet\")\n",
        "   df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/tabla\")\n",
        "   \n",
        "   ‚úÖ Escribe distribuido (no pasa por Driver)\n",
        "\n",
        "7. saveAsTable() - Guardar como tabla\n",
        "   df.write.saveAsTable(\"mi_tabla\")\n",
        "   \n",
        "   ‚úÖ Disponible en cat√°logo de Databricks\n",
        "\n",
        "8. foreach() / foreachPartition() - Ejecutar funci√≥n en cada fila/partici√≥n\n",
        "   df.foreach(lambda row: enviar_a_api(row))\n",
        "   \n",
        "   ‚ö†Ô∏è Para side effects (escribir a DB externa, llamar APIs)\n",
        "\n",
        "9. reduce() - Reducir a un valor √∫nico\n",
        "   suma_edades = df.select(\"edad\").rdd.map(lambda r: r[0]).reduce(lambda a,b: a+b)\n",
        "   \n",
        "   ‚ö†Ô∏è Raro con DataFrames, m√°s com√∫n con RDDs\n",
        "\n",
        "10. toPandas() - Convertir a Pandas DataFrame\n",
        "    pdf = df.toPandas()\n",
        "    \n",
        "    ‚ö†Ô∏è PELIGROSO: Trae TODO al Driver\n",
        "    ‚úÖ √ötil para datasets peque√±os o despu√©s de agregaciones\n",
        "\"\"\"\n",
        "\n",
        "print(acciones_comunes)\n",
        "```\n",
        "\n",
        "#### **Acciones: Cu√°ndo Usarlas**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "GU√çA DE USO DE ACCIONES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüí° CU√ÅNDO USAR CADA ACCI√ìN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "guia_acciones = \"\"\"\n",
        "‚úÖ DESARROLLO / EXPLORACI√ìN:\n",
        "   ‚Ä¢ show() - Ver primeras filas r√°pidamente\n",
        "   ‚Ä¢ take(10) - Inspeccionar datos\n",
        "   ‚Ä¢ count() - Verificar tama√±o\n",
        "   ‚Ä¢ describe() - Estad√≠sticas descriptivas\n",
        "\n",
        "‚úÖ PRODUCCI√ìN / ETL:\n",
        "   ‚Ä¢ write() - Persistir resultados (Delta, Parquet)\n",
        "   ‚Ä¢ saveAsTable() - Guardar en cat√°logo\n",
        "   ‚Ä¢ foreach() - Side effects controlados\n",
        "\n",
        "‚ùå EVITAR EN PRODUCCI√ìN:\n",
        "   ‚Ä¢ collect() - ¬°NUNCA con datos grandes!\n",
        "   ‚Ä¢ toPandas() - Solo despu√©s de agregaciones fuertes\n",
        "   \n",
        "‚ö†Ô∏è REGLA DE ORO:\n",
        "   Si tu dataset no cabe en la memoria del Driver ‚Üí NO uses collect()\n",
        "   \n",
        "   Driver t√≠pico: 16-64 GB RAM\n",
        "   Dataset t√≠pico en big data: 100 GB - varios TB\n",
        "   \n",
        "   collect() causar√°: OutOfMemoryError üí•\n",
        "\"\"\"\n",
        "\n",
        "print(guia_acciones)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 3: Lazy Evaluation - El Coraz√≥n de Spark**\n",
        "\n",
        "#### **¬øQu√© es Lazy Evaluation?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "LAZY EVALUATION = Evaluaci√≥n Perezosa\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüõèÔ∏è LAZY EVALUATION EXPLICADA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "lazy_explicacion = \"\"\"\n",
        "CONCEPTO:\n",
        "Spark NO ejecuta transformaciones inmediatamente.\n",
        "En su lugar, construye un \"plan de ejecuci√≥n\" y solo\n",
        "lo ejecuta cuando encuentra una ACCI√ìN.\n",
        "\n",
        "ANALOG√çA: Lista de compras\n",
        "\n",
        "‚ùå EJECUCI√ìN INMEDIATA (No lazy):\n",
        "Tu: \"Compra leche\"\n",
        "Amigo: *Va al super, compra leche, vuelve*\n",
        "Tu: \"Compra pan\"\n",
        "Amigo: *Va al super, compra pan, vuelve*\n",
        "Tu: \"Compra huevos\"\n",
        "Amigo: *Va al super, compra huevos, vuelve*\n",
        "\n",
        "‚Üí 3 viajes al supermercado üò´\n",
        "\n",
        "‚úÖ EVALUACI√ìN LAZY (Spark):\n",
        "Tu: \"Compra leche\" ‚Üí Anota en lista\n",
        "Tu: \"Compra pan\" ‚Üí Anota en lista\n",
        "Tu: \"Compra huevos\" ‚Üí Anota en lista\n",
        "Tu: \"Ahora ve al super\" (ACCI√ìN)\n",
        "Amigo: *Va una vez, compra todo, vuelve*\n",
        "\n",
        "‚Üí 1 viaje al supermercado üòé\n",
        "\n",
        "BENEFICIOS:\n",
        "1. Optimizaci√≥n autom√°tica (ver toda la pipeline)\n",
        "2. Evita trabajo innecesario\n",
        "3. Ejecuci√≥n eficiente\n",
        "\"\"\"\n",
        "\n",
        "print(lazy_explicacion)\n",
        "```\n",
        "\n",
        "#### **Ejemplo Paso a Paso: Lazy Evaluation**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "EJEMPLO PR√ÅCTICO: LAZY EVALUATION EN ACCI√ìN\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüî¨ EJEMPLO: LAZY EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ejemplo_lazy = \"\"\"\n",
        "# Datos: 1 mill√≥n de registros de empleados (100 MB)\n",
        "\n",
        "import time\n",
        "\n",
        "# ===== PASO 1: Leer datos (TRANSFORMACI√ìN - LAZY) =====\n",
        "inicio = time.time()\n",
        "df = spark.read.parquet(\"/data/empleados.parquet\")\n",
        "tiempo_lectura = time.time() - inicio\n",
        "\n",
        "print(f\"Tiempo lectura: {tiempo_lectura:.3f} segundos\")\n",
        "# Output: Tiempo lectura: 0.001 segundos  ‚Üê ¬°Solo milisegundos!\n",
        "\n",
        "# ü§î ¬øC√≥mo ley√≥ 100 MB en 1ms?\n",
        "# Respuesta: ¬°NO LO HIZO! Solo ley√≥ metadata\n",
        "\n",
        "# ===== PASO 2: Filtrar (TRANSFORMACI√ìN - LAZY) =====\n",
        "inicio = time.time()\n",
        "df_filtrado = df.filter(\"salario > 50000\")\n",
        "tiempo_filtro = time.time() - inicio\n",
        "\n",
        "print(f\"Tiempo filtro: {tiempo_filtro:.3f} segundos\")\n",
        "# Output: Tiempo filtro: 0.001 segundos  ‚Üê ¬°Tambi√©n instant√°neo!\n",
        "\n",
        "# ü§î ¬øFiltr√≥ 1M registros en 1ms?\n",
        "# Respuesta: ¬°NO! Solo agreg√≥ \"filtro\" al plan\n",
        "\n",
        "# ===== PASO 3: Seleccionar columnas (TRANSFORMACI√ìN - LAZY) =====\n",
        "inicio = time.time()\n",
        "df_seleccionado = df_filtrado.select(\"nombre\", \"salario\", \"departamento\")\n",
        "tiempo_select = time.time() - inicio\n",
        "\n",
        "print(f\"Tiempo select: {tiempo_select:.3f} segundos\")\n",
        "# Output: Tiempo select: 0.001 segundos\n",
        "\n",
        "# ===== PASO 4: Agrupar (TRANSFORMACI√ìN - LAZY) =====\n",
        "inicio = time.time()\n",
        "df_agrupado = df_seleccionado.groupBy(\"departamento\").agg(\n",
        "    avg(\"salario\").alias(\"salario_promedio\")\n",
        ")\n",
        "tiempo_groupby = time.time() - inicio\n",
        "\n",
        "print(f\"Tiempo groupBy: {tiempo_groupby:.3f} segundos\")\n",
        "# Output: Tiempo groupBy: 0.001 segundos\n",
        "\n",
        "# HASTA AQU√ç: ¬°TODO INSTANT√ÅNEO!\n",
        "# Spark solo construy√≥ el PLAN, no ejecut√≥ nada\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"PLAN DE EJECUCI√ìN (lo que Spark registr√≥):\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "1. Leer: /data/empleados.parquet\n",
        "2. Filtrar: salario > 50000\n",
        "3. Seleccionar: nombre, salario, departamento\n",
        "4. Agrupar por: departamento\n",
        "5. Agregar: promedio(salario)\n",
        "\"\"\")\n",
        "\n",
        "# ===== PASO 5: ACCI√ìN - ¬°AQU√ç SE EJECUTA TODO! =====\n",
        "inicio = time.time()\n",
        "resultado = df_agrupado.show()  # ‚Üê ACCI√ìN\n",
        "tiempo_total = time.time() - inicio\n",
        "\n",
        "print(f\"\\\\nTiempo ACCI√ìN (show): {tiempo_total:.2f} segundos\")\n",
        "# Output: Tiempo ACCI√ìN: 3.45 segundos\n",
        "\n",
        "# ‚ö° AHORA S√ç se ejecut√≥ toda la pipeline:\n",
        "#    Leer ‚Üí Filtrar ‚Üí Seleccionar ‚Üí Agrupar ‚Üí Mostrar\n",
        "\n",
        "print(\"\"\"\n",
        "+-------------+------------------+\n",
        "| departamento|salario_promedio  |\n",
        "+-------------+------------------+\n",
        "|           IT|           75000.0|\n",
        "|       Ventas|           62000.0|\n",
        "|          RH |           58000.0|\n",
        "+-------------+------------------+\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"RESUMEN:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Tiempo definiendo transformaciones: ~0.004 segundos\")\n",
        "print(f\"Tiempo ejecutando (acci√≥n):         ~3.45 segundos\")\n",
        "print(f\"\\\\n‚úÖ Spark optimiz√≥ y ejecut√≥ todo en UN SOLO PASO\")\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_lazy)\n",
        "```\n",
        "\n",
        "#### **DAG: Directed Acyclic Graph**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DAG = Plan de Ejecuci√≥n de Spark\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüó∫Ô∏è DAG (DIRECTED ACYCLIC GRAPH)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "dag_explicacion = \"\"\"\n",
        "Cuando defines transformaciones, Spark construye un DAG:\n",
        "Un grafo que representa las dependencias entre operaciones.\n",
        "\n",
        "EJEMPLO: Pipeline de an√°lisis de ventas\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    DAG                              ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                     ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
        "‚îÇ  ‚îÇ  READ   ‚îÇ (Leer CSV de ventas)                 ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
        "‚îÇ       ‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
        "‚îÇ  ‚îÇ FILTER  ‚îÇ (fecha >= '2025-01-01')              ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
        "‚îÇ       ‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
        "‚îÇ  ‚îÇ SELECT  ‚îÇ (producto, cantidad, precio)          ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
        "‚îÇ       ‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                      ‚îÇ\n",
        "‚îÇ  ‚îÇwithColumn‚îÇ (total = cantidad * precio)          ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                      ‚îÇ\n",
        "‚îÇ       ‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
        "‚îÇ  ‚îÇ GROUPBY ‚îÇ (producto)  ‚Üê CAUSA SHUFFLE           ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
        "‚îÇ       ‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
        "‚îÇ  ‚îÇ   AGG   ‚îÇ (sum(total))                          ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
        "‚îÇ       ‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
        "‚îÇ  ‚îÇ ORDERBY ‚îÇ (total DESC)  ‚Üê CAUSA SHUFFLE         ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
        "‚îÇ       ‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
        "‚îÇ  ‚îÇ  SHOW   ‚îÇ ‚Üê ACCI√ìN (ejecuta todo el DAG)       ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "STAGES (Spark divide el DAG en stages):\n",
        "\n",
        "STAGE 1: Read ‚Üí Filter ‚Üí Select ‚Üí withColumn\n",
        "         (Todo narrow, sin shuffle)\n",
        "\n",
        "STAGE 2: GroupBy ‚Üí Agg\n",
        "         (Requiere shuffle - nueva stage)\n",
        "\n",
        "STAGE 3: OrderBy\n",
        "         (Requiere shuffle - nueva stage)\n",
        "\n",
        "STAGE 4: Show\n",
        "         (Recolectar resultados al Driver)\n",
        "\"\"\"\n",
        "\n",
        "print(dag_explicacion)\n",
        "```\n",
        "\n",
        "#### **Optimizaciones de Catalyst**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CATALYST OPTIMIZER: El cerebro detr√°s de Spark SQL\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüß† CATALYST OPTIMIZER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "catalyst_explicacion = \"\"\"\n",
        "Catalyst es el optimizador de Spark que mejora autom√°ticamente\n",
        "tu c√≥digo ANTES de ejecutarlo.\n",
        "\n",
        "EJEMPLO DE OPTIMIZACI√ìN:\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# TU C√ìDIGO (Naive):\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "df = spark.read.parquet(\"/data/ventas/\")  # 500 GB, 50 columnas\n",
        "\n",
        "df_resultado = df \\\\\n",
        "    .select(\"*\") \\\\                          # Seleccionar todas\n",
        "    .filter(\"fecha >= '2025-01-01'\") \\\\      # Filtrar por fecha\n",
        "    .select(\"producto\", \"cantidad\", \"precio\") \\\\  # Solo 3 columnas\n",
        "    .filter(\"precio > 100\")                  # Filtrar por precio\n",
        "\n",
        "df_resultado.count()\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# LO QUE CATALYST OPTIMIZA (Autom√°ticamente):\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# 1. COLUMN PRUNING (Poda de columnas)\n",
        "#    \"Solo necesita 3 columnas, no las 50\"\n",
        "#    ‚Üí Lee solo producto, cantidad, precio, fecha desde disco\n",
        "#    Ahorro: 47 columnas no le√≠das = ~90% menos datos\n",
        "\n",
        "# 2. PREDICATE PUSHDOWN (Empujar filtros)\n",
        "#    \"Los filtros se aplican mejor al leer\"\n",
        "#    ‚Üí Aplica ambos filtros durante la lectura\n",
        "#    Ahorro: Lee menos datos desde disco\n",
        "\n",
        "# 3. FILTER FUSION (Fusi√≥n de filtros)\n",
        "#    \"Dos filtros consecutivos = un filtro combinado\"\n",
        "#    ‚Üí fecha >= '2025-01-01' AND precio > 100\n",
        "#    Ahorro: Un solo paso en lugar de dos\n",
        "\n",
        "# 4. PROJECTION PUSHDOWN\n",
        "#    \"No necesita 'fecha' en el resultado final\"\n",
        "#    ‚Üí No mantiene 'fecha' en memoria despu√©s del filtro\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# PLAN OPTIMIZADO FINAL:\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Plan real que ejecuta Spark:\n",
        "\n",
        "1. Leer SOLO: producto, cantidad, precio, fecha (no las 50)\n",
        "2. Mientras lee, FILTRAR: fecha >= '2025-01-01' AND precio > 100\n",
        "3. Despu√©s de filtrar, DROP fecha (no se necesita)\n",
        "4. Contar resultados\n",
        "\n",
        "RESULTADO:\n",
        "- Original ingenuo: Lee 500 GB\n",
        "- Optimizado Catalyst: Lee ~50 GB (filtros tempranos) + solo 3 columnas\n",
        "- Speedup: 10x m√°s r√°pido ‚ö°\n",
        "- Sin cambiar tu c√≥digo ‚ú®\n",
        "\"\"\"\n",
        "\n",
        "print(catalyst_explicacion)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Identificando Transformaciones y Acciones**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "EJERCICIO: IDENTIFICA TRANSFORMACIONES VS ACCIONES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüß™ EJERCICIO PR√ÅCTICO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ejercicio = \"\"\"\n",
        "# Dado el siguiente c√≥digo, identifica:\n",
        "# T = Transformaci√≥n (lazy)\n",
        "# A = Acci√≥n (eager, ejecuta)\n",
        "\n",
        "df = spark.read.csv(\"/data/ventas.csv\", header=True)  # ___\n",
        "\n",
        "df2 = df.filter(\"cantidad > 0\")                       # ___\n",
        "\n",
        "df3 = df2.select(\"producto\", \"cantidad\", \"precio\")    # ___\n",
        "\n",
        "df4 = df3.withColumn(\"total\", col(\"cantidad\") * col(\"precio\"))  # ___\n",
        "\n",
        "df_agrupado = df4.groupBy(\"producto\").sum(\"total\")    # ___\n",
        "\n",
        "print(\"Hasta aqu√≠, ¬øse ejecut√≥ algo?\")                # ___\n",
        "\n",
        "df_agrupado.show()                                     # ___\n",
        "\n",
        "df_agrupado.write.parquet(\"/output/resumen.parquet\")  # ___\n",
        "\n",
        "total_productos = df_agrupado.count()                 # ___\n",
        "\n",
        "print(f\"Total: {total_productos}\")                    # ___\n",
        "\n",
        "top_5 = df_agrupado.orderBy(\"sum(total)\", ascending=False).take(5)  # ___\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# RESPUESTAS:\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "spark.read.csv(...)                 ‚Üí T (lazy)\n",
        "df.filter(...)                      ‚Üí T (lazy)\n",
        "df.select(...)                      ‚Üí T (lazy)\n",
        "df.withColumn(...)                  ‚Üí T (lazy)\n",
        "df.groupBy(...).sum(...)            ‚Üí T (lazy)\n",
        "print(\"Hasta aqu√≠...\")              ‚Üí No es Spark (Python)\n",
        "df_agrupado.show()                  ‚Üí A (ejecuta toda la pipeline hasta aqu√≠)\n",
        "df_agrupado.write.parquet(...)      ‚Üí A (ejecuta y escribe)\n",
        "df_agrupado.count()                 ‚Üí A (ejecuta y cuenta)\n",
        "print(f\"Total...\")                  ‚Üí No es Spark (Python)\n",
        "df_agrupado.orderBy(...).take(5)    ‚Üí A (take es acci√≥n)\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# ¬øCU√ÅNTAS VECES SE EJECUT√ì LA PIPELINE COMPLETA?\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "3 VECES:\n",
        "1. show() - ejecuta todo hasta groupBy\n",
        "2. write.parquet() - ejecuta todo de nuevo\n",
        "3. count() - ejecuta todo de nuevo\n",
        "4. take(5) - ejecuta todo de nuevo (con orderBy adicional)\n",
        "\n",
        "‚ö†Ô∏è PROBLEMA: ¬°4 ejecuciones del mismo procesamiento!\n",
        "\n",
        "üí° SOLUCI√ìN: Usar CACHE\n",
        "\"\"\"\n",
        "\n",
        "print(ejercicio)\n",
        "```\n",
        "\n",
        "#### **Optimizaci√≥n con Cache/Persist**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CACHE/PERSIST: Evitar re-c√≥mputo\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüíæ OPTIMIZACI√ìN CON CACHE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "cache_explicacion = \"\"\"\n",
        "Cuando una acci√≥n ejecuta, Spark NO guarda los resultados\n",
        "intermedios por defecto. Si ejecutas otra acci√≥n, Spark\n",
        "re-ejecuta TODA la pipeline desde el inicio.\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# SIN CACHE (Ineficiente):\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "df_procesado = df.filter(...).select(...).groupBy(...)  # Lazy\n",
        "\n",
        "df_procesado.show()         # Ejecuta toda la pipeline\n",
        "df_procesado.count()        # Re-ejecuta TODA la pipeline\n",
        "df_procesado.write.parquet(...)  # Re-ejecuta OTRA VEZ\n",
        "\n",
        "‚Üí 3 ejecuciones completas üò´\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# CON CACHE (Eficiente):\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "df_procesado = df.filter(...).select(...).groupBy(...)\n",
        "\n",
        "# IMPORTANTE: cache() es una transformaci√≥n lazy\n",
        "df_procesado = df_procesado.cache()\n",
        "\n",
        "# Primera acci√≥n: ejecuta y guarda en memoria\n",
        "df_procesado.show()         # Ejecuta y cachea\n",
        "\n",
        "# Siguientes acciones: leen de cache (r√°pido)\n",
        "df_procesado.count()        # Lee de cache ‚ö°\n",
        "df_procesado.write.parquet(...)  # Lee de cache ‚ö°\n",
        "\n",
        "‚Üí 1 ejecuci√≥n + 2 lecturas de cache üòé\n",
        "\n",
        "# Liberar cache cuando termines\n",
        "df_procesado.unpersist()\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# PERSIST con niveles de almacenamiento:\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "from pyspark import StorageLevel\n",
        "\n",
        "# Solo memoria (default de cache)\n",
        "df.persist(StorageLevel.MEMORY_ONLY)\n",
        "\n",
        "# Memoria + disco (si no cabe en RAM)\n",
        "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "# Serializado (usa menos RAM pero m√°s lento)\n",
        "df.persist(StorageLevel.MEMORY_ONLY_SER)\n",
        "\n",
        "# Replicado (fault tolerance extra)\n",
        "df.persist(StorageLevel.MEMORY_AND_DISK_2)  # 2 copias\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# ¬øCU√ÅNDO USAR CACHE?\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "‚úÖ USA CACHE cuando:\n",
        "   - Vas a usar el mismo DataFrame m√∫ltiples veces\n",
        "   - El DataFrame es resultado de transformaciones costosas\n",
        "   - Iteraciones (ML training)\n",
        "\n",
        "‚ùå NO USES CACHE cuando:\n",
        "   - Solo usas el DataFrame una vez\n",
        "   - El DataFrame es muy grande y no cabe en memoria\n",
        "   - Es el resultado de lectura simple (ya optimizado)\n",
        "\n",
        "‚ö†Ô∏è REGLA:\n",
        "   Si haces 2+ acciones en el mismo DF ‚Üí considera cache\n",
        "\"\"\"\n",
        "\n",
        "print(cache_explicacion)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 2.3 - Optimizaci√≥n de C√≥digo**\n",
        "\n",
        "```markdown\n",
        "### Ejercicio: Optimiza este c√≥digo\n",
        "\n",
        "**C√≥digo Original** (ineficiente):\n",
        "\n",
        "```python\n",
        "# Leer datos\n",
        "df_ventas = spark.read.parquet(\"/data/ventas/\")\n",
        "\n",
        "# An√°lisis 1: Ventas totales\n",
        "total_ventas = df_ventas.agg(sum(\"monto\")).collect()[0][0]\n",
        "print(f\"Total ventas: {total_ventas}\")\n",
        "\n",
        "# An√°lisis 2: Ventas por producto\n",
        "df_por_producto = df_ventas.groupBy(\"producto\").sum(\"monto\")\n",
        "df_por_producto.show()\n",
        "\n",
        "# An√°lisis 3: Top 10 productos\n",
        "top_10 = df_por_producto.orderBy(col(\"sum(monto)\").desc()).take(10)\n",
        "\n",
        "# An√°lisis 4: Exportar reporte\n",
        "df_por_producto.write.csv(\"/output/reporte.csv\")\n",
        "```\n",
        "\n",
        "**Problemas identificados**:\n",
        "1. _____________\n",
        "2. _____________\n",
        "3. _____________\n",
        "\n",
        "**Tu c√≥digo optimizado**:\n",
        "```python\n",
        "# Escribe aqu√≠ tu versi√≥n optimizada\n",
        "```\n",
        "\n",
        "**Explicaci√≥n de mejoras**:\n",
        "1. _____________\n",
        "2. _____________\n",
        "3. _____________\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **Transformaciones = Lazy** (no ejecutan, construyen plan)\n",
        "2. **Acciones = Eager** (ejecutan el plan completo)\n",
        "3. **Narrow transformations** no requieren shuffle (r√°pidas)\n",
        "4. **Wide transformations** requieren shuffle (lentas pero necesarias)\n",
        "5. **Lazy evaluation** permite optimizaci√≥n autom√°tica\n",
        "6. **Catalyst Optimizer** mejora tu c√≥digo autom√°ticamente\n",
        "7. **DAG** es el plan de ejecuci√≥n que Spark construye\n",
        "8. **Cache/Persist** evita re-computaci√≥n cuando usas un DF m√∫ltiples veces\n",
        "9. **collect()** es peligroso con datos grandes (OOM)\n",
        "10. **Usa .explain()** para ver el plan de ejecuci√≥n\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **Usa .explain() para debugging**\n",
        "```python\n",
        "df.filter(...).groupBy(...).explain(True)\n",
        "# Muestra: Parsed, Analyzed, Optimized y Physical plans\n",
        "```\n",
        "\n",
        "‚úÖ **Minimiza acciones en desarrollo**\n",
        "```python\n",
        "# Mal: m√∫ltiples show() mientras desarrollas\n",
        "df.filter(...).show()\n",
        "df.select(...).show()\n",
        "df.groupBy(...).show()\n",
        "\n",
        "# Bien: construye toda la pipeline, un show() al final\n",
        "df.filter(...).select(...).groupBy(...).show()\n",
        "```\n",
        "\n",
        "‚úÖ **Persist DataFrames usados m√∫ltiples veces**\n",
        "```python\n",
        "df_procesado = df.filter(...).groupBy(...).cache()\n",
        "# Ahora √∫salo m√∫ltiples veces sin re-computaci√≥n\n",
        "```\n",
        "\n",
        "‚úÖ **Monitorea Spark UI**\n",
        "```python\n",
        "# En Databricks, el Spark UI muestra:\n",
        "# - Cu√°ntos stages\n",
        "# - Cu√°ntos shuffles\n",
        "# - Tiempo por operaci√≥n\n",
        "# ‚Üí Identifica cuellos de botella\n",
        "```\n",
        "\n",
        "‚úÖ **Evita collect() en producci√≥n**\n",
        "```python\n",
        "# Mal\n",
        "todos_los_datos = df.collect()  # ¬°OOM con datos grandes!\n",
        "\n",
        "# Bien\n",
        "df.write.parquet(\"/output/\")    # Escribe distribuido\n",
        "# O\n",
        "primeros_1000 = df.limit(1000).collect()  # L√≠mite seguro\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Has completado el Tema 2! üéâ**\n",
        "\n",
        "Ahora entiendes:\n",
        "- Las tres abstracciones de Spark (RDD, DataFrame, Dataset)\n",
        "- La diferencia entre transformaciones y acciones\n",
        "- Lazy evaluation y c√≥mo Spark optimiza autom√°ticamente\n",
        "- Cu√°ndo usar cache para optimizar\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para comenzar el **M√≥dulo 2: Primeros Pasos Pr√°cticos** con el **Tema 3: Configuraci√≥n del Entorno**? üöÄ"
      ],
      "metadata": {
        "id": "A_qB9M-uYEFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **M√≥dulo 2: Primeros Pasos Pr√°cticos**"
      ],
      "metadata": {
        "id": "hX1yWFHDht_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tema 3: Configuraci√≥n del Entorno**\n",
        "\n",
        "---\n",
        "\n",
        "### **3.1 Creaci√≥n de cuenta en Databricks (Free Edition)**\n",
        "\n",
        "#### **Introducci√≥n: Tu Primer Paso en Databricks**\n",
        "\n",
        "Databricks ofrece una **edici√≥n gratuita (Free Edition)**, antes conocida como Community Edition, dise√±ada para uso personal, aprendizaje y colaboraci√≥n en notebooks de Apache Spark sin costo, sin requerir tarjeta de cr√©dito.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DATABRICKS FREE EDITION\n",
        "\"\"\"\n",
        "\n",
        "free_edition_info = {\n",
        "    'nombre_actual': 'Databricks Free Edition',\n",
        "    'nombre_anterior': 'Community Edition',\n",
        "    'costo': '$0 - Completamente gratis',\n",
        "    'tarjeta_credito': 'No requerida',\n",
        "    'prop√≥sito': [\n",
        "        'Aprendizaje personal',\n",
        "        'Experimentaci√≥n con Spark',\n",
        "        'Pr√°ctica con notebooks',\n",
        "        'Desarrollo de POCs peque√±os'\n",
        "    ],\n",
        "    'limitaciones': [\n",
        "        'Cluster de tama√±o fijo (15 GB RAM)',\n",
        "        'Inactivo despu√©s de 2 horas sin uso',\n",
        "        'No acceso a todas las features enterprise',\n",
        "        'Almacenamiento limitado'\n",
        "    ],\n",
        "    'suficiente_para': [\n",
        "        '‚úÖ Este curso completo',\n",
        "        '‚úÖ Aprender Spark y PySpark',\n",
        "        '‚úÖ Practicar SQL',\n",
        "        '‚úÖ Proyectos personales peque√±os',\n",
        "        '‚úÖ Certificaciones de Databricks'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"üÜì DATABRICKS FREE EDITION\")\n",
        "print(\"=\"*60)\n",
        "for key, value in free_edition_info.items():\n",
        "    print(f\"{key.replace('_', ' ').title()}:\")\n",
        "    if isinstance(value, list):\n",
        "        for item in value:\n",
        "            print(f\"  ‚Ä¢ {item}\")\n",
        "    else:\n",
        "        print(f\"  {value}\")\n",
        "    print()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Paso a Paso: Crear tu Cuenta Free Edition**\n",
        "\n",
        "#### **Paso 1: Acceder al Sitio de Databricks**\n",
        "\n",
        "```markdown\n",
        "üåê PASO 1: IR AL SITIO WEB\n",
        "\n",
        "1. Abre tu navegador\n",
        "2. Ve a: https://www.databricks.com/try-databricks\n",
        "   \n",
        "   O directamente a Free Edition:\n",
        "   https://community.cloud.databricks.com/\n",
        "\n",
        "üìå IMPORTANTE:\n",
        "   - Usa un navegador moderno (Chrome, Firefox, Edge, Safari)\n",
        "   - Aseg√∫rate de tener conexi√≥n estable a internet\n",
        "```\n",
        "\n",
        "#### **Paso 2: Seleccionar Free Edition**\n",
        "\n",
        "```markdown\n",
        "üìù PASO 2: SELECCIONAR PLAN GRATUITO\n",
        "\n",
        "En la p√°gina de registro ver√°s varias opciones:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Get started with Databricks                ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚óã Try Databricks (14-day trial)           ‚îÇ\n",
        "‚îÇ     - Features completas                    ‚îÇ\n",
        "‚îÇ     - Requiere tarjeta de cr√©dito          ‚îÇ\n",
        "‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚óè Get started for free (FREE EDITION)     ‚îÇ\n",
        "‚îÇ     - Sin tarjeta de cr√©dito  ‚Üê ELIGE ESTA‚îÇ\n",
        "‚îÇ     - Perfecta para aprendizaje            ‚îÇ\n",
        "‚îÇ                                             ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚úÖ Selecciona: \"Get started for free\" o \"Community Edition\"\n",
        "```\n",
        "\n",
        "#### **Paso 3: Completar el Formulario de Registro**\n",
        "\n",
        "```markdown\n",
        "üìã PASO 3: FORMULARIO DE REGISTRO\n",
        "\n",
        "Completa los siguientes campos:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Create your Databricks account            ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                             ‚îÇ\n",
        "‚îÇ  First Name: ____________________           ‚îÇ\n",
        "‚îÇ  Last Name:  ____________________           ‚îÇ\n",
        "‚îÇ  Email:      ____________________           ‚îÇ\n",
        "‚îÇ              (usa email personal/trabajo)   ‚îÇ\n",
        "‚îÇ                                             ‚îÇ\n",
        "‚îÇ  Company:    ____________________           ‚îÇ\n",
        "‚îÇ              (opcional - puede ser \"N/A\")   ‚îÇ\n",
        "‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚òë I agree to the Terms of Service         ‚îÇ\n",
        "‚îÇ                                             ‚îÇ\n",
        "‚îÇ  [    Create Account    ]                  ‚îÇ\n",
        "‚îÇ                                             ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚ö†Ô∏è IMPORTANTE:\n",
        "   - Usa un email al que tengas acceso (verificaci√≥n requerida)\n",
        "   - El campo \"Company\" es opcional para Free Edition\n",
        "   - Lee los t√©rminos de servicio\n",
        "```\n",
        "\n",
        "#### **Paso 4: Verificar Email**\n",
        "\n",
        "```markdown\n",
        "üìß PASO 4: VERIFICACI√ìN DE EMAIL\n",
        "\n",
        "1. Revisa tu bandeja de entrada\n",
        "   \n",
        "   Subject: \"Verify your Databricks account\"\n",
        "   From: no-reply@databricks.com\n",
        "\n",
        "2. Haz clic en el enlace de verificaci√≥n\n",
        "   \n",
        "   \"Verify Email Address\"\n",
        "\n",
        "3. Ser√°s redirigido a la p√°gina de configuraci√≥n\n",
        "\n",
        "‚è∞ Si no recibes el email:\n",
        "   - Revisa spam/correo no deseado\n",
        "   - Espera 5 minutos\n",
        "   - Solicita reenv√≠o del email\n",
        "```\n",
        "\n",
        "#### **Paso 5: Configurar Contrase√±a**\n",
        "\n",
        "```markdown\n",
        "üîê PASO 5: CREAR CONTRASE√ëA\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Set your password                          ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                             ‚îÇ\n",
        "‚îÇ  Password:         ____________________     ‚îÇ\n",
        "‚îÇ  Confirm Password: ____________________     ‚îÇ\n",
        "‚îÇ                                             ‚îÇ\n",
        "‚îÇ  Requirements:                              ‚îÇ\n",
        "‚îÇ  ‚úì At least 8 characters                   ‚îÇ\n",
        "‚îÇ  ‚úì Contains uppercase letter               ‚îÇ\n",
        "‚îÇ  ‚úì Contains lowercase letter               ‚îÇ\n",
        "‚îÇ  ‚úì Contains number                         ‚îÇ\n",
        "‚îÇ  ‚úì Contains special character              ‚îÇ\n",
        "‚îÇ                                             ‚îÇ\n",
        "‚îÇ  [    Set Password    ]                    ‚îÇ\n",
        "‚îÇ                                             ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "üí° CONSEJO:\n",
        "   - Usa un gestor de contrase√±as (LastPass, 1Password, Bitwarden)\n",
        "   - Crea una contrase√±a fuerte y √∫nica\n",
        "   - Gu√°rdala en un lugar seguro\n",
        "```\n",
        "\n",
        "#### **Paso 6: Primer Acceso - Workspace**\n",
        "\n",
        "```markdown\n",
        "üéâ PASO 6: ¬°BIENVENIDO A DATABRICKS!\n",
        "\n",
        "Despu√©s de configurar tu contrase√±a, ser√°s redirigido al\n",
        "DATABRICKS WORKSPACE:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  üè† Databricks                        üë§ Tu Nombre ‚ñº    ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                         ‚îÇ\n",
        "‚îÇ  üéØ Quick Start                                         ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ\n",
        "‚îÇ  ‚îÇ  Welcome to Databricks Free Edition!        ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îÇ                                             ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îÇ  [ Create a cluster ]                       ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îÇ  [ Import notebook ]                        ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îÇ  [ Browse sample notebooks ]                ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ\n",
        "‚îÇ                                                         ‚îÇ\n",
        "‚îÇ  üìÅ Recent items:                                       ‚îÇ\n",
        "‚îÇ  (vac√≠o por ahora)                                      ‚îÇ\n",
        "‚îÇ                                                         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚úÖ ¬°Cuenta creada exitosamente!\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Navegando por la Interfaz (Primera Exploraci√≥n)**\n",
        "\n",
        "#### **Vista General del Workspace**\n",
        "\n",
        "```markdown\n",
        "üó∫Ô∏è MAPA DEL WORKSPACE DE DATABRICKS\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  BARRA SUPERIOR                                       ‚îÇ\n",
        "‚îÇ  üè† Databricks | üîç Search | üîî | üë§ Profile ‚ñº       ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  SIDEBAR (Men√∫ Lateral)    ‚îÇ  √ÅREA PRINCIPAL         ‚îÇ\n",
        "‚îÇ                            ‚îÇ                         ‚îÇ\n",
        "‚îÇ  üè† Home                   ‚îÇ  Contenido principal    ‚îÇ\n",
        "‚îÇ  üìÅ Workspace              ‚îÇ  (var√≠a seg√∫n secci√≥n)  ‚îÇ\n",
        "‚îÇ  üî¨ Data Science           ‚îÇ                         ‚îÇ\n",
        "‚îÇ  üíº Data Engineering       ‚îÇ                         ‚îÇ\n",
        "‚îÇ  üìä SQL                    ‚îÇ                         ‚îÇ\n",
        "‚îÇ  ‚öôÔ∏è  Compute               ‚îÇ                         ‚îÇ\n",
        "‚îÇ  üì¶ Catalog                ‚îÇ                         ‚îÇ\n",
        "‚îÇ  üìà Dashboards             ‚îÇ                         ‚îÇ\n",
        "‚îÇ  ‚ö° Workflows              ‚îÇ                         ‚îÇ\n",
        "‚îÇ  ‚öôÔ∏è  Settings              ‚îÇ                         ‚îÇ\n",
        "‚îÇ                            ‚îÇ                         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "SECCIONES PRINCIPALES:\n",
        "```\n",
        "\n",
        "#### **Exploraci√≥n de Secciones Clave**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "SECCIONES DEL WORKSPACE\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìç NAVEGACI√ìN PRINCIPAL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "secciones = {\n",
        "    'üè† Home': {\n",
        "        'descripci√≥n': 'P√°gina de inicio con acceso r√°pido',\n",
        "        'contiene': [\n",
        "            'Quick actions (acciones r√°pidas)',\n",
        "            'Recent items (elementos recientes)',\n",
        "            'Getting started tutorials'\n",
        "        ],\n",
        "        'uso': 'Punto de partida cada vez que accedes'\n",
        "    },\n",
        "    \n",
        "    'üìÅ Workspace': {\n",
        "        'descripci√≥n': 'Sistema de archivos para notebooks y carpetas',\n",
        "        'contiene': [\n",
        "            'Tus notebooks',\n",
        "            'Carpetas organizadas',\n",
        "            'Notebooks compartidos',\n",
        "            'Importar/exportar notebooks'\n",
        "        ],\n",
        "        'uso': 'Organizar tu trabajo como archivos en un disco'\n",
        "    },\n",
        "    \n",
        "    '‚öôÔ∏è Compute': {\n",
        "        'descripci√≥n': 'Gesti√≥n de clusters (m√°quinas virtuales)',\n",
        "        'contiene': [\n",
        "            'Lista de clusters',\n",
        "            'Crear nuevos clusters',\n",
        "            'Configurar recursos',\n",
        "            'Monitorear estado'\n",
        "        ],\n",
        "        'uso': 'Crear y gestionar clusters para ejecutar c√≥digo'\n",
        "    },\n",
        "    \n",
        "    'üì¶ Catalog': {\n",
        "        'descripci√≥n': 'Navegador de datos (bases de datos, tablas)',\n",
        "        'contiene': [\n",
        "            'Databases',\n",
        "            'Tables',\n",
        "            'Views',\n",
        "            'Schema information'\n",
        "        ],\n",
        "        'uso': 'Explorar qu√© datos tienes disponibles'\n",
        "    },\n",
        "    \n",
        "    '‚ö° Workflows': {\n",
        "        'descripci√≥n': 'Automatizaci√≥n de tareas (Jobs)',\n",
        "        'contiene': [\n",
        "            'Scheduled jobs',\n",
        "            'Job runs history',\n",
        "            'Pipeline orchestration'\n",
        "        ],\n",
        "        'uso': 'Programar notebooks para ejecuci√≥n autom√°tica'\n",
        "    },\n",
        "    \n",
        "    '‚öôÔ∏è Settings': {\n",
        "        'descripci√≥n': 'Configuraci√≥n de la cuenta',\n",
        "        'contiene': [\n",
        "            'User settings',\n",
        "            'Admin console',\n",
        "            'Access tokens',\n",
        "            'Git integration'\n",
        "        ],\n",
        "        'uso': 'Personalizar tu entorno'\n",
        "    }\n",
        "}\n",
        "\n",
        "for seccion, info in secciones.items():\n",
        "    print(f\"\\n{seccion}\")\n",
        "    print(f\"  {info['descripci√≥n']}\")\n",
        "    print(\"  Contiene:\")\n",
        "    for item in info['contiene']:\n",
        "        print(f\"    ‚Ä¢ {item}\")\n",
        "    print(f\"  Cu√°ndo usar: {info['uso']}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Diferencias: Free Edition vs Versiones de Pago**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "COMPARACI√ìN: FREE EDITION vs PAID\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüíé FREE EDITION vs PREMIUM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "comparacion = pd.DataFrame({\n",
        "    'Feature': [\n",
        "        'Costo',\n",
        "        'Tarjeta de cr√©dito',\n",
        "        'Tama√±o de cluster',\n",
        "        'N√∫mero de clusters',\n",
        "        'Tiempo de inactividad',\n",
        "        'Usuarios colaboradores',\n",
        "        'Unity Catalog',\n",
        "        'Delta Lake',\n",
        "        'MLflow',\n",
        "        'Notebooks',\n",
        "        'SQL Warehouses',\n",
        "        'Jobs/Workflows',\n",
        "        'Git integration',\n",
        "        'Almacenamiento',\n",
        "        'Soporte',\n",
        "        'Ideal para'\n",
        "    ],\n",
        "    'Free Edition': [\n",
        "        'üÜì $0',\n",
        "        '‚ùå No requerida',\n",
        "        'üîí Fijo (~15 GB RAM)',\n",
        "        '1 cluster a la vez',\n",
        "        '‚è±Ô∏è 2 horas sin uso',\n",
        "        'üë§ Solo t√∫',\n",
        "        '‚ùå No',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ B√°sico',\n",
        "        '‚úÖ Ilimitados',\n",
        "        '‚ùå No',\n",
        "        '‚ùå No',\n",
        "        '‚úÖ S√≠ (GitHub)',\n",
        "        'üì¶ Limitado (~15 GB)',\n",
        "        'üí¨ Community forum',\n",
        "        'Aprendizaje, pr√°ctica'\n",
        "    ],\n",
        "    'Standard/Premium': [\n",
        "        'üí∞ $$$',\n",
        "        '‚úÖ Requerida',\n",
        "        '‚öôÔ∏è Configurable',\n",
        "        'M√∫ltiples',\n",
        "        '‚ôæÔ∏è Configurable',\n",
        "        'üë• Equipos completos',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ Completo',\n",
        "        '‚úÖ Ilimitados',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ S√≠ (m√∫ltiples)',\n",
        "        'üì¶ Cloud storage ilimitado',\n",
        "        'üìû Enterprise support',\n",
        "        'Producci√≥n, empresas'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(comparacion.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üí° CONCLUSI√ìN:\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "Free Edition es PERFECTA para:\n",
        "‚úÖ Completar este curso\n",
        "‚úÖ Aprender Spark y PySpark\n",
        "‚úÖ Practicar para certificaciones\n",
        "‚úÖ Proyectos personales peque√±os\n",
        "‚úÖ POCs y experimentaci√≥n\n",
        "\n",
        "NO es adecuada para:\n",
        "‚ùå Aplicaciones en producci√≥n\n",
        "‚ùå Procesamiento de datasets > 10 GB\n",
        "‚ùå Trabajo colaborativo en equipo\n",
        "‚ùå Jobs autom√°ticos programados\n",
        "‚ùå An√°lisis 24/7\n",
        "\n",
        "‚Üí Para producci√≥n, necesitar√°s upgrade a Standard/Premium\n",
        "‚Üí Pero para aprender, ¬°Free Edition es suficiente!\n",
        "\"\"\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Verificar tu Cuenta**\n",
        "\n",
        "```markdown\n",
        "üß™ EJERCICIO: VERIFICACI√ìN DE CUENTA\n",
        "\n",
        "Sigue estos pasos para verificar que todo funciona:\n",
        "\n",
        "### ‚úÖ CHECKLIST DE VERIFICACI√ìN\n",
        "\n",
        "1. **Acceso al Workspace**\n",
        "   - [ ] Puedo iniciar sesi√≥n en https://community.cloud.databricks.com\n",
        "   - [ ] Veo mi nombre en la esquina superior derecha\n",
        "   - [ ] Puedo navegar entre las diferentes secciones del men√∫\n",
        "\n",
        "2. **Navegaci√≥n B√°sica**\n",
        "   - [ ] Puedo acceder a Home\n",
        "   - [ ] Puedo acceder a Workspace\n",
        "   - [ ] Puedo acceder a Compute\n",
        "   - [ ] Puedo acceder a Catalog\n",
        "\n",
        "3. **Informaci√≥n de la Cuenta**\n",
        "   - [ ] Email de registro verificado\n",
        "   - [ ] Contrase√±a funcionando correctamente\n",
        "   - [ ] Puedo cambiar mi foto de perfil (opcional)\n",
        "\n",
        "### üì∏ CAPTURA DE PANTALLA\n",
        "\n",
        "Toma una captura de tu Workspace home page para tus apuntes.\n",
        "Incluye:\n",
        "- La barra superior con tu nombre\n",
        "- El men√∫ lateral\n",
        "- El √°rea de \"Quick Start\"\n",
        "\n",
        "### üìù NOTAS\n",
        "\n",
        "Anota aqu√≠:\n",
        "- Email utilizado: _________________\n",
        "- Fecha de creaci√≥n: _________________\n",
        "- URL de tu workspace: _________________\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Soluci√≥n de Problemas Comunes**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "TROUBLESHOOTING: PROBLEMAS COMUNES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîß SOLUCI√ìN DE PROBLEMAS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "problemas_comunes = {\n",
        "    '‚ùå No recib√≠ el email de verificaci√≥n': {\n",
        "        'causas': [\n",
        "            'Email en carpeta de spam',\n",
        "            'Email bloqueado por firewall corporativo',\n",
        "            'Direcci√≥n incorrecta'\n",
        "        ],\n",
        "        'soluciones': [\n",
        "            '1. Revisar carpeta spam/correo no deseado',\n",
        "            '2. Agregar no-reply@databricks.com a contactos',\n",
        "            '3. Esperar 5-10 minutos',\n",
        "            '4. Solicitar reenv√≠o en la p√°gina de login',\n",
        "            '5. Intentar con email personal (Gmail, Outlook)'\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    '‚ùå No puedo acceder despu√©s de crear cuenta': {\n",
        "        'causas': [\n",
        "            'Contrase√±a incorrecta',\n",
        "            'Email no verificado',\n",
        "            'Cuenta no activada completamente'\n",
        "        ],\n",
        "        'soluciones': [\n",
        "            '1. Verificar que hiciste clic en el link del email',\n",
        "            '2. Usar \"Forgot password\" para resetear',\n",
        "            '3. Verificar que usas el URL correcto:',\n",
        "            '   https://community.cloud.databricks.com',\n",
        "            '4. Limpiar cache del navegador',\n",
        "            '5. Probar con navegador en modo inc√≥gnito'\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    '‚ùå La p√°gina carga muy lento': {\n",
        "        'causas': [\n",
        "            'Conexi√≥n a internet lenta',\n",
        "            'Muchas pesta√±as abiertas',\n",
        "            'Problemas temporales del servidor'\n",
        "        ],\n",
        "        'soluciones': [\n",
        "            '1. Verificar velocidad de internet (>5 Mbps recomendado)',\n",
        "            '2. Cerrar pesta√±as innecesarias',\n",
        "            '3. Refrescar la p√°gina (F5)',\n",
        "            '4. Intentar m√°s tarde',\n",
        "            '5. Probar con otro navegador'\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    '‚ùå \"This email is already registered\"': {\n",
        "        'causas': [\n",
        "            'Ya creaste una cuenta anteriormente',\n",
        "            'Alguien m√°s usa ese email'\n",
        "        ],\n",
        "        'soluciones': [\n",
        "            '1. Intentar login en vez de registro',\n",
        "            '2. Usar \"Forgot password\" para recuperar acceso',\n",
        "            '3. Verificar con IT si es email corporativo',\n",
        "            '4. Usar email personal diferente'\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    '‚ùå Mi pa√≠s/regi√≥n no est√° soportado': {\n",
        "        'causas': [\n",
        "            'Restricciones geogr√°ficas temporales'\n",
        "        ],\n",
        "        'soluciones': [\n",
        "            '1. Databricks Free Edition est√° disponible globalmente',\n",
        "            '2. Si hay problemas, contactar soporte:',\n",
        "            '   https://help.databricks.com',\n",
        "            '3. Alternativa: Usar Databricks en Azure/AWS trial'\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "for problema, info in problemas_comunes.items():\n",
        "    print(f\"\\n{problema}\")\n",
        "    print(\"\\nCausas posibles:\")\n",
        "    for causa in info['causas']:\n",
        "        print(f\"  ‚Ä¢ {causa}\")\n",
        "    print(\"\\nSoluciones:\")\n",
        "    for solucion in info['soluciones']:\n",
        "        print(f\"  {solucion}\")\n",
        "    print()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Recursos Adicionales para Free Edition**\n",
        "\n",
        "```markdown\n",
        "üìö RECURSOS √öTILES\n",
        "\n",
        "### Documentaci√≥n Oficial\n",
        "üîó https://docs.databricks.com/free-edition/index.html\n",
        "   - Gu√≠as espec√≠ficas para Free Edition\n",
        "   - Limitaciones detalladas\n",
        "   - Mejores pr√°cticas\n",
        "\n",
        "### Community Forums\n",
        "üîó https://community.databricks.com\n",
        "   - Hacer preguntas\n",
        "   - Aprender de otros usuarios\n",
        "   - Compartir conocimiento\n",
        "\n",
        "### Databricks Academy\n",
        "üîó https://academy.databricks.com\n",
        "   - Cursos gratuitos\n",
        "   - Certificaciones\n",
        "   - Tutoriales paso a paso\n",
        "\n",
        "### GitHub - Notebooks de Ejemplo\n",
        "üîó https://github.com/databricks/databricks-notebooks\n",
        "   - Notebooks listos para importar\n",
        "   - Casos de uso reales\n",
        "   - C√≥digo de referencia\n",
        "\n",
        "### YouTube - Databricks Channel\n",
        "üîó https://www.youtube.com/@Databricks\n",
        "   - Tutoriales en video\n",
        "   - Demos de features\n",
        "   - Webinars t√©cnicos\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 3.1 - Familiarizaci√≥n**\n",
        "\n",
        "```markdown\n",
        "### Ejercicio: Explora tu Workspace\n",
        "\n",
        "**Objetivo**: Familiarizarte con la interfaz de Databricks\n",
        "\n",
        "**Tareas**:\n",
        "\n",
        "1. **Navegaci√≥n** (5 minutos)\n",
        "   - [ ] Visita cada secci√≥n del men√∫ lateral\n",
        "   - [ ] Anota qu√© ves en cada una:\n",
        "   \n",
        "   Home: _______________________\n",
        "   Workspace: _______________________\n",
        "   Compute: _______________________\n",
        "   Catalog: _______________________\n",
        "\n",
        "2. **Perfil de Usuario** (2 minutos)\n",
        "   - [ ] Haz clic en tu nombre (esquina superior derecha)\n",
        "   - [ ] Explora las opciones del men√∫ desplegable\n",
        "   - [ ] Anota qu√© opciones ves:\n",
        "   \n",
        "   _______________________\n",
        "   _______________________\n",
        "\n",
        "3. **B√∫squeda** (2 minutos)\n",
        "   - [ ] Haz clic en el icono de b√∫squeda (üîç)\n",
        "   - [ ] Escribe \"notebook\"\n",
        "   - [ ] Observa qu√© resultados aparecen\n",
        "\n",
        "4. **Quick Start** (3 minutos)\n",
        "   - [ ] En Home, observa la secci√≥n \"Quick Start\"\n",
        "   - [ ] Lee las opciones disponibles\n",
        "   - [ ] Anota cu√°les te parecen m√°s √∫tiles para empezar\n",
        "\n",
        "**Tiempo total**: ~15 minutos\n",
        "\n",
        "**Reflexi√≥n**:\n",
        "¬øQu√© secci√≥n te parece m√°s intuitiva? ¬øCu√°l menos?\n",
        "_______________________________________________________\n",
        "_______________________________________________________\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **Databricks Free Edition** (antes Community Edition) es gratuita y no requiere tarjeta\n",
        "2. **Perfecta para aprendizaje** - suficiente para este curso completo\n",
        "3. **Limitaciones aceptables** para pr√°ctica (cluster fijo, 2h inactividad)\n",
        "4. **Verificaci√≥n de email** es obligatoria\n",
        "5. **Workspace** es tu entorno de trabajo principal\n",
        "6. **Secciones principales**: Home, Workspace, Compute, Catalog\n",
        "7. **URL para Free Edition**: https://community.cloud.databricks.com\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **Guarda la URL de tu workspace**\n",
        "   - Agr√©gala a favoritos de tu navegador\n",
        "   - An√≥tala en tus apuntes\n",
        "\n",
        "‚úÖ **Usa un email personal si es posible**\n",
        "   - Los emails corporativos a veces tienen restricciones\n",
        "   - Gmail, Outlook, etc. funcionan perfecto\n",
        "\n",
        "‚úÖ **Configura 2FA (opcional pero recomendado)**\n",
        "   - Mayor seguridad para tu cuenta\n",
        "   - Settings ‚Üí Security\n",
        "\n",
        "‚úÖ **√önete al Community Forum**\n",
        "   - Fuente invaluable de ayuda\n",
        "   - Respuestas r√°pidas de la comunidad\n",
        "\n",
        "‚úÖ **No te preocupes por las limitaciones**\n",
        "   - Free Edition tiene todo lo que necesitas para aprender\n",
        "   - Cuando necesites m√°s, upgrade ser√° natural\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el siguiente punto **3.2 Navegaci√≥n por la interfaz de usuario**? üöÄ"
      ],
      "metadata": {
        "id": "92-18590Zm6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2 Navegaci√≥n por la interfaz de usuario**\n",
        "\n",
        "#### **Introducci√≥n: Tu Mapa del Workspace**\n",
        "\n",
        "Ahora que tienes tu cuenta creada, es momento de conocer a fondo la interfaz de Databricks. Piensa en esto como aprender a conducir: primero necesitas saber d√≥nde est√°n los controles antes de arrancar el motor.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ANATOM√çA DE LA INTERFAZ DE DATABRICKS\n",
        "\"\"\"\n",
        "\n",
        "interfaz_componentes = {\n",
        "    'Barra Superior': 'Navegaci√≥n global y acciones r√°pidas',\n",
        "    'Sidebar (Men√∫ Lateral)': 'Navegaci√≥n principal entre secciones',\n",
        "    '√Årea Principal': 'Contenido de la secci√≥n actual',\n",
        "    'Panel Contextual': 'Informaci√≥n y acciones espec√≠ficas (aparece seg√∫n contexto)'\n",
        "}\n",
        "\n",
        "print(\"üó∫Ô∏è COMPONENTES DE LA INTERFAZ\")\n",
        "print(\"=\"*60)\n",
        "for componente, descripcion in interfaz_componentes.items():\n",
        "    print(f\"{componente}: {descripcion}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 1: La Barra Superior (Top Bar)**\n",
        "\n",
        "#### **Elementos de la Barra Superior**\n",
        "\n",
        "```markdown\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  üè† Databricks | üîç Search | üì• Create | üîî | üë§ Tu Nombre‚ñº ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "ELEMENTOS DE IZQUIERDA A DERECHA:\n",
        "```\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "BARRA SUPERIOR - DESGLOSE\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîù BARRA SUPERIOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "elementos_barra = {\n",
        "    'üè† Logo Databricks': {\n",
        "        'funci√≥n': 'Volver a Home desde cualquier lugar',\n",
        "        'acci√≥n': 'Click ‚Üí Redirige a p√°gina principal',\n",
        "        'uso': 'Resetear navegaci√≥n r√°pidamente'\n",
        "    },\n",
        "    \n",
        "    'üîç Search (B√∫squeda)': {\n",
        "        'funci√≥n': 'Buscar en todo el workspace',\n",
        "        'acci√≥n': 'Click o Ctrl+K (Cmd+K en Mac)',\n",
        "        'busca': [\n",
        "            'Notebooks',\n",
        "            'Tablas',\n",
        "            'Clusters',\n",
        "            'Jobs',\n",
        "            'Usuarios',\n",
        "            'Comandos'\n",
        "        ],\n",
        "        'uso': 'Encontrar recursos r√°pidamente',\n",
        "        'tip': 'Usa @ para buscar usuarios, # para notebooks'\n",
        "    },\n",
        "    \n",
        "    'üì• Create (Crear)': {\n",
        "        'funci√≥n': 'Acceso r√°pido para crear recursos',\n",
        "        'opciones': [\n",
        "            'Notebook',\n",
        "            'Folder',\n",
        "            'Library',\n",
        "            'MLflow Experiment',\n",
        "            'Table'\n",
        "        ],\n",
        "        'uso': 'Crear nuevos recursos sin navegar',\n",
        "        'atajo': 'Disponible desde cualquier secci√≥n'\n",
        "    },\n",
        "    \n",
        "    'üîî Notifications': {\n",
        "        'funci√≥n': 'Notificaciones del sistema',\n",
        "        'muestra': [\n",
        "            'Jobs completados/fallidos',\n",
        "            'Colaboradores editando notebooks',\n",
        "            'Cambios importantes del sistema',\n",
        "            'Actualizaciones de features'\n",
        "        ],\n",
        "        'uso': 'Mantenerse informado de eventos'\n",
        "    },\n",
        "    \n",
        "    '‚ùì Help': {\n",
        "        'funci√≥n': 'Ayuda y documentaci√≥n',\n",
        "        'opciones': [\n",
        "            'Documentation',\n",
        "            'Release notes',\n",
        "            'Databricks University',\n",
        "            'Community forum',\n",
        "            'Support'\n",
        "        ],\n",
        "        'uso': 'Acceso r√°pido a recursos de ayuda'\n",
        "    },\n",
        "    \n",
        "    'üë§ User Menu (Perfil)': {\n",
        "        'funci√≥n': 'Configuraci√≥n de usuario y cuenta',\n",
        "        'opciones': [\n",
        "            'User Settings',\n",
        "            'Admin Console',\n",
        "            'Billing (en versiones pagas)',\n",
        "            'Sign Out'\n",
        "        ],\n",
        "        'uso': 'Gestionar tu cuenta y preferencias'\n",
        "    }\n",
        "}\n",
        "\n",
        "for elemento, info in elementos_barra.items():\n",
        "    print(f\"\\n{elemento}\")\n",
        "    print(f\"  Funci√≥n: {info['funci√≥n']}\")\n",
        "    if 'opciones' in info:\n",
        "        print(\"  Opciones:\")\n",
        "        for opcion in info['opciones']:\n",
        "            print(f\"    ‚Ä¢ {opcion}\")\n",
        "    if 'busca' in info:\n",
        "        print(\"  Busca:\")\n",
        "        for item in info['busca']:\n",
        "            print(f\"    ‚Ä¢ {item}\")\n",
        "    if 'tip' in info:\n",
        "        print(f\"  üí° Tip: {info['tip']}\")\n",
        "    print(f\"  Uso: {info['uso']}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 2: El Sidebar (Men√∫ Lateral)**\n",
        "\n",
        "#### **Navegaci√≥n Principal**\n",
        "\n",
        "```markdown\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  SIDEBAR                ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                         ‚îÇ\n",
        "‚îÇ  üè† Home                ‚îÇ\n",
        "‚îÇ  üìÅ Workspace           ‚îÇ\n",
        "‚îÇ  üî¨ Data Science        ‚îÇ  ‚Üê En algunas versiones\n",
        "‚îÇ  üíº Data Engineering    ‚îÇ  ‚Üê En algunas versiones\n",
        "‚îÇ  üìä SQL                 ‚îÇ\n",
        "‚îÇ  ‚öôÔ∏è  Compute            ‚îÇ\n",
        "‚îÇ  üì¶ Catalog             ‚îÇ\n",
        "‚îÇ  üìà Dashboards          ‚îÇ\n",
        "‚îÇ  ‚ö° Workflows            ‚îÇ\n",
        "‚îÇ  ‚öôÔ∏è  Settings           ‚îÇ\n",
        "‚îÇ                         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "NOTA: El sidebar puede variar ligeramente seg√∫n:\n",
        "- Versi√≥n de Databricks (Free vs Premium)\n",
        "- Configuraci√≥n del workspace\n",
        "- Permisos del usuario\n",
        "```\n",
        "\n",
        "#### **Exploraci√≥n Detallada de Cada Secci√≥n**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "SECCIONES DEL SIDEBAR - GU√çA COMPLETA\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìç SECCIONES DEL SIDEBAR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== HOME =====\n",
        "print(\"\\nüè† HOME\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "home_info = \"\"\"\n",
        "QU√â ES:\n",
        "  P√°gina de inicio personalizada con acceso r√°pido a recursos\n",
        "\n",
        "QU√â VER√ÅS:\n",
        "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "  ‚îÇ  Quick actions:                     ‚îÇ\n",
        "  ‚îÇ  ‚Ä¢ Create notebook                  ‚îÇ\n",
        "  ‚îÇ  ‚Ä¢ Create cluster                   ‚îÇ\n",
        "  ‚îÇ  ‚Ä¢ Import data                      ‚îÇ\n",
        "  ‚îÇ                                     ‚îÇ\n",
        "  ‚îÇ  Recent items:                      ‚îÇ\n",
        "  ‚îÇ  ‚Ä¢ Notebooks recientes              ‚îÇ\n",
        "  ‚îÇ  ‚Ä¢ Clusters usados                  ‚îÇ\n",
        "  ‚îÇ  ‚Ä¢ Queries ejecutados               ‚îÇ\n",
        "  ‚îÇ                                     ‚îÇ\n",
        "  ‚îÇ  Learning resources:                ‚îÇ\n",
        "  ‚îÇ  ‚Ä¢ Tutorials                        ‚îÇ\n",
        "  ‚îÇ  ‚Ä¢ Sample notebooks                 ‚îÇ\n",
        "  ‚îÇ  ‚Ä¢ Documentation links              ‚îÇ\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "CU√ÅNDO USAR:\n",
        "  ‚úÖ Punto de partida cada sesi√≥n\n",
        "  ‚úÖ Acceso r√°pido a trabajos recientes\n",
        "  ‚úÖ Comenzar nuevo proyecto\n",
        "\"\"\"\n",
        "print(home_info)\n",
        "\n",
        "# ===== WORKSPACE =====\n",
        "print(\"\\nüìÅ WORKSPACE\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "workspace_info = \"\"\"\n",
        "QU√â ES:\n",
        "  Sistema de archivos jer√°rquico para organizar notebooks\n",
        "\n",
        "ESTRUCTURA:\n",
        "  Workspace\n",
        "  ‚îú‚îÄ‚îÄ Users\n",
        "  ‚îÇ   ‚îî‚îÄ‚îÄ tu-email@example.com\n",
        "  ‚îÇ       ‚îú‚îÄ‚îÄ üìì Mi Primer Notebook\n",
        "  ‚îÇ       ‚îú‚îÄ‚îÄ üìÇ Proyecto 1\n",
        "  ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìì An√°lisis\n",
        "  ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ üìì Visualizaci√≥n\n",
        "  ‚îÇ       ‚îî‚îÄ‚îÄ üìÇ Curso Databricks\n",
        "  ‚îÇ           ‚îî‚îÄ‚îÄ üìì Pr√°ctica 1\n",
        "  ‚îú‚îÄ‚îÄ Shared  (si tienes acceso)\n",
        "  ‚îî‚îÄ‚îÄ Repos   (integraci√≥n Git)\n",
        "\n",
        "ACCIONES DISPONIBLES:\n",
        "  ‚Ä¢ Crear nuevo notebook\n",
        "  ‚Ä¢ Crear carpeta\n",
        "  ‚Ä¢ Importar notebook (.dbc, .ipynb, .html)\n",
        "  ‚Ä¢ Exportar notebook\n",
        "  ‚Ä¢ Mover/copiar/eliminar\n",
        "  ‚Ä¢ Compartir (en versiones premium)\n",
        "\n",
        "TIPOS DE ARCHIVOS:\n",
        "  üìì Notebook - Archivo ejecutable (.py, .sql, .scala, .r)\n",
        "  üìÇ Folder - Carpeta para organizar\n",
        "  üìö Library - C√≥digo reutilizable\n",
        "  üìã Dashboard - Visualizaci√≥n compartida\n",
        "\n",
        "CU√ÅNDO USAR:\n",
        "  ‚úÖ Organizar tu trabajo\n",
        "  ‚úÖ Encontrar notebooks antiguos\n",
        "  ‚úÖ Importar/exportar notebooks\n",
        "  ‚úÖ Navegar entre proyectos\n",
        "\"\"\"\n",
        "print(workspace_info)\n",
        "\n",
        "# ===== COMPUTE =====\n",
        "print(\"\\n‚öôÔ∏è  COMPUTE\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "compute_info = \"\"\"\n",
        "QU√â ES:\n",
        "  Gesti√≥n de clusters (recursos computacionales)\n",
        "\n",
        "QU√â VER√ÅS:\n",
        "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "  ‚îÇ  [ + Create Cluster ]                           ‚îÇ\n",
        "  ‚îÇ                                                 ‚îÇ\n",
        "  ‚îÇ  Active Clusters:                               ‚îÇ\n",
        "  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n",
        "  ‚îÇ  ‚îÇ my-cluster-1        ‚óèRunning  |Edit|Stop‚îÇ‚îÇ   ‚îÇ\n",
        "  ‚îÇ  ‚îÇ Type: All-Purpose                       ‚îÇ‚îÇ   ‚îÇ\n",
        "  ‚îÇ  ‚îÇ Workers: 1 | Driver: 15 GB RAM          ‚îÇ‚îÇ   ‚îÇ\n",
        "  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
        "  ‚îÇ                                                 ‚îÇ\n",
        "  ‚îÇ  Terminated Clusters:                           ‚îÇ\n",
        "  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n",
        "  ‚îÇ  ‚îÇ old-cluster         ‚óãTerminated |Start|  ‚îÇ   ‚îÇ\n",
        "  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "ESTADOS DE CLUSTER:\n",
        "  üü¢ Running   - Activo y listo para usar\n",
        "  üü° Pending   - Arrancando (1-5 minutos)\n",
        "  üî¥ Terminated - Apagado (no consume recursos)\n",
        "  ‚ö™ Restarting - Reiniciando\n",
        "\n",
        "ACCIONES:\n",
        "  ‚Ä¢ Create - Crear nuevo cluster\n",
        "  ‚Ä¢ Start - Arrancar cluster terminado\n",
        "  ‚Ä¢ Stop - Detener cluster activo\n",
        "  ‚Ä¢ Restart - Reiniciar cluster\n",
        "  ‚Ä¢ Edit - Modificar configuraci√≥n\n",
        "  ‚Ä¢ Delete - Eliminar cluster permanentemente\n",
        "\n",
        "EN FREE EDITION:\n",
        "  ‚ö†Ô∏è Solo 1 cluster a la vez\n",
        "  ‚ö†Ô∏è Configuraci√≥n fija (~15 GB RAM)\n",
        "  ‚ö†Ô∏è Auto-termina despu√©s de 2h inactividad\n",
        "\n",
        "CU√ÅNDO USAR:\n",
        "  ‚úÖ Antes de ejecutar notebooks (necesitas cluster activo)\n",
        "  ‚úÖ Verificar estado de recursos\n",
        "  ‚úÖ Detener clusters para liberar recursos\n",
        "\"\"\"\n",
        "print(compute_info)\n",
        "\n",
        "# ===== CATALOG =====\n",
        "print(\"\\nüì¶ CATALOG\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "catalog_info = \"\"\"\n",
        "QU√â ES:\n",
        "  Explorador de datos (databases, tablas, views)\n",
        "\n",
        "ESTRUCTURA:\n",
        "  üì¶ Catalog\n",
        "  ‚îî‚îÄ‚îÄ üìä default (database)\n",
        "      ‚îú‚îÄ‚îÄ üìã tabla_clientes\n",
        "      ‚îÇ   ‚îú‚îÄ‚îÄ üîπ id (int)\n",
        "      ‚îÇ   ‚îú‚îÄ‚îÄ üîπ nombre (string)\n",
        "      ‚îÇ   ‚îî‚îÄ‚îÄ üîπ ciudad (string)\n",
        "      ‚îú‚îÄ‚îÄ üìã tabla_ventas\n",
        "      ‚îî‚îÄ‚îÄ üëÅÔ∏è vista_resumen\n",
        "\n",
        "QU√â VER√ÅS:\n",
        "  ‚Ä¢ Databases (esquemas)\n",
        "  ‚Ä¢ Tables (tablas f√≠sicas)\n",
        "  ‚Ä¢ Views (vistas SQL)\n",
        "  ‚Ä¢ Schema de cada tabla (columnas, tipos)\n",
        "  ‚Ä¢ Sample data (primeras filas)\n",
        "  ‚Ä¢ Metadata (tama√±o, ubicaci√≥n, propietario)\n",
        "\n",
        "ACCIONES:\n",
        "  ‚Ä¢ Explorar esquema\n",
        "  ‚Ä¢ Ver datos de ejemplo\n",
        "  ‚Ä¢ Crear tabla desde UI\n",
        "  ‚Ä¢ Ejecutar query directamente\n",
        "  ‚Ä¢ Ver lineage (dependencias)\n",
        "\n",
        "CU√ÅNDO USAR:\n",
        "  ‚úÖ Descubrir qu√© datos tienes\n",
        "  ‚úÖ Verificar estructura de tablas\n",
        "  ‚úÖ Explorar antes de escribir c√≥digo\n",
        "  ‚úÖ Validar que tu ETL funcion√≥\n",
        "\"\"\"\n",
        "print(catalog_info)\n",
        "\n",
        "# ===== SQL =====\n",
        "print(\"\\nüìä SQL\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "sql_info = \"\"\"\n",
        "QU√â ES:\n",
        "  Editor SQL para an√°lisis de datos sin c√≥digo\n",
        "\n",
        "QU√â VER√ÅS:\n",
        "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "  ‚îÇ  [ + New Query ]                          ‚îÇ\n",
        "  ‚îÇ                                           ‚îÇ\n",
        "  ‚îÇ  Query Editor:                            ‚îÇ\n",
        "  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
        "  ‚îÇ  ‚îÇ SELECT * FROM ventas                ‚îÇ  ‚îÇ\n",
        "  ‚îÇ  ‚îÇ WHERE fecha >= '2025-01-01'         ‚îÇ  ‚îÇ\n",
        "  ‚îÇ  ‚îÇ                                     ‚îÇ  ‚îÇ\n",
        "  ‚îÇ  ‚îÇ [‚ñ∂ Run]  [üíæ Save]  [üìä Visualize]‚îÇ ‚îÇ  ‚îÇ\n",
        "  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
        "  ‚îÇ                                           ‚îÇ\n",
        "  ‚îÇ  Results:                                 ‚îÇ\n",
        "  ‚îÇ  [Tabla con resultados]                   ‚îÇ\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "FEATURES:\n",
        "  ‚Ä¢ Editor SQL con syntax highlighting\n",
        "  ‚Ä¢ Autocompletado de tablas/columnas\n",
        "  ‚Ä¢ Ejecuci√≥n de queries\n",
        "  ‚Ä¢ Visualizaciones integradas\n",
        "  ‚Ä¢ Guardar queries frecuentes\n",
        "  ‚Ä¢ Compartir resultados\n",
        "\n",
        "‚ö†Ô∏è EN FREE EDITION:\n",
        "  Funcionalidad SQL limitada\n",
        "  Para full SQL features ‚Üí usa notebooks con %sql\n",
        "\n",
        "CU√ÅNDO USAR:\n",
        "  ‚úÖ Queries SQL r√°pidas\n",
        "  ‚úÖ An√°lisis ad-hoc\n",
        "  ‚úÖ Exploraci√≥n de datos\n",
        "\"\"\"\n",
        "print(sql_info)\n",
        "\n",
        "# ===== WORKFLOWS =====\n",
        "print(\"\\n‚ö° WORKFLOWS (JOBS)\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "workflows_info = \"\"\"\n",
        "QU√â ES:\n",
        "  Automatizaci√≥n de notebooks (scheduling)\n",
        "\n",
        "‚ö†Ô∏è EN FREE EDITION:\n",
        "  ‚ùå NO disponible\n",
        "  Esta feature solo est√° en versiones de pago\n",
        "\n",
        "EN VERSIONES PREMIUM:\n",
        "  ‚Ä¢ Programar ejecuci√≥n de notebooks\n",
        "  ‚Ä¢ Crear pipelines de datos\n",
        "  ‚Ä¢ Monitorear ejecuciones\n",
        "  ‚Ä¢ Alertas de fallos\n",
        "  ‚Ä¢ Orquestaci√≥n compleja\n",
        "\n",
        "ALTERNATIVA EN FREE EDITION:\n",
        "  Ejecutar notebooks manualmente\n",
        "  (Suficiente para aprendizaje)\n",
        "\"\"\"\n",
        "print(workflows_info)\n",
        "\n",
        "# ===== SETTINGS =====\n",
        "print(\"\\n‚öôÔ∏è  SETTINGS\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "settings_info = \"\"\"\n",
        "QU√â ES:\n",
        "  Configuraci√≥n de usuario y workspace\n",
        "\n",
        "SECCIONES PRINCIPALES:\n",
        "\n",
        "1Ô∏è‚É£ USER SETTINGS\n",
        "  ‚Ä¢ Profile (nombre, email, foto)\n",
        "  ‚Ä¢ Access tokens (para API)\n",
        "  ‚Ä¢ Git integration\n",
        "  ‚Ä¢ Notifications preferences\n",
        "\n",
        "2Ô∏è‚É£ DEVELOPER\n",
        "  ‚Ä¢ Access tokens\n",
        "  ‚Ä¢ Git credentials\n",
        "  ‚Ä¢ API documentation\n",
        "\n",
        "3Ô∏è‚É£ ADMIN CONSOLE (si eres admin)\n",
        "  ‚Ä¢ Workspace settings\n",
        "  ‚Ä¢ Users management\n",
        "  ‚Ä¢ Cluster policies\n",
        "  ‚Ä¢ Security\n",
        "\n",
        "EN FREE EDITION:\n",
        "  ‚úÖ User settings completos\n",
        "  ‚úÖ Git integration\n",
        "  ‚úÖ Access tokens\n",
        "  ‚ùå Admin console limitado\n",
        "\n",
        "CU√ÅNDO USAR:\n",
        "  ‚úÖ Conectar con GitHub\n",
        "  ‚úÖ Generar tokens para API\n",
        "  ‚úÖ Cambiar preferencias\n",
        "  ‚úÖ Configurar notificaciones\n",
        "\"\"\"\n",
        "print(settings_info)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 3: √Årea Principal (Main Content Area)**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "√ÅREA PRINCIPAL - COMPORTAMIENTO DIN√ÅMICO\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüì∫ √ÅREA PRINCIPAL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "area_principal_info = \"\"\"\n",
        "El contenido del √°rea principal CAMBIA seg√∫n la secci√≥n del\n",
        "sidebar que tengas seleccionada.\n",
        "\n",
        "EJEMPLOS:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Si est√°s en HOME:                                       ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ ‚Ä¢ Quick actions                                         ‚îÇ\n",
        "‚îÇ ‚Ä¢ Recent items                                          ‚îÇ\n",
        "‚îÇ ‚Ä¢ Learning resources                                    ‚îÇ\n",
        "‚îÇ ‚Ä¢ Getting started guides                                ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Si est√°s en WORKSPACE:                                  ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ ‚Ä¢ √Årbol de carpetas y notebooks                         ‚îÇ\n",
        "‚îÇ ‚Ä¢ Breadcrumbs (ruta actual)                             ‚îÇ\n",
        "‚îÇ ‚Ä¢ Botones de acci√≥n (Create, Import, etc.)              ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Si est√°s en COMPUTE:                                    ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ ‚Ä¢ Lista de clusters                                     ‚îÇ\n",
        "‚îÇ ‚Ä¢ Estado de cada cluster                                ‚îÇ\n",
        "‚îÇ ‚Ä¢ Botones de control (Start/Stop)                       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Si abres un NOTEBOOK:                                   ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ ‚Ä¢ Editor de c√≥digo                                      ‚îÇ\n",
        "‚îÇ ‚Ä¢ Celdas ejecutables                                    ‚îÇ\n",
        "‚îÇ ‚Ä¢ Resultados inline                                     ‚îÇ\n",
        "‚îÇ ‚Ä¢ Toolbar del notebook                                  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "üí° PRINCIPIO:\n",
        "   Sidebar = Navegaci√≥n ENTRE secciones\n",
        "   √Årea Principal = Contenido DE la secci√≥n\n",
        "\"\"\"\n",
        "\n",
        "print(area_principal_info)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 4: Atajos de Teclado Esenciales**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ATAJOS DE TECLADO - PRODUCTIVIDAD\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n‚å®Ô∏è ATAJOS DE TECLADO ESENCIALES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "atajos = {\n",
        "    'NAVEGACI√ìN': {\n",
        "        'Ctrl+K (Cmd+K)': 'Abrir b√∫squeda r√°pida',\n",
        "        'Ctrl+N (Cmd+N)': 'Crear nuevo notebook',\n",
        "        'Ctrl+O (Cmd+O)': 'Abrir notebook existente',\n",
        "        'Esc': 'Cerrar di√°logos/modales'\n",
        "    },\n",
        "    \n",
        "    'EN NOTEBOOKS (Veremos m√°s adelante)': {\n",
        "        'Shift+Enter': 'Ejecutar celda y avanzar',\n",
        "        'Ctrl+Enter': 'Ejecutar celda sin avanzar',\n",
        "        'Ctrl+/': 'Comentar/descomentar l√≠nea',\n",
        "        'Tab': 'Autocompletar c√≥digo',\n",
        "        'Shift+Tab': 'Ver documentaci√≥n de funci√≥n'\n",
        "    },\n",
        "    \n",
        "    'GENERALES': {\n",
        "        'Ctrl+S (Cmd+S)': 'Guardar (en notebooks)',\n",
        "        'Ctrl+Z (Cmd+Z)': 'Deshacer',\n",
        "        'Ctrl+Shift+Z': 'Rehacer',\n",
        "        'F11': 'Pantalla completa'\n",
        "    }\n",
        "}\n",
        "\n",
        "for categoria, shortcuts in atajos.items():\n",
        "    print(f\"\\n{categoria}:\")\n",
        "    for atajo, descripcion in shortcuts.items():\n",
        "        print(f\"  {atajo:<20} ‚Üí {descripcion}\")\n",
        "\n",
        "print(\"\"\"\n",
        "\\nüí° TIP PROFESIONAL:\n",
        "   Practica los atajos gradualmente\n",
        "   Ctrl+K (b√∫squeda) es el m√°s √∫til - memor√≠zalo primero\n",
        "\"\"\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 5: Breadcrumbs y Navegaci√≥n Contextual**\n",
        "\n",
        "```markdown\n",
        "üçû BREADCRUMBS (MIGAS DE PAN)\n",
        "\n",
        "Databricks usa breadcrumbs para mostrar tu ubicaci√≥n:\n",
        "\n",
        "Ejemplo en Workspace:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Workspace > Users > tu-email > Curso > üìì Notebook 1   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "     ‚ñ≤         ‚ñ≤          ‚ñ≤         ‚ñ≤           ‚ñ≤\n",
        "   Ra√≠z    Secci√≥n    Tu folder  Proyecto   Archivo\n",
        "\n",
        "FUNCIONALIDAD:\n",
        "‚Ä¢ Click en cualquier nivel ‚Üí Navega a esa ubicaci√≥n\n",
        "‚Ä¢ Ver jerarqu√≠a completa\n",
        "‚Ä¢ Regresar a niveles superiores r√°pidamente\n",
        "\n",
        "EJEMPLO DE USO:\n",
        "\n",
        "Est√°s editando: Workspace > Users > email > Proyecto > Notebook1\n",
        "Quieres ver otros notebooks del proyecto:\n",
        "‚Üí Click en \"Proyecto\" en breadcrumbs\n",
        "‚Üí Ves lista de todos los notebooks\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica Guiada - Tour Completo**\n",
        "\n",
        "```markdown\n",
        "üß™ EJERCICIO: TOUR COMPLETO DE LA INTERFAZ\n",
        "\n",
        "Vamos a hacer un recorrido sistem√°tico por TODA la interfaz.\n",
        "Sigue estos pasos en tu Databricks:\n",
        "\n",
        "### PARTE 1: BARRA SUPERIOR (5 minutos)\n",
        "\n",
        "1. **B√∫squeda**\n",
        "   - [ ] Click en üîç o presiona Ctrl+K\n",
        "   - [ ] Escribe \"cluster\"\n",
        "   - [ ] Observa los resultados\n",
        "   - [ ] Presiona Esc para cerrar\n",
        "\n",
        "2. **Create**\n",
        "   - [ ] Click en \"Create\" (si est√° disponible)\n",
        "   - [ ] Ve qu√© opciones aparecen\n",
        "   - [ ] NO crees nada a√∫n, solo explora\n",
        "   - [ ] Click fuera para cerrar\n",
        "\n",
        "3. **Notificaciones**\n",
        "   - [ ] Click en üîî\n",
        "   - [ ] Probablemente est√© vac√≠o (cuenta nueva)\n",
        "   - [ ] Cierra el panel\n",
        "\n",
        "4. **Help**\n",
        "   - [ ] Click en ‚ùì\n",
        "   - [ ] Explora las opciones disponibles\n",
        "   - [ ] Click en \"Documentation\"\n",
        "   - [ ] Regresa a tu workspace\n",
        "\n",
        "5. **User Menu**\n",
        "   - [ ] Click en tu nombre/foto\n",
        "   - [ ] Anota las opciones que ves:\n",
        "     _______________________________\n",
        "     _______________________________\n",
        "   - [ ] Click en \"User Settings\"\n",
        "   - [ ] Explora las pesta√±as disponibles\n",
        "   - [ ] Regresa a Home\n",
        "\n",
        "### PARTE 2: SIDEBAR (10 minutos)\n",
        "\n",
        "6. **Home**\n",
        "   - [ ] Click en üè† Home\n",
        "   - [ ] Anota qu√© ves en \"Quick actions\":\n",
        "     _______________________________\n",
        "     _______________________________\n",
        "\n",
        "7. **Workspace**\n",
        "   - [ ] Click en üìÅ Workspace\n",
        "   - [ ] Navega a Users > tu-email\n",
        "   - [ ] Observa la estructura de carpetas\n",
        "   - [ ] Anota qu√© hay por defecto:\n",
        "     _______________________________\n",
        "\n",
        "8. **Compute**\n",
        "   - [ ] Click en ‚öôÔ∏è Compute\n",
        "   - [ ] Anota qu√© clusters ves (probablemente ninguno a√∫n):\n",
        "     _______________________________\n",
        "   - [ ] Observa el bot√≥n \"+ Create Cluster\"\n",
        "\n",
        "9. **Catalog**\n",
        "   - [ ] Click en üì¶ Catalog\n",
        "   - [ ] Expande \"hive_metastore\"\n",
        "   - [ ] Expande \"default\" database\n",
        "   - [ ] Anota qu√© tablas ves (si alguna):\n",
        "     _______________________________\n",
        "\n",
        "10. **SQL**\n",
        "    - [ ] Click en üìä SQL\n",
        "    - [ ] Observa el editor SQL\n",
        "    - [ ] NO ejecutes nada a√∫n\n",
        "\n",
        "11. **Workflows**\n",
        "    - [ ] Click en ‚ö° Workflows (si disponible)\n",
        "    - [ ] Probablemente ver√°s mensaje de \"No jobs\"\n",
        "    - [ ] Est√° bien, esto es normal\n",
        "\n",
        "12. **Settings**\n",
        "    - [ ] Click en ‚öôÔ∏è Settings\n",
        "    - [ ] Explora las pesta√±as:\n",
        "       - [ ] User Settings\n",
        "       - [ ] Developer\n",
        "    - [ ] En Developer, ver√°s \"Access Tokens\" (lo usaremos despu√©s)\n",
        "\n",
        "### PARTE 3: PRUEBA DE ATAJOS (3 minutos)\n",
        "\n",
        "13. **B√∫squeda r√°pida**\n",
        "    - [ ] Presiona Ctrl+K (Cmd+K en Mac)\n",
        "    - [ ] Escribe \"home\"\n",
        "    - [ ] Presiona Enter\n",
        "    - [ ] Deber√≠as volver a Home\n",
        "\n",
        "14. **Navegaci√≥n**\n",
        "    - [ ] Presiona Ctrl+K de nuevo\n",
        "    - [ ] Escribe \"workspace\"\n",
        "    - [ ] Enter\n",
        "    - [ ] Deber√≠as ir a Workspace\n",
        "\n",
        "### PARTE 4: REFLEXI√ìN (2 minutos)\n",
        "\n",
        "Responde en tus apuntes:\n",
        "\n",
        "1. ¬øQu√© secci√≥n te parece m√°s √∫til?\n",
        "   _______________________________\n",
        "\n",
        "2. ¬øCu√°l te confunde m√°s?\n",
        "   _______________________________\n",
        "\n",
        "3. ¬øEncontraste algo que no esperabas?\n",
        "   _______________________________\n",
        "\n",
        "4. ¬øQu√© atajo de teclado probar√°s primero?\n",
        "   _______________________________\n",
        "\n",
        "**TIEMPO TOTAL**: ~20 minutos\n",
        "\n",
        "‚úÖ Al completar este ejercicio, habr√°s explorado el 100%\n",
        "   de la interfaz principal de Databricks\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Diferencias de Interfaz: Free vs Premium**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "COMPARACI√ìN DE INTERFAZ\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüÜö FREE EDITION vs PREMIUM - INTERFAZ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "diferencias_interfaz = {\n",
        "    'Sidebar - Secciones visibles': {\n",
        "        'Free': [\n",
        "            'Home', 'Workspace', 'Compute',\n",
        "            'Catalog', 'SQL (limitado)', 'Settings'\n",
        "        ],\n",
        "        'Premium': [\n",
        "            'Todo de Free +',\n",
        "            'Data Science', 'Data Engineering',\n",
        "            'Machine Learning', 'Workflows completo',\n",
        "            'Dashboards avanzados', 'Admin Console'\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    'Barra Superior': {\n",
        "        'Free': [\n",
        "            'Logo', 'Search', 'Notifications',\n",
        "            'Help', 'User menu'\n",
        "        ],\n",
        "        'Premium': [\n",
        "            'Todo de Free +',\n",
        "            'Create (m√°s opciones)',\n",
        "            'Workspace switcher (m√∫ltiples workspaces)'\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    'Compute': {\n",
        "        'Free': [\n",
        "            'Ver clusters', 'Crear 1 cluster fijo',\n",
        "            'Start/Stop b√°sico'\n",
        "        ],\n",
        "        'Premium': [\n",
        "            'Todo de Free +',\n",
        "            'Configuraci√≥n avanzada',\n",
        "            'M√∫ltiples clusters',\n",
        "            'Cluster policies',\n",
        "            'Pools'\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    'Workspace': {\n",
        "        'Free': [\n",
        "            'Crear notebooks', 'Organizar carpetas',\n",
        "            'Importar/Exportar'\n",
        "        ],\n",
        "        'Premium': [\n",
        "            'Todo de Free +',\n",
        "            'Compartir con equipo',\n",
        "            'Permisos granulares',\n",
        "            'Repos avanzados (m√∫ltiples Git)'\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "for seccion, comparacion in diferencias_interfaz.items():\n",
        "    print(f\"\\n{seccion}:\")\n",
        "    print(f\"  Free Edition:\")\n",
        "    for item in comparacion['Free']:\n",
        "        print(f\"    ‚Ä¢ {item}\")\n",
        "    print(f\"  Premium:\")\n",
        "    for item in comparacion['Premium']:\n",
        "        print(f\"    ‚Ä¢ {item}\")\n",
        "\n",
        "print(\"\"\"\n",
        "\\nüí° CONCLUSI√ìN:\n",
        "   Free Edition tiene TODO lo necesario para aprender\n",
        "   La interfaz es pr√°cticamente igual\n",
        "   Las diferencias son en features avanzadas de producci√≥n\n",
        "\"\"\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 3.2 - Mapa Mental**\n",
        "\n",
        "```markdown\n",
        "### Ejercicio: Crea tu Mapa Mental de Databricks\n",
        "\n",
        "**Objetivo**: Crear un mapa mental de la interfaz para referencia futura\n",
        "\n",
        "**Instrucciones**:\n",
        "\n",
        "Dibuja (en papel o digital) un mapa mental con:\n",
        "\n",
        "1. **Centro**: \"Databricks UI\"\n",
        "\n",
        "2. **Ramas Principales**:\n",
        "   - Barra Superior\n",
        "   - Sidebar\n",
        "   - √Årea Principal\n",
        "\n",
        "3. **Sub-ramas para cada secci√≥n**:\n",
        "\n",
        "   Ejemplo para Sidebar:\n",
        "   ```\n",
        "   Sidebar ‚îÄ‚î¨‚îÄ Home ‚îÄ‚îÄ‚îÄ‚îÄ Quick actions\n",
        "            ‚îú‚îÄ Workspace ‚îÄ‚îÄ‚î¨‚îÄ Notebooks\n",
        "            ‚îÇ              ‚îî‚îÄ Folders\n",
        "            ‚îú‚îÄ Compute ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Clusters\n",
        "            ‚îú‚îÄ Catalog ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Tables/DBs\n",
        "            ‚îî‚îÄ Settings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Config\n",
        "   ```\n",
        "\n",
        "4. **A√±ade √≠conos o colores**:\n",
        "   - Verde: Secciones que usar√°s frecuentemente\n",
        "   - Amarillo: Secciones importantes pero ocasionales\n",
        "   - Rojo: Secciones avanzadas (para despu√©s)\n",
        "\n",
        "5. **Agrega notas**:\n",
        "   - Atajos de teclado principales\n",
        "   - Workflows t√≠picos\n",
        "   - Recordatorios personales\n",
        "\n",
        "**Resultado esperado**:\n",
        "Un diagrama visual de referencia r√°pida que puedas consultar\n",
        "mientras aprendes.\n",
        "\n",
        "**Tiempo estimado**: 15-20 minutos\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **Barra Superior** = Navegaci√≥n global y acciones r√°pidas\n",
        "2. **Sidebar** = Navegaci√≥n entre secciones principales\n",
        "3. **√Årea Principal** = Contenido din√°mico de la secci√≥n actual\n",
        "4. **Ctrl+K** es tu mejor amigo (b√∫squeda r√°pida universal)\n",
        "5. **Breadcrumbs** muestran tu ubicaci√≥n y permiten navegaci√≥n r√°pida\n",
        "6. **Home** es tu punto de partida cada sesi√≥n\n",
        "7. **Workspace** es donde vive tu trabajo (notebooks, carpetas)\n",
        "8. **Compute** gestiona recursos (clusters)\n",
        "9. **Catalog** explora datos disponibles\n",
        "10. **Settings** configura tu entorno\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **Personaliza tu workspace**\n",
        "```markdown\n",
        "- Crea estructura de carpetas desde el inicio\n",
        "- Ejemplo: /Curso, /Pr√°ctica, /Proyectos\n",
        "- Te ahorrar√°s tiempo despu√©s\n",
        "```\n",
        "\n",
        "‚úÖ **Usa b√∫squeda (Ctrl+K) en vez de navegar**\n",
        "```markdown\n",
        "M√°s r√°pido buscar \"mi notebook\" que:\n",
        "Home ‚Üí Workspace ‚Üí Users ‚Üí email ‚Üí carpeta ‚Üí notebook\n",
        "```\n",
        "\n",
        "‚úÖ **Mant√©n pesta√±as organizadas**\n",
        "```markdown\n",
        "Databricks abre nuevas pesta√±as del navegador\n",
        "Cierra las que no uses para evitar confusi√≥n\n",
        "```\n",
        "\n",
        "‚úÖ **Aprende los √≠conos**\n",
        "```markdown\n",
        "üìì = Notebook\n",
        "üìÇ = Folder\n",
        "‚öôÔ∏è = Cluster/Config\n",
        "üìä = SQL/Query\n",
        "üíæ = Guardado\n",
        "‚ñ∂Ô∏è = Ejecutar\n",
        "```\n",
        "\n",
        "‚úÖ **No te abrumes**\n",
        "```markdown\n",
        "Al principio usar√°s principalmente:\n",
        "- Home (punto de partida)\n",
        "- Workspace (tus notebooks)\n",
        "- Compute (arrancar clusters)\n",
        "\n",
        "Las dem√°s secciones las conocer√°s gradualmente\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Resumen Visual de Navegaci√≥n**\n",
        "\n",
        "```markdown\n",
        "üó∫Ô∏è FLUJO T√çPICO DE NAVEGACI√ìN\n",
        "\n",
        "CASO 1: Crear y ejecutar notebook\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ 1. Home                                 ‚îÇ\n",
        "‚îÇ 2. Quick action: \"Create notebook\"      ‚îÇ\n",
        "‚îÇ 3. (Abre notebook en nueva pesta√±a)     ‚îÇ\n",
        "‚îÇ 4. Escribir c√≥digo                      ‚îÇ\n",
        "‚îÇ 5. Compute ‚Üí Start cluster              ‚îÇ\n",
        "‚îÇ 6. Ejecutar c√≥digo                      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "CASO 2: Encontrar notebook antiguo\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ 1. Ctrl+K                               ‚îÇ\n",
        "‚îÇ 2. Escribir nombre del notebook         ‚îÇ\n",
        "‚îÇ 3. Enter                                ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "CASO 3: Explorar datos disponibles\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ 1. Sidebar ‚Üí Catalog                    ‚îÇ\n",
        "‚îÇ 2. Expandir databases                   ‚îÇ\n",
        "‚îÇ 3. Click en tabla                       ‚îÇ\n",
        "‚îÇ 4. Ver schema y sample data             ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "CASO 4: Gestionar cluster\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ 1. Sidebar ‚Üí Compute                    ‚îÇ\n",
        "‚îÇ 2. Ver estado de cluster                ‚îÇ\n",
        "‚îÇ 3. Start/Stop seg√∫n necesidad           ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Has completado la exploraci√≥n de la interfaz! üéâ**\n",
        "\n",
        "Ahora conoces:\n",
        "- Todas las secciones principales\n",
        "- C√≥mo navegar eficientemente\n",
        "- Atajos de teclado esenciales\n",
        "- D√≥nde encontrar cada funcionalidad\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el siguiente punto **3.3 Estructura del workspace: Workspaces, Repos y Clusters**? üöÄ"
      ],
      "metadata": {
        "id": "Z3619gl6aowe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.3 Estructura del workspace: Workspaces, Repos y Clusters**\n",
        "\n",
        "#### **Introducci√≥n: Los Tres Pilares de tu Entorno**\n",
        "\n",
        "Tu workspace en Databricks se organiza alrededor de tres conceptos fundamentales que trabajan juntos:\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "LOS TRES PILARES DEL WORKSPACE\n",
        "\"\"\"\n",
        "\n",
        "pilares = {\n",
        "    'üìÅ Workspace': {\n",
        "        'qu√©_es': 'Sistema de archivos para notebooks y carpetas',\n",
        "        'analog√≠a': 'Tu disco duro en la nube',\n",
        "        'prop√≥sito': 'Organizar y almacenar tu c√≥digo'\n",
        "    },\n",
        "    \n",
        "    'üîó Repos': {\n",
        "        'qu√©_es': 'Integraci√≥n con Git (GitHub, GitLab, etc.)',\n",
        "        'analog√≠a': 'Tu repositorio Git dentro de Databricks',\n",
        "        'prop√≥sito': 'Control de versiones y colaboraci√≥n'\n",
        "    },\n",
        "    \n",
        "    '‚öôÔ∏è Clusters': {\n",
        "        'qu√©_es': 'Recursos computacionales (m√°quinas virtuales)',\n",
        "        'analog√≠a': 'La computadora que ejecuta tu c√≥digo',\n",
        "        'prop√≥sito': 'Procesar datos y ejecutar notebooks'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üèóÔ∏è ESTRUCTURA DEL WORKSPACE\")\n",
        "print(\"=\"*60)\n",
        "for nombre, info in pilares.items():\n",
        "    print(f\"\\n{nombre}\")\n",
        "    print(f\"  Qu√© es: {info['qu√©_es']}\")\n",
        "    print(f\"  Analog√≠a: {info['analog√≠a']}\")\n",
        "    print(f\"  Prop√≥sito: {info['prop√≥sito']}\")\n",
        "\n",
        "print(\"\"\"\n",
        "\\nüí° RELACI√ìN ENTRE ELLOS:\n",
        "\n",
        "Workspace (o Repos) ‚Üí Almacena notebooks\n",
        "         ‚Üì\n",
        "      Notebook ‚Üí Se ejecuta en\n",
        "         ‚Üì\n",
        "      Cluster ‚Üí Procesa los datos\n",
        "\n",
        "En otras palabras:\n",
        "Escribes c√≥digo en WORKSPACE/REPOS\n",
        "Lo ejecutas en un CLUSTER\n",
        "\"\"\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 1: Workspace - Tu Sistema de Archivos**\n",
        "\n",
        "#### **Estructura Jer√°rquica del Workspace**\n",
        "\n",
        "```markdown\n",
        "üìÇ ANATOM√çA DEL WORKSPACE\n",
        "\n",
        "Workspace (ra√≠z)\n",
        "‚îú‚îÄ‚îÄ üë• Users\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ usuario1@example.com\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìì Notebook 1\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìì Notebook 2\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÇ Mis Proyectos\n",
        "‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìì An√°lisis Ventas\n",
        "‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìì Dashboard\n",
        "‚îÇ   ‚îÇ\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ usuario2@example.com\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ üìì Sus notebooks\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ ü§ù Shared (si disponible)\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ üìÇ Equipo Marketing\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ üìÇ Equipo Data\n",
        "‚îÇ\n",
        "‚îî‚îÄ‚îÄ üìö Repos (integraci√≥n Git)\n",
        "    ‚îú‚îÄ‚îÄ üì¶ mi-proyecto-github\n",
        "    ‚îî‚îÄ‚îÄ üì¶ otro-repo\n",
        "\n",
        "NOTAS:\n",
        "‚Ä¢ Cada usuario tiene su carpeta personal\n",
        "‚Ä¢ En Free Edition: Solo tienes tu carpeta\n",
        "‚Ä¢ En Premium: Puedes ver carpetas compartidas\n",
        "```\n",
        "\n",
        "#### **Tipos de Objetos en Workspace**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "OBJETOS DEL WORKSPACE\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìã TIPOS DE OBJETOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "tipos_objetos = {\n",
        "    'üìì Notebook': {\n",
        "        'descripci√≥n': 'Archivo ejecutable interactivo',\n",
        "        'extensiones': ['.py', '.sql', '.scala', '.r'],\n",
        "        'contiene': 'C√≥digo en celdas ejecutables',\n",
        "        'uso': 'Desarrollo, an√°lisis, documentaci√≥n',\n",
        "        'editable': '‚úÖ S√≠, en Databricks UI',\n",
        "        'ejecutable': '‚úÖ S√≠, requiere cluster'\n",
        "    },\n",
        "    \n",
        "    'üìÇ Folder': {\n",
        "        'descripci√≥n': 'Carpeta para organizar notebooks',\n",
        "        'extensiones': ['N/A'],\n",
        "        'contiene': 'Notebooks y otras carpetas',\n",
        "        'uso': 'Organizaci√≥n jer√°rquica',\n",
        "        'editable': '‚úÖ Renombrar, mover',\n",
        "        'ejecutable': '‚ùå No'\n",
        "    },\n",
        "    \n",
        "    'üìö Library': {\n",
        "        'descripci√≥n': 'C√≥digo Python reutilizable (.py)',\n",
        "        'extensiones': ['.py'],\n",
        "        'contiene': 'Funciones y clases Python',\n",
        "        'uso': 'C√≥digo compartido entre notebooks',\n",
        "        'editable': '‚úÖ S√≠',\n",
        "        'ejecutable': '‚ùå No directamente (importar en notebook)'\n",
        "    },\n",
        "    \n",
        "    'üìä Dashboard': {\n",
        "        'descripci√≥n': 'Visualizaci√≥n compartible',\n",
        "        'extensiones': ['N/A'],\n",
        "        'contiene': 'Gr√°ficos y visualizaciones',\n",
        "        'uso': 'Presentar resultados',\n",
        "        'editable': '‚úÖ S√≠',\n",
        "        'ejecutable': '‚úÖ Se actualiza con datos'\n",
        "    }\n",
        "}\n",
        "\n",
        "for tipo, info in tipos_objetos.items():\n",
        "    print(f\"\\n{tipo}\")\n",
        "    for clave, valor in info.items():\n",
        "        if isinstance(valor, list):\n",
        "            print(f\"  {clave}: {', '.join(valor)}\")\n",
        "        else:\n",
        "            print(f\"  {clave}: {valor}\")\n",
        "```\n",
        "\n",
        "#### **Operaciones en Workspace**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "OPERACIONES COMUNES EN WORKSPACE\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîß OPERACIONES EN WORKSPACE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "operaciones = \"\"\"\n",
        "1Ô∏è‚É£ CREAR\n",
        "\n",
        "  üìì Crear Notebook:\n",
        "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "  ‚îÇ Click derecho en carpeta               ‚îÇ\n",
        "  ‚îÇ ‚Üí Create ‚Üí Notebook                    ‚îÇ\n",
        "  ‚îÇ O: Bot√≥n \"Create\" en barra superior    ‚îÇ\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "  \n",
        "  Opciones:\n",
        "  ‚Ä¢ Name: Nombre del notebook\n",
        "  ‚Ä¢ Default Language: Python, SQL, Scala, R\n",
        "  ‚Ä¢ Cluster: (Opcional) Asignar cluster\n",
        "  \n",
        "  üìÇ Crear Folder:\n",
        "  ‚Ä¢ Click derecho ‚Üí Create ‚Üí Folder\n",
        "  ‚Ä¢ Nombre de carpeta\n",
        "  ‚Ä¢ Enter\n",
        "\n",
        "2Ô∏è‚É£ IMPORTAR\n",
        "\n",
        "  Formatos soportados:\n",
        "  ‚Ä¢ .dbc (Databricks Archive)\n",
        "  ‚Ä¢ .html (Databricks HTML export)\n",
        "  ‚Ä¢ .ipynb (Jupyter Notebook)\n",
        "  ‚Ä¢ .py, .sql, .scala, .r (archivos de c√≥digo)\n",
        "  \n",
        "  Proceso:\n",
        "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "  ‚îÇ Click derecho en carpeta               ‚îÇ\n",
        "  ‚îÇ ‚Üí Import                               ‚îÇ\n",
        "  ‚îÇ ‚Üí Seleccionar archivo o URL            ‚îÇ\n",
        "  ‚îÇ ‚Üí Import                               ‚îÇ\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "  \n",
        "  üí° Tips:\n",
        "  - .ipynb se convierte autom√°ticamente\n",
        "  - Puedes importar desde URL\n",
        "  - Importar .dbc importa estructura completa\n",
        "\n",
        "3Ô∏è‚É£ EXPORTAR\n",
        "\n",
        "  Formatos disponibles:\n",
        "  ‚Ä¢ .dbc (recomendado - preserva todo)\n",
        "  ‚Ä¢ .html (para compartir sin Databricks)\n",
        "  ‚Ä¢ .ipynb (compatible con Jupyter)\n",
        "  ‚Ä¢ Source file (.py, .sql, etc.)\n",
        "  \n",
        "  Proceso:\n",
        "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "  ‚îÇ Click derecho en notebook              ‚îÇ\n",
        "  ‚îÇ ‚Üí Export                               ‚îÇ\n",
        "  ‚îÇ ‚Üí Seleccionar formato                  ‚îÇ\n",
        "  ‚îÇ ‚Üí Descargar                            ‚îÇ\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "4Ô∏è‚É£ MOVER / COPIAR\n",
        "\n",
        "  Mover:\n",
        "  ‚Ä¢ Arrastrar y soltar notebook a carpeta\n",
        "  ‚Ä¢ O: Click derecho ‚Üí Move\n",
        "  \n",
        "  Copiar:\n",
        "  ‚Ä¢ Click derecho ‚Üí Clone\n",
        "  ‚Ä¢ Especificar nueva ubicaci√≥n\n",
        "  ‚Ä¢ Crea copia independiente\n",
        "\n",
        "5Ô∏è‚É£ RENOMBRAR\n",
        "\n",
        "  ‚Ä¢ Click derecho ‚Üí Rename\n",
        "  ‚Ä¢ O: Click en nombre del notebook (cuando est√° abierto)\n",
        "  ‚Ä¢ Escribir nuevo nombre\n",
        "  ‚Ä¢ Enter\n",
        "\n",
        "6Ô∏è‚É£ ELIMINAR\n",
        "\n",
        "  ‚Ä¢ Click derecho ‚Üí Delete\n",
        "  ‚Ä¢ Confirmar eliminaci√≥n\n",
        "  \n",
        "  ‚ö†Ô∏è IMPORTANTE:\n",
        "  - Eliminaci√≥n es PERMANENTE en Free Edition\n",
        "  - No hay papelera de reciclaje\n",
        "  - Aseg√∫rate antes de eliminar\n",
        "\n",
        "7Ô∏è‚É£ BUSCAR\n",
        "\n",
        "  En workspace:\n",
        "  ‚Ä¢ Ctrl+K ‚Üí Escribe nombre\n",
        "  ‚Ä¢ O: Usar barra de b√∫squeda superior\n",
        "  \n",
        "  Busca:\n",
        "  ‚Ä¢ Nombres de notebooks\n",
        "  ‚Ä¢ Nombres de carpetas\n",
        "  ‚Ä¢ Contenido (en versiones premium)\n",
        "\"\"\"\n",
        "\n",
        "print(operaciones)\n",
        "```\n",
        "\n",
        "#### **Mejores Pr√°cticas de Organizaci√≥n**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ORGANIZACI√ìN DEL WORKSPACE\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìê MEJORES PR√ÅCTICAS DE ORGANIZACI√ìN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "estructura_recomendada = \"\"\"\n",
        "ESTRUCTURA RECOMENDADA PARA APRENDIZAJE:\n",
        "\n",
        "Workspace\n",
        "‚îî‚îÄ‚îÄ Users\n",
        "    ‚îî‚îÄ‚îÄ tu-email@example.com\n",
        "        ‚îú‚îÄ‚îÄ üìÇ 00_Curso_Databricks\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ Modulo_1_Fundamentos\n",
        "        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìì 1.1_Introduccion\n",
        "        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìì 1.2_Arquitectura\n",
        "        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìì 1.3_Casos_Uso\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ Modulo_2_Primeros_Pasos\n",
        "        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìì 2.1_RDDs_DataFrames\n",
        "        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìì 2.2_Transformaciones\n",
        "        ‚îÇ   ‚îî‚îÄ‚îÄ üìÇ Ejercicios\n",
        "        ‚îÇ       ‚îú‚îÄ‚îÄ üìì Ejercicio_1\n",
        "        ‚îÇ       ‚îî‚îÄ‚îÄ üìì Ejercicio_2\n",
        "        ‚îÇ\n",
        "        ‚îú‚îÄ‚îÄ üìÇ 01_Practica\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ ETL\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ Analisis\n",
        "        ‚îÇ   ‚îî‚îÄ‚îÄ üìÇ Visualizaciones\n",
        "        ‚îÇ\n",
        "        ‚îú‚îÄ‚îÄ üìÇ 02_Proyectos\n",
        "        ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ Proyecto_Ventas\n",
        "        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìì 01_Ingesta\n",
        "        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìì 02_Transformacion\n",
        "        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìì 03_Analisis\n",
        "        ‚îÇ   ‚îî‚îÄ‚îÄ üìÇ Proyecto_Clientes\n",
        "        ‚îÇ\n",
        "        ‚îî‚îÄ‚îÄ üìÇ 99_Sandbox\n",
        "            ‚îî‚îÄ‚îÄ üìì Experimentos\n",
        "\n",
        "PRINCIPIOS:\n",
        "\n",
        "‚úÖ USAR PREFIJOS NUM√âRICOS\n",
        "   - 00_, 01_, 02_ para ordenar carpetas\n",
        "   - Fuerza orden cronol√≥gico/l√≥gico\n",
        "\n",
        "‚úÖ NOMBRES DESCRIPTIVOS\n",
        "   ‚ùå Mal: \"Notebook1\", \"test\", \"final_final\"\n",
        "   ‚úÖ Bien: \"Analisis_Ventas_Q1\", \"ETL_Clientes\"\n",
        "\n",
        "‚úÖ SEPARAR POR CONTEXTO\n",
        "   - Aprendizaje vs Producci√≥n\n",
        "   - Por proyecto\n",
        "   - Por tipo de tarea\n",
        "\n",
        "‚úÖ CARPETA SANDBOX\n",
        "   - Para experimentos temporales\n",
        "   - Limpia regularmente\n",
        "\n",
        "‚úÖ FECHAS EN NOMBRES (Opcional)\n",
        "   - √ötil para an√°lisis temporales\n",
        "   - Formato: YYYY-MM-DD al inicio\n",
        "   - Ejemplo: \"2025-02-02_Reporte_Mensual\"\n",
        "\"\"\"\n",
        "\n",
        "print(estructura_recomendada)\n",
        "\n",
        "# Convenciones de nomenclatura\n",
        "print(\"\\nüìù CONVENCIONES DE NOMENCLATURA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "convenciones = \"\"\"\n",
        "REGLAS GENERALES:\n",
        "\n",
        "1. SIN ESPACIOS ‚Üí Usa guiones bajos\n",
        "   ‚ùå \"Mi Notebook\"\n",
        "   ‚úÖ \"Mi_Notebook\"\n",
        "\n",
        "2. SIN CARACTERES ESPECIALES\n",
        "   ‚ùå \"An√°lisis #1 (versi√≥n final!)\"\n",
        "   ‚úÖ \"Analisis_1_Version_Final\"\n",
        "\n",
        "3. CASE CONSISTENCY\n",
        "   Elige uno y mantenlo:\n",
        "   ‚Ä¢ snake_case: mi_notebook_ventas\n",
        "   ‚Ä¢ CamelCase: MiNotebookVentas\n",
        "   ‚Ä¢ kebab-case: mi-notebook-ventas (menos com√∫n)\n",
        "\n",
        "4. VERSIONES\n",
        "   Si necesitas versiones:\n",
        "   ‚Ä¢ nombre_v1, nombre_v2\n",
        "   ‚Ä¢ nombre_2025-02-02, nombre_2025-02-09\n",
        "   ‚Ä¢ Pero MEJOR: usa Git (Repos)\n",
        "\n",
        "5. PREFIJOS √öTILES\n",
        "   ‚Ä¢ WIP_ ‚Üí Work in Progress\n",
        "   ‚Ä¢ DRAFT_ ‚Üí Borrador\n",
        "   ‚Ä¢ ARCHIVE_ ‚Üí Archivado\n",
        "   ‚Ä¢ PROD_ ‚Üí Producci√≥n\n",
        "\n",
        "EJEMPLOS BUENOS:\n",
        "‚úÖ 01_Ingesta_Datos_Ventas\n",
        "‚úÖ Analisis_Cohortes_Clientes_Q1_2025\n",
        "‚úÖ ETL_Transform_Productos\n",
        "‚úÖ Dashboard_KPIs_Ejecutivos\n",
        "‚úÖ ML_Modelo_Churn_v3\n",
        "\n",
        "EJEMPLOS MALOS:\n",
        "‚ùå notebook1\n",
        "‚ùå test\n",
        "‚ùå COPIA DE final final (1)\n",
        "‚ùå asdasd\n",
        "‚ùå temp\n",
        "\"\"\"\n",
        "\n",
        "print(convenciones)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 2: Repos - Control de Versiones con Git**\n",
        "\n",
        "#### **¬øQu√© son los Repos?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "REPOS EN DATABRICKS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîó REPOS - INTEGRACI√ìN GIT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "repos_info = {\n",
        "    'concepto': 'Integraci√≥n de repositorios Git en Databricks',\n",
        "    'soporta': ['GitHub', 'GitLab', 'Bitbucket', 'Azure DevOps'],\n",
        "    'ventajas': [\n",
        "        'Control de versiones completo',\n",
        "        'Colaboraci√≥n con equipo',\n",
        "        'Historial de cambios',\n",
        "        'Branches y pull requests',\n",
        "        'CI/CD integration'\n",
        "    ],\n",
        "    'diferencia_workspace': [\n",
        "        'Workspace = Archivos sueltos sin versionado',\n",
        "        'Repos = Repositorio Git completo con versionado'\n",
        "    ]\n",
        "}\n",
        "\n",
        "for key, value in repos_info.items():\n",
        "    print(f\"\\n{key.replace('_', ' ').title()}:\")\n",
        "    if isinstance(value, list):\n",
        "        for item in value:\n",
        "            print(f\"  ‚Ä¢ {item}\")\n",
        "    else:\n",
        "        print(f\"  {value}\")\n",
        "```\n",
        "\n",
        "#### **Estructura de Repos**\n",
        "\n",
        "```markdown\n",
        "üì¶ ESTRUCTURA DE REPOS\n",
        "\n",
        "Repos (ra√≠z)\n",
        "‚îú‚îÄ‚îÄ üì¶ mi-proyecto-python\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ üìÇ notebooks\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìì analisis.py\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìì etl.py\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ üìÇ src\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ utils.py\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ üìÑ README.md\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ üìÑ .gitignore\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ üìÑ requirements.txt\n",
        "‚îÇ\n",
        "‚îî‚îÄ‚îÄ üì¶ otro-repositorio\n",
        "    ‚îî‚îÄ‚îÄ ...\n",
        "\n",
        "CARACTER√çSTICAS:\n",
        "\n",
        "‚úÖ Sincronizado con Git remoto\n",
        "‚úÖ Commits, branches, pull requests\n",
        "‚úÖ Colaboraci√≥n en equipo\n",
        "‚úÖ Historial completo de cambios\n",
        "\n",
        "‚ö†Ô∏è EN FREE EDITION:\n",
        "   Funcionalidad b√°sica de Repos disponible\n",
        "   Puedes conectar repositorios de GitHub\n",
        "```\n",
        "\n",
        "#### **Configurar tu Primer Repo**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "SETUP DE REPOS - PASO A PASO\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n‚öôÔ∏è CONFIGURAR GIT EN DATABRICKS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "setup_repos = \"\"\"\n",
        "PREREQUISITO: Tener cuenta en GitHub/GitLab/etc.\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PASO 1: GENERAR TOKEN DE ACCESO (GitHub ejemplo)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "1. Ve a GitHub.com ‚Üí Settings\n",
        "2. Developer settings ‚Üí Personal access tokens ‚Üí Tokens (classic)\n",
        "3. Generate new token (classic)\n",
        "4. Nombre: \"Databricks Access\"\n",
        "5. Scopes necesarios:\n",
        "   ‚òë repo (full control)\n",
        "6. Generate token\n",
        "7. ‚ö†Ô∏è COPIA EL TOKEN (no lo ver√°s de nuevo)\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PASO 2: CONFIGURAR GIT EN DATABRICKS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "En Databricks:\n",
        "1. Settings ‚Üí User Settings ‚Üí Git Integration\n",
        "2. Git provider: GitHub (o tu proveedor)\n",
        "3. Paste token\n",
        "4. Save\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PASO 3: CLONAR REPOSITORIO\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Opci√≥n A: Repo existente\n",
        "1. Sidebar ‚Üí Workspace (o donde veas \"Repos\")\n",
        "2. Click en \"Repos\"\n",
        "3. Add Repo\n",
        "4. Git repository URL: https://github.com/tu-usuario/tu-repo\n",
        "5. Git provider: GitHub\n",
        "6. Repository name: (auto-llena)\n",
        "7. Create repo\n",
        "\n",
        "Opci√≥n B: Crear nuevo repo\n",
        "1. Primero crea repo en GitHub\n",
        "2. Luego cl√≥nalo en Databricks (Opci√≥n A)\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PASO 4: TRABAJAR CON EL REPO\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Workflow t√≠pico:\n",
        "1. Editar notebook en el repo\n",
        "2. Guardar cambios (Ctrl+S)\n",
        "3. Commit desde Databricks:\n",
        "   - Click en branch name (arriba del notebook)\n",
        "   - \"Commit & Push\"\n",
        "   - Mensaje de commit\n",
        "   - Push to GitHub\n",
        "\n",
        "4. Ver cambios en GitHub\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "OPERACIONES GIT DISPONIBLES\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Desde la UI de Databricks (panel Git):\n",
        "\n",
        "‚Ä¢ Pull - Traer cambios del remoto\n",
        "‚Ä¢ Commit - Guardar cambios locales\n",
        "‚Ä¢ Push - Subir commits al remoto\n",
        "‚Ä¢ Create Branch - Crear nueva rama\n",
        "‚Ä¢ Switch Branch - Cambiar de rama\n",
        "‚Ä¢ Merge - Fusionar ramas\n",
        "\n",
        "Acceso:\n",
        "Click en nombre del branch (esquina superior del notebook)\n",
        "‚Üí Abre panel Git\n",
        "\"\"\"\n",
        "\n",
        "print(setup_repos)\n",
        "```\n",
        "\n",
        "#### **Repos vs Workspace: Cu√°ndo Usar Cada Uno**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DECISI√ìN: REPOS vs WORKSPACE\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nü§î REPOS vs WORKSPACE: ¬øCU√ÅNDO USAR QU√â?\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comparacion_uso = \"\"\"\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ USA WORKSPACE CUANDO:                                   ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ ‚úÖ Aprendes y practicas (como este curso)              ‚îÇ\n",
        "‚îÇ ‚úÖ Experimentos r√°pidos                                 ‚îÇ\n",
        "‚îÇ ‚úÖ An√°lisis ad-hoc                                      ‚îÇ\n",
        "‚îÇ ‚úÖ Trabajas solo                                        ‚îÇ\n",
        "‚îÇ ‚úÖ No necesitas historial de versiones                 ‚îÇ\n",
        "‚îÇ ‚úÖ Prototipado r√°pido                                   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ USA REPOS CUANDO:                                       ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ ‚úÖ Trabajo en equipo                                    ‚îÇ\n",
        "‚îÇ ‚úÖ C√≥digo de producci√≥n                                 ‚îÇ\n",
        "‚îÇ ‚úÖ Necesitas historial de cambios                       ‚îÇ\n",
        "‚îÇ ‚úÖ CI/CD pipelines                                      ‚îÇ\n",
        "‚îÇ ‚úÖ Code reviews                                         ‚îÇ\n",
        "‚îÇ ‚úÖ Proyecto a largo plazo                               ‚îÇ\n",
        "‚îÇ ‚úÖ Sincronizar con repo existente                       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "üí° RECOMENDACI√ìN PARA ESTE CURSO:\n",
        "   \n",
        "   Usa WORKSPACE para:\n",
        "   ‚Ä¢ Seguir el curso\n",
        "   ‚Ä¢ Hacer ejercicios\n",
        "   ‚Ä¢ Pr√°cticas r√°pidas\n",
        "   \n",
        "   Opcionalmente, crea un REPO para:\n",
        "   ‚Ä¢ Tu proyecto final\n",
        "   ‚Ä¢ Si quieres mostrar tu trabajo en GitHub\n",
        "   ‚Ä¢ Practicar Git workflow\n",
        "\n",
        "üéØ EN PRODUCCI√ìN (despu√©s del curso):\n",
        "   ‚Ä¢ Repos = C√≥digo de producci√≥n\n",
        "   ‚Ä¢ Workspace = Exploraciones y pruebas\n",
        "\"\"\"\n",
        "\n",
        "print(comparacion_uso)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 3: Clusters - Tu Motor de Procesamiento**\n",
        "\n",
        "#### **¬øQu√© es un Cluster?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CLUSTERS EN DATABRICKS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n‚öôÔ∏è CLUSTERS - RECURSOS COMPUTACIONALES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "cluster_concepto = \"\"\"\n",
        "CLUSTER = Conjunto de m√°quinas virtuales que procesan tus datos\n",
        "\n",
        "ANALOG√çA:\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Notebook = Receta de cocina                       ‚îÇ\n",
        "‚îÇ Datos = Ingredientes                              ‚îÇ\n",
        "‚îÇ Cluster = La cocina con hornos y utensilios      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "Sin cluster ‚Üí No puedes ejecutar c√≥digo Spark\n",
        "Con cluster ‚Üí Puedes procesar desde KB hasta PB\n",
        "\n",
        "COMPONENTES DE UN CLUSTER:\n",
        "\n",
        "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "    ‚îÇ         DRIVER NODE                 ‚îÇ\n",
        "    ‚îÇ   (Coordina el trabajo)             ‚îÇ\n",
        "    ‚îÇ   ‚Ä¢ 1 nodo siempre                  ‚îÇ\n",
        "    ‚îÇ   ‚Ä¢ Ejecuta tu c√≥digo del notebook  ‚îÇ\n",
        "    ‚îÇ   ‚Ä¢ Distribuye tareas               ‚îÇ\n",
        "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "               ‚îÇ\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ             ‚îÇ          ‚îÇ\n",
        "    ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "    ‚îÇWorker 1‚îÇ   ‚îÇWorker 2‚îÇ  ‚îÇWorker N‚îÇ\n",
        "    ‚îÇ        ‚îÇ   ‚îÇ        ‚îÇ  ‚îÇ        ‚îÇ\n",
        "    ‚îÇExecutor‚îÇ   ‚îÇExecutor‚îÇ  ‚îÇExecutor‚îÇ\n",
        "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "    \n",
        "    Workers = Hacen el trabajo pesado\n",
        "    ‚Ä¢ Ejecutan tareas en paralelo\n",
        "    ‚Ä¢ Procesan particiones de datos\n",
        "    ‚Ä¢ Pueden ser 0, 1, 2, ... N nodos\n",
        "\"\"\"\n",
        "\n",
        "print(cluster_concepto)\n",
        "```\n",
        "\n",
        "#### **Tipos de Clusters**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "TIPOS DE CLUSTERS EN DATABRICKS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìä TIPOS DE CLUSTERS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "tipos_clusters = {\n",
        "    'üîß All-Purpose Cluster': {\n",
        "        'descripci√≥n': 'Cluster interactivo para desarrollo',\n",
        "        'uso': [\n",
        "            'Desarrollo de notebooks',\n",
        "            'An√°lisis exploratorio',\n",
        "            'Debugging',\n",
        "            'Testing'\n",
        "        ],\n",
        "        'caracter√≠sticas': [\n",
        "            'Mantiene estado entre ejecuciones',\n",
        "            'Puede compartirse entre notebooks',\n",
        "            'Se puede detener/reiniciar manualmente',\n",
        "            'Costos mientras est√° activo'\n",
        "        ],\n",
        "        'free_edition': '‚úÖ Disponible (1 cluster fijo)',\n",
        "        'cu√°ndo_usar': 'Desarrollo interactivo y aprendizaje'\n",
        "    },\n",
        "    \n",
        "    '‚ö° Job Cluster': {\n",
        "        'descripci√≥n': 'Cluster ef√≠mero para jobs automatizados',\n",
        "        'uso': [\n",
        "            'Ejecutar jobs programados',\n",
        "            'Workflows automatizados',\n",
        "            'Producci√≥n'\n",
        "        ],\n",
        "        'caracter√≠sticas': [\n",
        "            'Se crea al iniciar el job',\n",
        "            'Se destruye al terminar',\n",
        "            'Optimizado para costo',\n",
        "            'No interactivo'\n",
        "        ],\n",
        "        'free_edition': '‚ùå No disponible',\n",
        "        'cu√°ndo_usar': 'Producci√≥n y automatizaci√≥n'\n",
        "    },\n",
        "    \n",
        "    'üë• High-Concurrency Cluster': {\n",
        "        'descripci√≥n': 'Cluster para m√∫ltiples usuarios simult√°neos',\n",
        "        'uso': [\n",
        "            'Equipos grandes',\n",
        "            'SQL Analytics',\n",
        "            'M√∫ltiples notebooks compartidos'\n",
        "        ],\n",
        "        'caracter√≠sticas': [\n",
        "            'Aislamiento entre usuarios',\n",
        "            'Compartici√≥n eficiente de recursos',\n",
        "            'Optimizado para SQL'\n",
        "        ],\n",
        "        'free_edition': '‚ùå No disponible',\n",
        "        'cu√°ndo_usar': 'Equipos empresariales'\n",
        "    }\n",
        "}\n",
        "\n",
        "for tipo, info in tipos_clusters.items():\n",
        "    print(f\"\\n{tipo}\")\n",
        "    print(f\"  {info['descripci√≥n']}\")\n",
        "    print(f\"\\n  Uso t√≠pico:\")\n",
        "    for uso in info['uso']:\n",
        "        print(f\"    ‚Ä¢ {uso}\")\n",
        "    print(f\"\\n  Free Edition: {info['free_edition']}\")\n",
        "    print(f\"  Cu√°ndo usar: {info['cu√°ndo_usar']}\")\n",
        "```\n",
        "\n",
        "#### **Estados del Cluster**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CICLO DE VIDA DE UN CLUSTER\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîÑ ESTADOS DEL CLUSTER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "estados = \"\"\"\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                 CICLO DE VIDA                            ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "    START\n",
        "      ‚Üì\n",
        "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "  ‚îÇPENDING ‚îÇ  üü° Arrancando (1-5 minutos)\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚Ä¢ Provisionando VMs\n",
        "      ‚îÇ       ‚Ä¢ Instalando Spark\n",
        "      ‚îÇ       ‚Ä¢ Configurando red\n",
        "      ‚Üì\n",
        "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "  ‚îÇRUNNING ‚îÇ  üü¢ Activo y listo\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚Ä¢ Puede ejecutar c√≥digo\n",
        "      ‚îÇ       ‚Ä¢ Consume recursos\n",
        "      ‚îÇ       ‚Ä¢ Costo activo\n",
        "      ‚îÇ\n",
        "      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí RESTART ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "      ‚îÇ                       ‚îÇ\n",
        "      ‚Üì                       ‚Üì\n",
        "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "  ‚îÇRESTARTING‚îÇ  üü°    ‚îÇRESIZING  ‚îÇ  üü°\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "       ‚îÇ                     ‚îÇ\n",
        "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                  ‚Üì\n",
        "            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "            ‚îÇ RUNNING ‚îÇ  üü¢\n",
        "            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                 ‚îÇ\n",
        "      TERMINATE  ‚îÇ\n",
        "                 ‚Üì\n",
        "            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "            ‚îÇTERMINATING‚îÇ  üü° Apagando\n",
        "            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                 ‚Üì\n",
        "            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "            ‚îÇTERMINATED‚îÇ  ‚ö™ Apagado\n",
        "            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚Ä¢ No consume recursos\n",
        "                          ‚Ä¢ Costo = $0\n",
        "                          ‚Ä¢ Configuraci√≥n guardada\n",
        "\n",
        "‚è∞ AUTO-TERMINATION (Free Edition):\n",
        "   Despu√©s de 2 horas de inactividad ‚Üí TERMINATED\n",
        "\n",
        "üíæ NOTA IMPORTANTE:\n",
        "   ‚Ä¢ Variables en memoria SE PIERDEN al terminar\n",
        "   ‚Ä¢ Notebooks guardados PERMANECEN\n",
        "   ‚Ä¢ Datos en Delta Lake PERMANECEN\n",
        "   ‚Ä¢ Solo se pierde el \"estado\" del cluster\n",
        "\"\"\"\n",
        "\n",
        "print(estados)\n",
        "```\n",
        "\n",
        "#### **Configuraci√≥n de Cluster en Free Edition**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CLUSTER EN FREE EDITION\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüÜì CLUSTER EN FREE EDITION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "free_cluster_info = \"\"\"\n",
        "CARACTER√çSTICAS:\n",
        "\n",
        "Tama√±o:\n",
        "‚îú‚îÄ Driver: ~15 GB RAM\n",
        "‚îú‚îÄ Workers: 0 (single-node)\n",
        "‚îî‚îÄ Cores: ~2 cores\n",
        "\n",
        "Runtime:\n",
        "‚îú‚îÄ Databricks Runtime (versi√≥n latest)\n",
        "‚îú‚îÄ Python 3\n",
        "‚îú‚îÄ PySpark incluido\n",
        "‚îî‚îÄ Librer√≠as comunes pre-instaladas\n",
        "\n",
        "Limitaciones:\n",
        "‚îú‚îÄ NO configurable (tama√±o fijo)\n",
        "‚îú‚îÄ 1 solo cluster a la vez\n",
        "‚îú‚îÄ Auto-termina despu√©s de 2h inactividad\n",
        "‚îú‚îÄ NO auto-scaling\n",
        "‚îî‚îÄ NO spot instances\n",
        "\n",
        "Suficiente para:\n",
        "‚úÖ Datasets hasta ~10 GB\n",
        "‚úÖ Aprendizaje y pr√°ctica\n",
        "‚úÖ Certificaciones\n",
        "‚úÖ Proyectos personales peque√±os\n",
        "‚úÖ Este curso completo\n",
        "\n",
        "NO suficiente para:\n",
        "‚ùå Datasets > 10 GB\n",
        "‚ùå Procesamiento paralelo pesado\n",
        "‚ùå Aplicaciones de producci√≥n\n",
        "‚ùå An√°lisis 24/7\n",
        "\n",
        "üí° TIPS PARA FREE EDITION:\n",
        "   ‚Ä¢ Det√©n el cluster manualmente cuando no lo uses\n",
        "   ‚Ä¢ Evita cargar datasets muy grandes\n",
        "   ‚Ä¢ Usa .sample() para probar c√≥digo con subconjuntos\n",
        "   ‚Ä¢ Optimiza queries antes de ejecutar en datos completos\n",
        "\"\"\"\n",
        "\n",
        "print(free_cluster_info)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Crear y Gestionar tu Primer Cluster**\n",
        "\n",
        "```markdown\n",
        "üß™ EJERCICIO PR√ÅCTICO: TU PRIMER CLUSTER\n",
        "\n",
        "Vamos a crear, configurar y usar tu primer cluster.\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 1: CREAR CLUSTER (5 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "1. **Navegar a Compute**\n",
        "   - [ ] Click en Sidebar ‚Üí ‚öôÔ∏è Compute\n",
        "\n",
        "2. **Crear Cluster**\n",
        "   - [ ] Click en \"Create Cluster\" (o \"+ Create Compute\")\n",
        "   \n",
        "3. **Configurar** (en Free Edition, opciones limitadas):\n",
        "   \n",
        "   Cluster name: _______________________\n",
        "   (Sugerencia: \"mi-primer-cluster\")\n",
        "   \n",
        "   Cluster mode: Personal Compute (auto-selected)\n",
        "   \n",
        "   Databricks runtime version: (usar default)\n",
        "   \n",
        "   ‚ö†Ô∏è Las dem√°s opciones est√°n fijas en Free Edition\n",
        "   \n",
        "   - [ ] Click \"Create Cluster\"\n",
        "\n",
        "4. **Esperar a que arranque**\n",
        "   - [ ] Observa el estado: Pending ‚Üí Running\n",
        "   - [ ] Tiempo aproximado: 2-5 minutos\n",
        "   - [ ] Estado final debe ser üü¢ Running\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 2: EXPLORAR CONFIGURACI√ìN (3 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "5. **Ver detalles del cluster**\n",
        "   - [ ] Click en el nombre de tu cluster\n",
        "   - [ ] Explora las pesta√±as:\n",
        "   \n",
        "   Configuration: _______________________\n",
        "   Event Log: _______________________\n",
        "   Spark UI: _______________________\n",
        "   Driver Logs: _______________________\n",
        "   \n",
        "6. **Anotar informaci√≥n**:\n",
        "   - Runtime version: _______________________\n",
        "   - Driver: _______________________\n",
        "   - State: _______________________\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 3: PROBAR EL CLUSTER (5 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "7. **Crear notebook de prueba**\n",
        "   - [ ] Workspace ‚Üí Tu carpeta\n",
        "   - [ ] Create ‚Üí Notebook\n",
        "   - [ ] Name: \"Test_Cluster\"\n",
        "   - [ ] Connect to: [tu cluster]\n",
        "   - [ ] Create\n",
        "\n",
        "8. **Ejecutar c√≥digo de prueba**\n",
        "   \n",
        "   Copia esto en una celda:\n",
        "   ```python\n",
        "   # Verificar que Spark funciona\n",
        "   print(\"üéâ ¬°Cluster funcionando!\")\n",
        "   print(f\"Spark version: {spark.version}\")\n",
        "   \n",
        "   # Crear DataFrame de prueba\n",
        "   data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
        "   df = spark.createDataFrame(data, [\"id\", \"nombre\"])\n",
        "   \n",
        "   # Mostrar\n",
        "   df.show()\n",
        "   ```\n",
        "   \n",
        "   - [ ] Shift+Enter para ejecutar\n",
        "   - [ ] Verificar que ves resultados sin errores\n",
        "\n",
        "9. **Verificar salida esperada**:\n",
        "   ```\n",
        "   üéâ ¬°Cluster funcionando!\n",
        "   Spark version: 3.x.x\n",
        "   \n",
        "   +---+-------+\n",
        "   | id| nombre|\n",
        "   +---+-------+\n",
        "   |  1|  Alice|\n",
        "   |  2|    Bob|\n",
        "   |  3|Charlie|\n",
        "   +---+-------+\n",
        "   ```\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 4: GESTI√ìN DEL CLUSTER (3 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "10. **Practicar operaciones**:\n",
        "    \n",
        "    - [ ] Ve a Compute\n",
        "    - [ ] Ubica tu cluster (debe estar Running)\n",
        "    - [ ] Click en \"...\" (men√∫ de opciones)\n",
        "    - [ ] Observa las opciones:\n",
        "      ‚Ä¢ Restart\n",
        "      ‚Ä¢ Terminate\n",
        "      ‚Ä¢ View\n",
        "      ‚Ä¢ Edit (limitado en Free)\n",
        "      ‚Ä¢ Delete\n",
        "    \n",
        "    ‚ö†Ô∏è NO termines el cluster a√∫n si vas a seguir practicando\n",
        "\n",
        "11. **Cuando termines la sesi√≥n**:\n",
        "    - [ ] Compute ‚Üí Tu cluster\n",
        "    - [ ] Click \"Terminate\"\n",
        "    - [ ] Confirmar\n",
        "    - [ ] Estado cambia a Terminated\n",
        "    - [ ] ‚úÖ Ahora NO consume recursos\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "REFLEXI√ìN\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Responde:\n",
        "\n",
        "1. ¬øCu√°nto tard√≥ en arrancar tu cluster?\n",
        "   _______________________\n",
        "\n",
        "2. ¬øQu√© versi√≥n de Spark tiene?\n",
        "   _______________________\n",
        "\n",
        "3. ¬øEntiendes por qu√© debes terminar clusters?\n",
        "   _______________________\n",
        "\n",
        "4. ¬øTuviste alg√∫n error? ¬øCu√°l?\n",
        "   _______________________\n",
        "\n",
        "**TIEMPO TOTAL**: ~15 minutos\n",
        "\n",
        "‚úÖ ¬°Has creado, usado y gestionado tu primer cluster!\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 3.3 - Organizaci√≥n Completa**\n",
        "\n",
        "```markdown\n",
        "### Ejercicio: Estructura tu Workspace\n",
        "\n",
        "**Objetivo**: Crear una estructura organizada para todo el curso\n",
        "\n",
        "**Tareas**:\n",
        "\n",
        "1. **Crear estructura de carpetas** (10 min)\n",
        "   \n",
        "   En Workspace ‚Üí Users ‚Üí tu-email:\n",
        "   \n",
        "   - [ ] Crear carpeta \"00_Curso_Databricks\"\n",
        "   - [ ] Dentro, crear:\n",
        "     - [ ] \"Modulo_1_Fundamentos\"\n",
        "     - [ ] \"Modulo_2_Primeros_Pasos\"\n",
        "     - [ ] \"Ejercicios\"\n",
        "     - [ ] \"Proyecto_Final\"\n",
        "   \n",
        "   - [ ] Crear carpeta \"01_Practica\" (al mismo nivel)\n",
        "   - [ ] Crear carpeta \"99_Sandbox\"\n",
        "\n",
        "2. **Crear notebook de √≠ndice** (5 min)\n",
        "   \n",
        "   - [ ] En \"00_Curso_Databricks\"\n",
        "   - [ ] Crear notebook \"README\"\n",
        "   - [ ] Documentar tu estructura:\n",
        "   \n",
        "   ```markdown\n",
        "   # Mi Curso de Databricks\n",
        "   \n",
        "   ## Estructura\n",
        "   - Modulo_1_Fundamentos: Teor√≠a inicial\n",
        "   - Modulo_2_Primeros_Pasos: Pr√°ctica hands-on\n",
        "   - Ejercicios: Todos los ejercicios del curso\n",
        "   - Proyecto_Final: Mi proyecto integrador\n",
        "   \n",
        "   ## Cluster\n",
        "   - Nombre: mi-primer-cluster\n",
        "   - Creado: 2025-02-02\n",
        "   \n",
        "   ## Notas\n",
        "   [Tus notas personales aqu√≠]\n",
        "   ```\n",
        "\n",
        "3. **Mover notebooks existentes** (3 min)\n",
        "   \n",
        "   - [ ] Mueve \"Test_Cluster\" a carpeta apropiada\n",
        "   - [ ] Organiza cualquier otro notebook\n",
        "\n",
        "4. **Captura de pantalla** (2 min)\n",
        "   \n",
        "   - [ ] Toma captura de tu workspace organizado\n",
        "   - [ ] Gu√°rdala en tus apuntes\n",
        "\n",
        "**BONUS** (Opcional):\n",
        "\n",
        "5. **Setup Git** (15 min)\n",
        "   - [ ] Si tienes GitHub, configura Repos\n",
        "   - [ ] Clona un repo de ejemplo\n",
        "   - [ ] Experimenta con commits\n",
        "\n",
        "**TIEMPO TOTAL**: ~20 minutos (+ 15 opcional)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **Workspace** = Tu sistema de archivos para notebooks\n",
        "2. **Repos** = Integraci√≥n Git para control de versiones\n",
        "3. **Clusters** = Recursos computacionales que ejecutan tu c√≥digo\n",
        "4. **Free Edition** = 1 cluster fijo, auto-termina en 2h\n",
        "5. **Organizaci√≥n** desde el inicio ahorra tiempo despu√©s\n",
        "6. **Terminar clusters** cuando no los uses (ahorra recursos)\n",
        "7. **Workspace** para aprendizaje, **Repos** para producci√≥n\n",
        "8. **Estado del cluster** se pierde al terminar, pero datos en Delta no\n",
        "9. **Pending ‚Üí Running** toma 2-5 minutos\n",
        "10. **Sin cluster activo** = no puedes ejecutar c√≥digo Spark\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **Siempre termina tu cluster**\n",
        "```markdown\n",
        "Al final de cada sesi√≥n:\n",
        "Compute ‚Üí Terminate\n",
        "Ahorra recursos, es buena pr√°ctica\n",
        "```\n",
        "\n",
        "‚úÖ **Crea estructura ANTES de empezar**\n",
        "```markdown\n",
        "10 minutos organizando\n",
        "= Horas ahorradas despu√©s\n",
        "```\n",
        "\n",
        "‚úÖ **Nombra clusters descriptivamente**\n",
        "```markdown\n",
        "‚ùå \"cluster-1\"\n",
        "‚úÖ \"curso-databricks\"\n",
        "‚úÖ \"proyecto-ventas\"\n",
        "```\n",
        "\n",
        "‚úÖ **Usa Workspace para aprender**\n",
        "```markdown\n",
        "Repos es genial, pero a√±ade complejidad\n",
        "Domina notebooks primero\n",
        "Repos despu√©s\n",
        "```\n",
        "\n",
        "‚úÖ **Documentaci√≥n en notebooks README**\n",
        "```markdown\n",
        "Cada carpeta importante ‚Üí README notebook\n",
        "Documenta qu√© contiene\n",
        "Tu yo futuro te lo agradecer√°\n",
        "```\n",
        "\n",
        "‚úÖ **Sandbox es tu amigo**\n",
        "```markdown\n",
        "Experimentos van a 99_Sandbox\n",
        "Limpia regularmente\n",
        "No contamina tu workspace principal\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Has completado la configuraci√≥n del entorno! üéâ**\n",
        "\n",
        "Ahora tienes:\n",
        "- Cuenta Free Edition activa ‚úÖ\n",
        "- Interfaz dominada ‚úÖ\n",
        "- Workspace organizado ‚úÖ\n",
        "- Primer cluster funcionando ‚úÖ\n",
        "- Listo para escribir c√≥digo ‚úÖ\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el punto 3.4?"
      ],
      "metadata": {
        "id": "2j0rYcuwb5y8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.4 Configuraci√≥n b√°sica de clusters (autoscaling, autotermination)**\n",
        "\n",
        "#### **Introducci√≥n: Optimizando tu Cluster**\n",
        "\n",
        "Aunque en Free Edition las opciones de configuraci√≥n son limitadas, es fundamental entender c√≥mo se configuran los clusters para cuando trabajes con versiones de pago. Adem√°s, algunos par√°metros como **autotermination** s√≠ est√°n disponibles y son cruciales para gestionar recursos eficientemente.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CONFIGURACI√ìN DE CLUSTERS\n",
        "\"\"\"\n",
        "\n",
        "configuracion_intro = {\n",
        "    'en_free_edition': 'Configuraci√≥n mayormente fija, autotermination configurable',\n",
        "    'en_premium': 'Control total sobre recursos y comportamiento',\n",
        "    'importancia': 'Afecta directamente costo, rendimiento y disponibilidad',\n",
        "    'conceptos_clave': [\n",
        "        'Autoscaling - Escalado autom√°tico',\n",
        "        'Autotermination - Apagado autom√°tico',\n",
        "        'Worker nodes - N√∫mero de workers',\n",
        "        'Instance types - Tipo de m√°quinas',\n",
        "        'Runtime - Versi√≥n de Databricks'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"‚öôÔ∏è CONFIGURACI√ìN DE CLUSTERS\")\n",
        "print(\"=\"*60)\n",
        "for key, value in configuracion_intro.items():\n",
        "    if isinstance(value, list):\n",
        "        print(f\"{key.replace('_', ' ').title()}:\")\n",
        "        for item in value:\n",
        "            print(f\"  ‚Ä¢ {item}\")\n",
        "    else:\n",
        "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 1: Autotermination (Auto-terminaci√≥n)**\n",
        "\n",
        "#### **¬øQu√© es Autotermination?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "AUTOTERMINATION - CONCEPTO FUNDAMENTAL\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n‚è∞ AUTOTERMINATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "autotermination_explicacion = \"\"\"\n",
        "DEFINICI√ìN:\n",
        "Apagado autom√°tico del cluster despu√©s de un per√≠odo de inactividad\n",
        "\n",
        "ANALOG√çA:\n",
        "Como las luces que se apagan autom√°ticamente si no hay movimiento\n",
        "‚Üí Ahorra electricidad (recursos/dinero)\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ         TIMELINE DE AUTOTERMINATION                    ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                        ‚îÇ\n",
        "‚îÇ 10:00 AM ‚Üí Ejecutas c√≥digo                             ‚îÇ\n",
        "‚îÇ           Cluster: üü¢ ACTIVO (√∫ltima actividad)        ‚îÇ\n",
        "‚îÇ                                                        ‚îÇ\n",
        "‚îÇ 10:05 AM ‚Üí C√≥digo termina                              ‚îÇ\n",
        "‚îÇ           Cluster: üü¢ ACTIVO (esperando...)            ‚îÇ\n",
        "‚îÇ                                                        ‚îÇ\n",
        "‚îÇ 10:30 AM ‚Üí No hay actividad                            ‚îÇ\n",
        "‚îÇ           Cluster: üü¢ ACTIVO (esperando...)            ‚îÇ\n",
        "‚îÇ                                                        ‚îÇ\n",
        "‚îÇ 12:05 PM ‚Üí 2 horas sin actividad                       ‚îÇ\n",
        "‚îÇ           Cluster: üü° TERMINATING                      ‚îÇ\n",
        "‚îÇ                                                        ‚îÇ\n",
        "‚îÇ 12:07 PM ‚Üí Apagado completo                            ‚îÇ\n",
        "‚îÇ           Cluster: ‚ö™ TERMINATED                       ‚îÇ\n",
        "‚îÇ           üí∞ COSTO = $0 desde aqu√≠                     ‚îÇ\n",
        "‚îÇ                                                        ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "QU√â CUENTA COMO ACTIVIDAD:\n",
        "‚úÖ Ejecutar celda en notebook\n",
        "‚úÖ Ejecutar comando\n",
        "‚úÖ Job en ejecuci√≥n\n",
        "‚úÖ Stream activo\n",
        "\n",
        "QU√â NO CUENTA:\n",
        "‚ùå Tener notebook abierto sin ejecutar\n",
        "‚ùå Ver resultados anteriores\n",
        "‚ùå Editar c√≥digo sin ejecutar\n",
        "‚ùå Navegar por la UI\n",
        "\n",
        "IMPORTANTE:\n",
        "üî¥ Estado del cluster SE PIERDE al terminar\n",
        "   ‚Ä¢ Variables en memoria ‚Üí PERDIDAS\n",
        "   ‚Ä¢ DataFrames no persistidos ‚Üí PERDIDOS\n",
        "   ‚Ä¢ Cache ‚Üí PERDIDO\n",
        "\n",
        "üü¢ Datos persistidos PERMANECEN\n",
        "   ‚Ä¢ Tablas Delta Lake ‚Üí PERSISTEN\n",
        "   ‚Ä¢ Archivos en DBFS ‚Üí PERSISTEN\n",
        "   ‚Ä¢ Notebooks guardados ‚Üí PERSISTEN\n",
        "\"\"\"\n",
        "\n",
        "print(autotermination_explicacion)\n",
        "```\n",
        "\n",
        "#### **Configurar Autotermination**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CONFIGURACI√ìN DE AUTOTERMINATION\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîß CONFIGURAR AUTOTERMINATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "configuracion_autotermination = \"\"\"\n",
        "EN FREE EDITION:\n",
        "‚îú‚îÄ Valor fijo: 120 minutos (2 horas)\n",
        "‚îú‚îÄ NO configurable\n",
        "‚îî‚îÄ Siempre activo\n",
        "\n",
        "EN VERSIONES PREMIUM:\n",
        "‚îú‚îÄ Configurable desde 10 minutos hasta infinito\n",
        "‚îú‚îÄ Se puede desactivar\n",
        "‚îî‚îÄ Personalizable por cluster\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "C√ìMO CONFIGURAR (Premium/Standard)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "1. Compute ‚Üí Create/Edit Cluster\n",
        "\n",
        "2. Busca secci√≥n \"Autopilot options\" o \"Advanced options\"\n",
        "\n",
        "3. Encuentra \"Terminate after ___ minutes of inactivity\"\n",
        "\n",
        "4. Configura valor deseado:\n",
        "   \n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ Terminate after [  60  ] minutes       ‚îÇ\n",
        "   ‚îÇ of inactivity                          ‚îÇ\n",
        "   ‚îÇ                                        ‚îÇ\n",
        "   ‚îÇ ‚òê Disable autotermination              ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "5. Save\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "VALORES RECOMENDADOS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "DESARROLLO INTERACTIVO:\n",
        "‚Ä¢ 30-60 minutos\n",
        "‚Ä¢ Raz√≥n: Pausas para pensar, pero no demasiado largo\n",
        "‚Ä¢ Ahorra costos entre sesiones\n",
        "\n",
        "PRODUCCI√ìN/JOBS:\n",
        "‚Ä¢ Desactivado o muy corto (10 min)\n",
        "‚Ä¢ Jobs manejan ciclo de vida del cluster\n",
        "\n",
        "DEMOS/PRESENTACIONES:\n",
        "‚Ä¢ 120+ minutos\n",
        "‚Ä¢ No quieres que se apague durante la demo\n",
        "\n",
        "APRENDIZAJE (como t√∫):\n",
        "‚Ä¢ 60-120 minutos\n",
        "‚Ä¢ Balance entre comodidad y eficiencia\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "BUENAS PR√ÅCTICAS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "‚úÖ SIEMPRE habilita autotermination (salvo casos espec√≠ficos)\n",
        "   ‚Üí Evita costos inesperados\n",
        "\n",
        "‚úÖ Ajusta seg√∫n tu workflow\n",
        "   ‚Üí Muchas pausas = tiempo largo\n",
        "   ‚Üí Trabajo continuo = tiempo corto\n",
        "\n",
        "‚úÖ Termina manualmente si terminas antes\n",
        "   ‚Üí No esperes al autotermination\n",
        "\n",
        "‚ùå NUNCA desactives en desarrollo\n",
        "   ‚Üí Olvidar un cluster puede costar cientos de d√≥lares\n",
        "\n",
        "‚ùå NO pongas tiempo muy corto (<30 min) en desarrollo\n",
        "   ‚Üí Frustrante si se apaga mientras piensas\n",
        "\"\"\"\n",
        "\n",
        "print(configuracion_autotermination)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 2: Autoscaling (Escalado Autom√°tico)**\n",
        "\n",
        "#### **¬øQu√© es Autoscaling?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "AUTOSCALING - ELASTICIDAD DIN√ÅMICA\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìà AUTOSCALING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "autoscaling_explicacion = \"\"\"\n",
        "DEFINICI√ìN:\n",
        "Ajuste autom√°tico del n√∫mero de workers seg√∫n la carga de trabajo\n",
        "\n",
        "ANALOG√çA:\n",
        "Como un restaurante que contrata m√°s meseros cuando hay m√°s clientes\n",
        "\n",
        "SIN AUTOSCALING (Cluster Fijo):\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ 09:00 ‚Üí Carga baja  (2 workers) ‚Üí 2 activos, 0 ociosos     ‚îÇ\n",
        "‚îÇ 12:00 ‚Üí Carga alta  (2 workers) ‚Üí 2 activos, LENTO üêå      ‚îÇ\n",
        "‚îÇ 18:00 ‚Üí Carga baja  (2 workers) ‚Üí 2 activos, $$ DESPERDICIO‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "Problemas:\n",
        "‚ùå Pico de carga ‚Üí Rendimiento malo\n",
        "‚ùå Carga baja ‚Üí Desperdicio de dinero\n",
        "\n",
        "CON AUTOSCALING (2-10 workers):\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ 09:00 ‚Üí Carga baja  (2 workers) ‚Üí √ìptimo ‚úÖ            ‚îÇ\n",
        "‚îÇ 12:00 ‚Üí Carga alta  (8 workers) ‚Üí Escala UP ‚¨ÜÔ∏è         ‚îÇ\n",
        "‚îÇ 14:00 ‚Üí Procesa     (8 workers) ‚Üí R√°pido ‚ö°             ‚îÇ\n",
        "‚îÇ 18:00 ‚Üí Carga baja  (2 workers) ‚Üí Escala DOWN ‚¨áÔ∏è       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "Ventajas:\n",
        "‚úÖ Rendimiento √≥ptimo en picos\n",
        "‚úÖ Ahorro en valles\n",
        "‚úÖ No necesitas adivinar el tama√±o correcto\n",
        "\n",
        "C√ìMO FUNCIONA:\n",
        "\n",
        "1. SCALE UP (Aumentar):\n",
        "   ‚Ä¢ Detecta tareas pendientes en cola\n",
        "   ‚Ä¢ A√±ade workers autom√°ticamente\n",
        "   ‚Ä¢ Tiempo: 1-3 minutos para nuevos workers\n",
        "\n",
        "2. SCALE DOWN (Reducir):\n",
        "   ‚Ä¢ Detecta workers ociosos\n",
        "   ‚Ä¢ Espera per√≠odo de gracia (10-30 min)\n",
        "   ‚Ä¢ Remueve workers no necesarios\n",
        "   ‚Ä¢ Tiempo: Instant√°neo\n",
        "\n",
        "M√âTRICAS QUE SPARK MONITOREA:\n",
        "‚îú‚îÄ Tareas en cola esperando worker\n",
        "‚îú‚îÄ CPU utilization por worker\n",
        "‚îú‚îÄ Memoria utilizada por worker\n",
        "‚îî‚îÄ Workers ociosos\n",
        "\"\"\"\n",
        "\n",
        "print(autoscaling_explicacion)\n",
        "```\n",
        "\n",
        "#### **Configurar Autoscaling**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CONFIGURACI√ìN DE AUTOSCALING\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîß CONFIGURAR AUTOSCALING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "config_autoscaling = \"\"\"\n",
        "EN FREE EDITION:\n",
        "‚îú‚îÄ ‚ùå NO disponible\n",
        "‚îú‚îÄ Cluster single-node (0 workers)\n",
        "‚îî‚îÄ Tama√±o fijo\n",
        "\n",
        "EN VERSIONES PREMIUM:\n",
        "‚îú‚îÄ ‚úÖ Completamente configurable\n",
        "‚îú‚îÄ Min workers: 1-100+\n",
        "‚îî‚îÄ Max workers: 1-100+\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "C√ìMO CONFIGURAR (Premium/Standard)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "1. Compute ‚Üí Create/Edit Cluster\n",
        "\n",
        "2. Secci√≥n \"Cluster size\" o \"Workers\"\n",
        "\n",
        "3. Configurar:\n",
        "   \n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ ‚òë Enable autoscaling                           ‚îÇ\n",
        "   ‚îÇ                                                ‚îÇ\n",
        "   ‚îÇ Workers:                                       ‚îÇ\n",
        "   ‚îÇ   Min workers: [  2  ]                         ‚îÇ\n",
        "   ‚îÇ   Max workers: [ 10  ]                         ‚îÇ\n",
        "   ‚îÇ                                                ‚îÇ\n",
        "   ‚îÇ Driver type:   Standard_DS3_v2                 ‚îÇ\n",
        "   ‚îÇ Worker type:   Standard_DS3_v2                 ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "4. Save\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "VALORES RECOMENDADOS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "DESARROLLO/EXPLORACI√ìN:\n",
        "Min: 1-2 workers\n",
        "Max: 4-8 workers\n",
        "‚Ä¢ Raz√≥n: Cargas impredecibles, presupuesto limitado\n",
        "\n",
        "AN√ÅLISIS BATCH (ETL):\n",
        "Min: 2 workers\n",
        "Max: 20-50 workers\n",
        "‚Ä¢ Raz√≥n: Picos durante procesamiento, idle entre jobs\n",
        "\n",
        "STREAMING:\n",
        "Min: 4-8 workers (m√°s alto)\n",
        "Max: 20-40 workers\n",
        "‚Ä¢ Raz√≥n: Carga constante pero con picos\n",
        "\n",
        "ML TRAINING:\n",
        "Min: 4 workers\n",
        "Max: 8-16 workers\n",
        "‚Ä¢ Raz√≥n: Carga predecible, beneficio limitado de muchos workers\n",
        "\n",
        "AD-HOC QUERIES:\n",
        "Min: 2 workers\n",
        "Max: 10-20 workers\n",
        "‚Ä¢ Raz√≥n: Queries var√≠an mucho en complejidad\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "ESTRATEGIAS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "ESTRATEGIA 1: Conservative (conservadora)\n",
        "‚îú‚îÄ Min = Max (sin autoscaling)\n",
        "‚îú‚îÄ Uso: Carga predecible\n",
        "‚îî‚îÄ Ventaja: Costos predecibles\n",
        "\n",
        "ESTRATEGIA 2: Aggressive (agresiva)\n",
        "‚îú‚îÄ Min muy bajo (1-2)\n",
        "‚îú‚îÄ Max muy alto (50-100)\n",
        "‚îú‚îÄ Uso: Cargas muy variables\n",
        "‚îî‚îÄ Ventaja: M√°xima flexibilidad\n",
        "\n",
        "ESTRATEGIA 3: Balanced (balanceada)\n",
        "‚îú‚îÄ Min = 20-30% de carga t√≠pica\n",
        "‚îú‚îÄ Max = 2-3x carga t√≠pica\n",
        "‚îú‚îÄ Uso: La mayor√≠a de casos\n",
        "‚îî‚îÄ Ventaja: Balance costo/rendimiento\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "ADVERTENCIAS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "‚ö†Ô∏è Scale UP NO es instant√°neo\n",
        "   ‚Ä¢ Toma 1-3 minutos provisionar nuevos workers\n",
        "   ‚Ä¢ Planifica con anticipaci√≥n en cargas cr√≠ticas\n",
        "\n",
        "‚ö†Ô∏è Scale DOWN conserva datos\n",
        "   ‚Ä¢ Spark mueve datos antes de remover worker\n",
        "   ‚Ä¢ No pierdes trabajo\n",
        "\n",
        "‚ö†Ô∏è Costo m√≠nimo = Min workers √ó tiempo\n",
        "   ‚Ä¢ Min workers SIEMPRE activos\n",
        "   ‚Ä¢ No ahorras si cargas < min\n",
        "\n",
        "‚ö†Ô∏è Shuffle complica autoscaling\n",
        "   ‚Ä¢ Workers con datos en shuffle no se pueden remover\n",
        "   ‚Ä¢ Puede limitar scale down temporalmente\n",
        "\"\"\"\n",
        "\n",
        "print(config_autoscaling)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 3: Otros Par√°metros de Configuraci√≥n**\n",
        "\n",
        "#### **Databricks Runtime Version**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DATABRICKS RUNTIME\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüöÄ DATABRICKS RUNTIME VERSION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "runtime_info = \"\"\"\n",
        "QU√â ES:\n",
        "Versi√≥n del software Databricks (incluye Spark, librer√≠as, etc.)\n",
        "\n",
        "FORMATO:\n",
        "Runtime XX.X (Scala X.XX, Spark X.X.X)\n",
        "\n",
        "Ejemplo: Runtime 15.4 (Scala 2.12, Spark 3.5.0)\n",
        "\n",
        "TIPOS DE RUNTIME:\n",
        "\n",
        "1Ô∏è‚É£ STANDARD RUNTIME\n",
        "   ‚Ä¢ Spark + Python + Scala + R\n",
        "   ‚Ä¢ Uso general\n",
        "   ‚Ä¢ M√°s usado\n",
        "\n",
        "2Ô∏è‚É£ ML RUNTIME\n",
        "   ‚Ä¢ Standard + TensorFlow, PyTorch, etc.\n",
        "   ‚Ä¢ Machine Learning pre-configurado\n",
        "   ‚Ä¢ Usa cuando hagas ML/DL\n",
        "\n",
        "3Ô∏è‚É£ PHOTON RUNTIME\n",
        "   ‚Ä¢ Motor optimizado (3-8x m√°s r√°pido)\n",
        "   ‚Ä¢ Solo SQL/DataFrames\n",
        "   ‚Ä¢ Premium feature\n",
        "\n",
        "4Ô∏è‚É£ GPU RUNTIME\n",
        "   ‚Ä¢ Para deep learning\n",
        "   ‚Ä¢ Incluye drivers CUDA\n",
        "   ‚Ä¢ Clusters con GPUs\n",
        "\n",
        "EN FREE EDITION:\n",
        "‚îú‚îÄ Standard Runtime (versi√≥n reciente)\n",
        "‚îú‚îÄ NO Photon\n",
        "‚îú‚îÄ NO GPU\n",
        "‚îî‚îÄ ML Runtime disponible\n",
        "\n",
        "C√ìMO SELECCIONAR:\n",
        "\n",
        "Compute ‚Üí Create Cluster ‚Üí Databricks Runtime Version\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Databricks Runtime Version: [‚ñº]                ‚îÇ\n",
        "‚îÇ                                                ‚îÇ\n",
        "‚îÇ ‚Ä¢ 15.4 LTS (Scala 2.12, Spark 3.5.0)           ‚îÇ\n",
        "‚îÇ ‚Ä¢ 15.4 LTS ML (includes ML libraries)          ‚îÇ\n",
        "‚îÇ ‚Ä¢ 14.3 LTS                                     ‚îÇ\n",
        "‚îÇ ‚Ä¢ 13.3 LTS                                     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "RECOMENDACIONES:\n",
        "\n",
        "‚úÖ USA LTS (Long Term Support)\n",
        "   ‚Ä¢ M√°s estable\n",
        "   ‚Ä¢ Soporte extendido\n",
        "   ‚Ä¢ Menos breaking changes\n",
        "\n",
        "‚úÖ Usa versi√≥n reciente (no latest)\n",
        "   ‚Ä¢ Latest puede tener bugs\n",
        "   ‚Ä¢ Una versi√≥n atr√°s = m√°s estable\n",
        "\n",
        "‚úÖ ML Runtime solo si haces ML\n",
        "   ‚Ä¢ Tarda m√°s en arrancar\n",
        "   ‚Ä¢ Usa m√°s espacio\n",
        "\n",
        "‚ùå NO cambies runtime constantemente\n",
        "   ‚Ä¢ Incompatibilidades entre versiones\n",
        "   ‚Ä¢ Notebooks pueden romperse\n",
        "\"\"\"\n",
        "\n",
        "print(runtime_info)\n",
        "```\n",
        "\n",
        "#### **Worker Nodes y Driver**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CONFIGURACI√ìN DE NODOS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüíª WORKER NODES Y DRIVER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "nodos_config = \"\"\"\n",
        "COMPONENTES DEL CLUSTER:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    CLUSTER                          ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                     ‚îÇ\n",
        "‚îÇ  DRIVER NODE (1)                                    ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ\n",
        "‚îÇ  ‚îÇ ‚Ä¢ Coordina workers                        ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îÇ ‚Ä¢ Ejecuta c√≥digo del notebook             ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îÇ ‚Ä¢ Almacena resultados peque√±os            ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îÇ ‚Ä¢ collect() trae datos aqu√≠               ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ\n",
        "‚îÇ                                                     ‚îÇ\n",
        "‚îÇ  WORKER NODES (0 a N)                               ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ\n",
        "‚îÇ  ‚îÇ Worker 1 ‚îÇ  ‚îÇ Worker 2 ‚îÇ  ‚îÇ Worker N ‚îÇ           ‚îÇ\n",
        "‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ          ‚îÇ           ‚îÇ\n",
        "‚îÇ  ‚îÇProcesa   ‚îÇ  ‚îÇProcesa   ‚îÇ  ‚îÇProcesa   ‚îÇ           ‚îÇ\n",
        "‚îÇ  ‚îÇdatos     ‚îÇ  ‚îÇdatos     ‚îÇ  ‚îÇdatos     ‚îÇ           ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ\n",
        "‚îÇ                                                     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "CONFIGURACI√ìN:\n",
        "\n",
        "EN FREE EDITION:\n",
        "‚îú‚îÄ Driver: Fijo (~15 GB RAM, 2 cores)\n",
        "‚îú‚îÄ Workers: 0 (single-node cluster)\n",
        "‚îî‚îÄ NO configurable\n",
        "\n",
        "EN PREMIUM:\n",
        "‚îú‚îÄ Driver: Seleccionar tipo de instancia\n",
        "‚îú‚îÄ Workers: Seleccionar tipo y cantidad\n",
        "‚îî‚îÄ Completamente configurable\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "TIPOS DE INSTANCIA (Premium - Ejemplo AWS)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "MEMORIA-OPTIMIZADA (para datasets grandes):\n",
        "‚Ä¢ r5.xlarge    - 32 GB RAM, 4 cores\n",
        "‚Ä¢ r5.2xlarge   - 64 GB RAM, 8 cores\n",
        "‚Ä¢ r5.4xlarge   - 128 GB RAM, 16 cores\n",
        "\n",
        "COMPUTE-OPTIMIZADA (para procesamiento):\n",
        "‚Ä¢ c5.xlarge    - 8 GB RAM, 4 cores\n",
        "‚Ä¢ c5.2xlarge   - 16 GB RAM, 8 cores\n",
        "‚Ä¢ c5.4xlarge   - 32 GB RAM, 16 cores\n",
        "\n",
        "GENERAL PURPOSE (balance):\n",
        "‚Ä¢ m5.xlarge    - 16 GB RAM, 4 cores\n",
        "‚Ä¢ m5.2xlarge   - 32 GB RAM, 8 cores\n",
        "‚Ä¢ m5.4xlarge   - 64 GB RAM, 16 cores\n",
        "\n",
        "GPU (para ML/DL):\n",
        "‚Ä¢ p3.2xlarge   - 61 GB RAM, 8 cores, 1 GPU\n",
        "‚Ä¢ p3.8xlarge   - 244 GB RAM, 32 cores, 4 GPUs\n",
        "\n",
        "C√ìMO ELEGIR:\n",
        "\n",
        "PREGUNTA 1: ¬øCu√°ntos datos?\n",
        "‚îú‚îÄ < 10 GB ‚Üí General Purpose peque√±o\n",
        "‚îú‚îÄ 10-100 GB ‚Üí General Purpose mediano\n",
        "‚îú‚îÄ 100GB-1TB ‚Üí Memoria-optimizada\n",
        "‚îî‚îÄ > 1TB ‚Üí Memoria-optimizada grande\n",
        "\n",
        "PREGUNTA 2: ¬øQu√© operaciones?\n",
        "‚îú‚îÄ Lecturas/escrituras ‚Üí Memoria-optimizada\n",
        "‚îú‚îÄ C√°lculos complejos ‚Üí Compute-optimizada\n",
        "‚îú‚îÄ Deep Learning ‚Üí GPU\n",
        "‚îî‚îÄ Mixto ‚Üí General Purpose\n",
        "\n",
        "PREGUNTA 3: ¬øCu√°ntos workers?\n",
        "‚îú‚îÄ Muchos peque√±os ‚Üí M√°s paralelismo, m√°s overhead\n",
        "‚îú‚îÄ Pocos grandes ‚Üí Menos overhead, menos paralelismo\n",
        "‚îî‚îÄ Balance: 4-8 workers medianos\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "CONFIGURACI√ìN DRIVER vs WORKER\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "OPCI√ìN 1: Driver = Worker (com√∫n)\n",
        "‚îú‚îÄ Mismo tipo de instancia\n",
        "‚îú‚îÄ Uso: General purpose\n",
        "‚îî‚îÄ Ventaja: Simplicidad\n",
        "\n",
        "OPCI√ìN 2: Driver > Worker\n",
        "‚îú‚îÄ Driver m√°s grande que workers\n",
        "‚îú‚îÄ Uso: collect() frecuente, muchos resultados al driver\n",
        "‚îî‚îÄ Ventaja: Driver no es cuello de botella\n",
        "\n",
        "OPCI√ìN 3: Driver < Worker\n",
        "‚îú‚îÄ Driver m√°s peque√±o que workers\n",
        "‚îú‚îÄ Uso: Poco uso del driver, workers hacen trabajo pesado\n",
        "‚îî‚îÄ Ventaja: Ahorro de costo\n",
        "\n",
        "RECOMENDACI√ìN GENERAL:\n",
        "‚Ä¢ Driver y Workers del mismo tipo\n",
        "‚Ä¢ O Driver 1 tier superior si usas mucho collect()\n",
        "\"\"\"\n",
        "\n",
        "print(nodos_config)\n",
        "```\n",
        "\n",
        "#### **Pol√≠ticas de Cluster (Cluster Policies)**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "POL√çTICAS DE CLUSTER\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìã CLUSTER POLICIES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "policies_info = \"\"\"\n",
        "QU√â SON:\n",
        "Templates y restricciones para creaci√≥n de clusters\n",
        "\n",
        "EN FREE EDITION:\n",
        "‚ùå NO disponible\n",
        "\n",
        "EN ENTERPRISE:\n",
        "‚úÖ Disponible y muy √∫til\n",
        "\n",
        "USO:\n",
        "Controlar qu√© configuraciones pueden usar los usuarios\n",
        "\n",
        "EJEMPLO DE POL√çTICA:\n",
        "\n",
        "{\n",
        "  \"cluster_type\": {\n",
        "    \"type\": \"fixed\",\n",
        "    \"value\": \"all-purpose\"\n",
        "  },\n",
        "  \"instance_pool_id\": {\n",
        "    \"type\": \"fixed\",\n",
        "    \"value\": \"pool-xyz\"\n",
        "  },\n",
        "  \"num_workers\": {\n",
        "    \"type\": \"range\",\n",
        "    \"min\": 1,\n",
        "    \"max\": 10\n",
        "  },\n",
        "  \"autotermination_minutes\": {\n",
        "    \"type\": \"fixed\",\n",
        "    \"value\": 60\n",
        "  }\n",
        "}\n",
        "\n",
        "BENEFICIOS:\n",
        "\n",
        "PARA ADMINISTRADORES:\n",
        "‚úÖ Control de costos\n",
        "‚úÖ Cumplimiento de est√°ndares\n",
        "‚úÖ Prevenir errores costosos\n",
        "‚úÖ Governance\n",
        "\n",
        "PARA USUARIOS:\n",
        "‚úÖ Configuraci√≥n simplificada\n",
        "‚úÖ Menos decisiones\n",
        "‚úÖ Menos errores\n",
        "\n",
        "CASOS DE USO:\n",
        "\n",
        "1. EQUIPO DE DESARROLLO:\n",
        "   ‚Ä¢ Max 5 workers\n",
        "   ‚Ä¢ Autotermination 30 min obligatorio\n",
        "   ‚Ä¢ Solo instancias econ√≥micas\n",
        "\n",
        "2. EQUIPO DE PRODUCCI√ìN:\n",
        "   ‚Ä¢ Instancias optimizadas\n",
        "   ‚Ä¢ Sin autotermination\n",
        "   ‚Ä¢ Logging obligatorio\n",
        "\n",
        "3. EQUIPO DE ML:\n",
        "   ‚Ä¢ Acceso a GPUs\n",
        "   ‚Ä¢ ML Runtime obligatorio\n",
        "   ‚Ä¢ Instancias con memoria alta\n",
        "\"\"\"\n",
        "\n",
        "print(policies_info)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 4: Configuraci√≥n Avanzada**\n",
        "\n",
        "#### **Instance Pools**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "INSTANCE POOLS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüèä INSTANCE POOLS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pools_info = \"\"\"\n",
        "QU√â SON:\n",
        "Conjunto pre-provisionado de instancias listas para usar\n",
        "\n",
        "ANALOG√çA:\n",
        "Como tener taxis esperando en una parada\n",
        "vs llamar un taxi cada vez que lo necesitas\n",
        "\n",
        "SIN POOL:\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Crear Cluster                                      ‚îÇ\n",
        "‚îÇ    ‚Üì                                               ‚îÇ\n",
        "‚îÇ Solicitar VMs al cloud (3-5 min) ‚è∞ LENTO          ‚îÇ\n",
        "‚îÇ    ‚Üì                                               ‚îÇ\n",
        "‚îÇ Instalar Spark                                     ‚îÇ\n",
        "‚îÇ    ‚Üì                                               ‚îÇ\n",
        "‚îÇ Cluster Ready                                      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "CON POOL:\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Crear Cluster                                      ‚îÇ\n",
        "‚îÇ    ‚Üì                                               ‚îÇ\n",
        "‚îÇ Tomar VMs del pool (30 seg) ‚ö° R√ÅPIDO               ‚îÇ\n",
        "‚îÇ    ‚Üì                                               ‚îÇ\n",
        "‚îÇ Cluster Ready                                      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "VENTAJAS:\n",
        "‚úÖ Clusters arrancan 5-10x m√°s r√°pido\n",
        "‚úÖ Menos espera para desarrolladores\n",
        "‚úÖ Mejor experiencia de usuario\n",
        "\n",
        "DESVENTAJAS:\n",
        "üí∞ Pagas por instancias idle en el pool\n",
        "üí∞ Costo continuo incluso sin uso\n",
        "\n",
        "EN FREE EDITION:\n",
        "‚ùå NO disponible\n",
        "\n",
        "CU√ÅNDO USAR (Premium):\n",
        "‚úÖ Equipos grandes con uso frecuente\n",
        "‚úÖ Desarrollo interactivo intensivo\n",
        "‚úÖ Costo de espera > costo de pool idle\n",
        "‚ùå Uso espor√°dico\n",
        "‚ùå Presupuesto ajustado\n",
        "\"\"\"\n",
        "\n",
        "print(pools_info)\n",
        "```\n",
        "\n",
        "#### **Spot Instances**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "SPOT INSTANCES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüí∞ SPOT INSTANCES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "spot_info = \"\"\"\n",
        "QU√â SON:\n",
        "Instancias de bajo costo que pueden ser interrumpidas\n",
        "\n",
        "ANALOG√çA:\n",
        "Como volar standby - m√°s barato pero sin garant√≠a\n",
        "\n",
        "CARACTER√çSTICAS:\n",
        "‚îú‚îÄ Hasta 80% de descuento vs instancias normales\n",
        "‚îú‚îÄ Pueden ser interrumpidas con poco aviso\n",
        "‚îú‚îÄ Disponibilidad no garantizada\n",
        "‚îî‚îÄ Cloud provider las recupera si las necesita\n",
        "\n",
        "C√ìMO FUNCIONA:\n",
        "\n",
        "1. Solicitas instancias spot\n",
        "2. Cloud te las asigna (si hay disponibles)\n",
        "3. Usas a precio reducido\n",
        "4. Si cloud necesita capacidad ‚Üí Te las quita\n",
        "5. Spark redistribuye trabajo a otras instancias\n",
        "\n",
        "TOLERANCIA A FALLOS DE SPARK:\n",
        "‚úÖ Spark RE-EJECUTA tareas perdidas autom√°ticamente\n",
        "‚úÖ NO pierdes el trabajo\n",
        "‚ö†Ô∏è Puede ser m√°s LENTO si muchas interrupciones\n",
        "\n",
        "EN FREE EDITION:\n",
        "‚ùå NO disponible\n",
        "\n",
        "CONFIGURACI√ìN (Premium):\n",
        "\n",
        "Compute ‚Üí Create Cluster ‚Üí Advanced Options\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ ‚òë Use spot instances                           ‚îÇ\n",
        "‚îÇ                                                ‚îÇ\n",
        "‚îÇ Spot bid price as % of on-demand: [ 100 ]%     ‚îÇ\n",
        "‚îÇ                                                ‚îÇ\n",
        "‚îÇ Fallback to on-demand if spots unavailable:    ‚îÇ\n",
        "‚îÇ ‚òë Yes                                          ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "RECOMENDACIONES:\n",
        "\n",
        "‚úÖ USA SPOT PARA:\n",
        "   ‚Ä¢ Batch processing (ETL)\n",
        "   ‚Ä¢ Jobs no cr√≠ticos\n",
        "   ‚Ä¢ Workers (no driver)\n",
        "   ‚Ä¢ Cargas fault-tolerant\n",
        "\n",
        "‚ùå NO USES SPOT PARA:\n",
        "   ‚Ä¢ Driver node (siempre on-demand)\n",
        "   ‚Ä¢ Streaming en tiempo real\n",
        "   ‚Ä¢ Jobs cr√≠ticos con SLA\n",
        "   ‚Ä¢ Desarrollo interactivo (frustrante)\n",
        "\n",
        "ESTRATEGIA H√çBRIDA:\n",
        "‚îú‚îÄ Driver: On-demand (estabilidad)\n",
        "‚îú‚îÄ Min workers: On-demand (base garantizada)\n",
        "‚îî‚îÄ Workers extra (autoscaling): Spot (ahorro)\n",
        "\n",
        "AHORRO ESTIMADO:\n",
        "‚Ä¢ All on-demand: $100/hora\n",
        "‚Ä¢ H√≠brido (50% spot): $60/hora\n",
        "‚Ä¢ All spot (arriesgado): $30/hora\n",
        "\"\"\"\n",
        "\n",
        "print(spot_info)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Experimentando con Configuraci√≥n**\n",
        "\n",
        "```markdown\n",
        "üß™ EJERCICIO: EXPLORAR CONFIGURACI√ìN DE CLUSTER\n",
        "\n",
        "Como est√°s en Free Edition, muchas opciones no est√°n disponibles,\n",
        "pero es importante que veas d√≥nde estar√≠an.\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 1: EXPLORAR TU CLUSTER ACTUAL (5 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "1. **Ver configuraci√≥n actual**\n",
        "   - [ ] Compute ‚Üí Tu cluster\n",
        "   - [ ] Click en el nombre del cluster\n",
        "   - [ ] Pesta√±a \"Configuration\"\n",
        "\n",
        "2. **Documentar configuraci√≥n**:\n",
        "   \n",
        "   Databricks Runtime Version: ___________________\n",
        "   \n",
        "   Driver: ___________________\n",
        "   \n",
        "   Workers: ___________________\n",
        "   \n",
        "   Autotermination: ___________________\n",
        "   \n",
        "   Spark Config (si visible): ___________________\n",
        "\n",
        "3. **Intentar editar** (para ver qu√© est√° bloqueado):\n",
        "   - [ ] Click en \"Edit\"\n",
        "   - [ ] Observa qu√© campos est√°n:\n",
        "     ‚òê Editables (pocos)\n",
        "     ‚òê Deshabilitados (la mayor√≠a)\n",
        "   - [ ] NO guardes cambios, solo explora\n",
        "   - [ ] Click \"Cancel\"\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 2: MONITOREAR AUTOTERMINATION (10 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Vamos a observar el autotermination en acci√≥n:\n",
        "\n",
        "4. **Setup inicial**:\n",
        "   - [ ] Aseg√∫rate de que tu cluster est√© Running\n",
        "   - [ ] Anota la hora: _____________\n",
        "\n",
        "5. **Ejecutar algo simple**:\n",
        "   - [ ] Abre cualquier notebook\n",
        "   - [ ] Ejecuta: `print(\"Test\")`\n",
        "   - [ ] Esto \"reinicia\" el contador de inactividad\n",
        "\n",
        "6. **Esperar y observar** (opcional - toma 2 horas):\n",
        "   - [ ] NO ejecutes nada m√°s\n",
        "   - [ ] Deja la pesta√±a abierta\n",
        "   - [ ] Revisa cada 30 minutos\n",
        "   - [ ] Documenta:\n",
        "   \n",
        "   30 min: Estado: ___________\n",
        "   60 min: Estado: ___________\n",
        "   90 min: Estado: ___________\n",
        "   120 min: Estado: ___________\n",
        "   \n",
        "   ‚ö†Ô∏è ALTERNATIVA si no quieres esperar 2 horas:\n",
        "   Simplemente entiende el concepto y pasa a la siguiente parte\n",
        "\n",
        "7. **Terminar manualmente**:\n",
        "   - [ ] Compute ‚Üí Terminate\n",
        "   - [ ] Observa c√≥mo cambia el estado\n",
        "   - [ ] Esto simula el autotermination\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 3: SIMULAR CONFIGURACI√ìN PREMIUM (5 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Aunque no puedes cambiar la configuraci√≥n en Free Edition,\n",
        "vamos a planificar c√≥mo configurar√≠as un cluster en Premium:\n",
        "\n",
        "8. **Dise√±a tu cluster ideal**:\n",
        "\n",
        "   Imagina que necesitas procesar 100 GB de datos de ventas\n",
        "   diariamente:\n",
        "   \n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ MI CONFIGURACI√ìN IDEAL                         ‚îÇ\n",
        "   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "   ‚îÇ                                                ‚îÇ\n",
        "   ‚îÇ Nombre: _____________________________          ‚îÇ\n",
        "   ‚îÇ                                                ‚îÇ\n",
        "   ‚îÇ Workers:                                       ‚îÇ\n",
        "   ‚îÇ   Min: ______                                  ‚îÇ\n",
        "   ‚îÇ   Max: ______                                  ‚îÇ\n",
        "   ‚îÇ   Raz√≥n: _____________________________         ‚îÇ\n",
        "   ‚îÇ                                                ‚îÇ\n",
        "   ‚îÇ Autoscaling: ‚òê S√≠  ‚òê No                        ‚îÇ\n",
        "   ‚îÇ   Raz√≥n: _____________________________         ‚îÇ\n",
        "   ‚îÇ                                                ‚îÇ\n",
        "   ‚îÇ Autotermination: ______ minutos                ‚îÇ\n",
        "   ‚îÇ   Raz√≥n: _____________________________         ‚îÇ\n",
        "   ‚îÇ                                                ‚îÇ\n",
        "   ‚îÇ Instance type:                                 ‚îÇ\n",
        "   ‚îÇ   Driver: _____________________________        ‚îÇ\n",
        "   ‚îÇ   Workers: ____________________________        ‚îÇ\n",
        "   ‚îÇ   Raz√≥n: _____________________________         ‚îÇ\n",
        "   ‚îÇ                                                ‚îÇ\n",
        "   ‚îÇ Spot instances: ‚òê S√≠  ‚òê No                     ‚îÇ\n",
        "   ‚îÇ   Raz√≥n: _____________________________         ‚îÇ\n",
        "   ‚îÇ                                                ‚îÇ\n",
        "   ‚îÇ Costo estimado: $______/hora                   ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 4: DOCUMENTAR APRENDIZAJES (5 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "9. **Reflexi√≥n**:\n",
        "\n",
        "   ¬øQu√© configuraci√≥n te parece m√°s importante?\n",
        "   _____________________________________________\n",
        "   \n",
        "   ¬øQu√© har√≠as diferente en Free vs Premium?\n",
        "   _____________________________________________\n",
        "   \n",
        "   ¬øEntiendes por qu√© autotermination es crucial?\n",
        "   _____________________________________________\n",
        "   \n",
        "   ¬øQu√© configuraci√≥n te genera m√°s dudas?\n",
        "   _____________________________________________\n",
        "\n",
        "**TIEMPO TOTAL**: ~25 minutos (sin esperar 2h de autotermination)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 3.4 - Casos de Uso**\n",
        "\n",
        "```markdown\n",
        "### Ejercicio: Configurar Cluster para Diferentes Escenarios\n",
        "\n",
        "**Objetivo**: Practicar toma de decisiones sobre configuraci√≥n\n",
        "\n",
        "**Instrucciones**: Para cada escenario, decide la configuraci√≥n ideal\n",
        "\n",
        "---\n",
        "\n",
        "**ESCENARIO 1: Startup de Analytics**\n",
        "\n",
        "Contexto:\n",
        "- Equipo de 3 analistas\n",
        "- 50 GB de datos\n",
        "- Consultas SQL ad-hoc\n",
        "- Presupuesto limitado ($500/mes)\n",
        "\n",
        "Tu configuraci√≥n:\n",
        "```\n",
        "Workers: Min ____ Max ____\n",
        "Autoscaling: ‚òê S√≠  ‚òê No\n",
        "Autotermination: ____ minutos\n",
        "Instance type: _______________\n",
        "Spot: ‚òê S√≠  ‚òê No\n",
        "\n",
        "Justificaci√≥n:\n",
        "_________________________________\n",
        "_________________________________\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**ESCENARIO 2: ETL de Producci√≥n**\n",
        "\n",
        "Contexto:\n",
        "- Pipeline diario autom√°tico\n",
        "- 500 GB de datos\n",
        "- Corre de 2 AM a 6 AM\n",
        "- SLA: Debe terminar antes de 7 AM\n",
        "\n",
        "Tu configuraci√≥n:\n",
        "```\n",
        "Workers: Min ____ Max ____\n",
        "Autoscaling: ‚òê S√≠  ‚òê No\n",
        "Autotermination: ____ minutos\n",
        "Instance type: _______________\n",
        "Spot: ‚òê S√≠  ‚òê No\n",
        "\n",
        "Justificaci√≥n:\n",
        "_________________________________\n",
        "_________________________________\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**ESCENARIO 3: Experimentaci√≥n de ML**\n",
        "\n",
        "Contexto:\n",
        "- 1 data scientist\n",
        "- Entrenamiento de modelos\n",
        "- Datasets de 10-100 GB\n",
        "- Experimentos de 1-8 horas\n",
        "- Uso espor√°dico\n",
        "\n",
        "Tu configuraci√≥n:\n",
        "```\n",
        "Workers: Min ____ Max ____\n",
        "Autoscaling: ‚òê S√≠  ‚òê No\n",
        "Autotermination: ____ minutos\n",
        "Instance type: _______________\n",
        "Spot: ‚òê S√≠  ‚òê No\n",
        "Runtime: Standard / ML / GPU\n",
        "\n",
        "Justificaci√≥n:\n",
        "_________________________________\n",
        "_________________________________\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**ESCENARIO 4: Streaming en Tiempo Real**\n",
        "\n",
        "Contexto:\n",
        "- Procesa eventos 24/7\n",
        "- 10K eventos/segundo\n",
        "- Latencia cr√≠tica (<1 seg)\n",
        "- Alta disponibilidad requerida\n",
        "\n",
        "Tu configuraci√≥n:\n",
        "```\n",
        "Workers: Min ____ Max ____\n",
        "Autoscaling: ‚òê S√≠  ‚òê No\n",
        "Autotermination: ____ minutos\n",
        "Instance type: _______________\n",
        "Spot: ‚òê S√≠  ‚òê No\n",
        "\n",
        "Justificaci√≥n:\n",
        "_________________________________\n",
        "_________________________________\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Respuestas Sugeridas** (verifica despu√©s de intentar):\n",
        "\n",
        "<details>\n",
        "<summary>Click para ver respuestas</summary>\n",
        "\n",
        "**Escenario 1**: Min 2, Max 6, Autoscaling S√≠, 60 min, General Purpose, Spot h√≠brido\n",
        "**Escenario 2**: Min 10, Max 20, Autoscaling S√≠, 10 min, Memoria-opt, Spot workers\n",
        "**Escenario 3**: Min 1, Max 8, Autoscaling S√≠, 120 min, GPU si DL/Memoria-opt, Spot No\n",
        "**Escenario 4**: Min 8, Max 12, Autoscaling S√≠, Desactivado, Compute-opt, Spot No\n",
        "</details>\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **Autotermination** es CR√çTICO para controlar costos\n",
        "2. **Free Edition** = 2 horas fijas de autotermination\n",
        "3. **Autoscaling** ajusta workers seg√∫n carga (Premium only)\n",
        "4. **Configuraci√≥n afecta**: Costo, Rendimiento, Disponibilidad\n",
        "5. **Driver** coordina, **Workers** procesan\n",
        "6. **Runtime version** determina versiones de Spark/librer√≠as\n",
        "7. **Spot instances** = 80% descuento pero pueden interrumpirse\n",
        "8. **Instance pools** = Clusters arrancan m√°s r√°pido (Premium)\n",
        "9. **Cluster policies** = Governance y control (Enterprise)\n",
        "10. **Balance** entre costo y rendimiento es clave\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **Siempre usa autotermination en desarrollo**\n",
        "```markdown\n",
        "Un cluster olvidado puede costar $100-$1000/d√≠a\n",
        "30-60 minutos es buen balance\n",
        "```\n",
        "\n",
        "‚úÖ **Empieza peque√±o, escala si necesitas**\n",
        "```markdown\n",
        "M√°s f√°cil aumentar que justificar costos altos\n",
        "Mide primero, optimiza despu√©s\n",
        "```\n",
        "\n",
        "‚úÖ **Driver = Worker en la mayor√≠a de casos**\n",
        "```markdown\n",
        "Solo haz driver m√°s grande si:\n",
        "- Usas collect() frecuentemente\n",
        "- Retornas muchos datos al notebook\n",
        "```\n",
        "\n",
        "‚úÖ **Spot para workers, nunca para driver**\n",
        "```markdown\n",
        "Si el driver se interrumpe, pierdes todo\n",
        "Workers se pueden reemplazar\n",
        "```\n",
        "\n",
        "‚úÖ **Documenta tu configuraci√≥n**\n",
        "```markdown\n",
        "Anota por qu√© elegiste cada valor\n",
        "√ötil para auditor√≠as y optimizaci√≥n\n",
        "```\n",
        "\n",
        "‚úÖ **Monitorea uso real**\n",
        "```markdown\n",
        "Spark UI muestra uso de CPU/memoria\n",
        "Ajusta configuraci√≥n basado en m√©tricas reales\n",
        "No adivines\n",
        "```\n",
        "\n",
        "‚úÖ **En Free Edition, optimiza c√≥digo no cluster**\n",
        "```markdown\n",
        "No puedes cambiar el cluster\n",
        "Enf√≥cate en escribir c√≥digo eficiente\n",
        "Usa .cache() apropiadamente\n",
        "Minimiza shuffles\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Resumen: Configuraci√≥n por Tipo de Trabajo**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "QUICK REFERENCE: CONFIGURACI√ìN POR CASO DE USO\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìã QUICK REFERENCE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "quick_ref = \"\"\"\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ DESARROLLO INTERACTIVO                                  ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Workers: 2-4 (min) a 8-10 (max)                         ‚îÇ\n",
        "‚îÇ Autoscaling: S√≠                                         ‚îÇ\n",
        "‚îÇ Autotermination: 60 minutos                             ‚îÇ\n",
        "‚îÇ Tipo: General Purpose                                   ‚îÇ\n",
        "‚îÇ Spot: No                                                ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ ETL BATCH (Producci√≥n)                                  ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Workers: 4-8 (min) a 20-40 (max)                        ‚îÇ\n",
        "‚îÇ Autoscaling: S√≠                                         ‚îÇ\n",
        "‚îÇ Autotermination: 10 minutos o Job cluster               ‚îÇ\n",
        "‚îÇ Tipo: Memoria-optimizada                                ‚îÇ\n",
        "‚îÇ Spot: S√≠ (workers)                                      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ MACHINE LEARNING                                        ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Workers: 2-4 (min) a 8-16 (max)                         ‚îÇ\n",
        "‚îÇ Autoscaling: S√≠                                         ‚îÇ\n",
        "‚îÇ Autotermination: 120 minutos                            ‚îÇ\n",
        "‚îÇ Tipo: GPU (DL) o Memoria-opt (ML cl√°sico)               ‚îÇ\n",
        "‚îÇ Spot: No (experimentos valiosos)                        ‚îÇ\n",
        "‚îÇ Runtime: ML                                             ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ STREAMING 24/7                                          ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Workers: 8-16 (min) a 20-30 (max)                       ‚îÇ\n",
        "‚îÇ Autoscaling: S√≠ (conservador)                           ‚îÇ\n",
        "‚îÇ Autotermination: Desactivado                            ‚îÇ\n",
        "‚îÇ Tipo: Compute-optimizada                                ‚îÇ\n",
        "‚îÇ Spot: No (alta disponibilidad)                          ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ APRENDIZAJE (FREE EDITION)                              ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Workers: 0 (fijo)                                       ‚îÇ\n",
        "‚îÇ Autoscaling: N/A                                        ‚îÇ\n",
        "‚îÇ Autotermination: 120 min (fijo)                         ‚îÇ\n",
        "‚îÇ Tipo: Fijo (~15GB RAM)                                  ‚îÇ\n",
        "‚îÇ Spot: N/A                                               ‚îÇ\n",
        "‚îÇ üí° Optimiza c√≥digo, no cluster                          ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\"\"\"\n",
        "\n",
        "print(quick_ref)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Has completado la configuraci√≥n de clusters! üéâ**\n",
        "\n",
        "Ahora entiendes:\n",
        "- Autotermination y su importancia ‚úÖ\n",
        "- Autoscaling y elasticidad ‚úÖ\n",
        "- Configuraci√≥n de workers y driver ‚úÖ\n",
        "- Spot instances y ahorro de costos ‚úÖ\n",
        "- C√≥mo configurar seg√∫n caso de uso ‚úÖ\n",
        "- Diferencias Free vs Premium ‚úÖ\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el **Tema 4: Introducci√≥n a Notebooks** donde finalmente empezaremos a escribir y ejecutar c√≥digo? üöÄ"
      ],
      "metadata": {
        "id": "aS6fEXladUVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tema 4: Introducci√≥n a Notebooks**\n",
        "\n",
        "---\n",
        "\n",
        "### **4.1 Creaci√≥n y ejecuci√≥n de notebooks**\n",
        "\n",
        "#### **Introducci√≥n: Los Notebooks - Tu Espacio de Trabajo**\n",
        "\n",
        "Los notebooks son el coraz√≥n de Databricks. Son documentos interactivos donde escribes c√≥digo, visualizas resultados, y documentas tu trabajo, todo en un solo lugar.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "¬øQU√â ES UN NOTEBOOK?\n",
        "\"\"\"\n",
        "\n",
        "notebook_concepto = {\n",
        "    'definici√≥n': 'Documento interactivo con c√≥digo ejecutable y texto',\n",
        "    'origen': 'Inspirado en Jupyter Notebooks',\n",
        "    'componentes': [\n",
        "        'Celdas de c√≥digo (ejecutables)',\n",
        "        'Celdas de markdown (documentaci√≥n)',\n",
        "        'Resultados inline',\n",
        "        'Visualizaciones'\n",
        "    ],\n",
        "    'analog√≠a': 'Como un documento de Word, pero con c√≥digo que se ejecuta',\n",
        "    'ventajas': [\n",
        "        'Desarrollo iterativo',\n",
        "        'Documentaci√≥n integrada',\n",
        "        'Resultados visuales inmediatos',\n",
        "        'Compartible con equipo',\n",
        "        'Reproducible'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"üìì NOTEBOOKS EN DATABRICKS\")\n",
        "print(\"=\"*60)\n",
        "for key, value in notebook_concepto.items():\n",
        "    if isinstance(value, list):\n",
        "        print(f\"\\n{key.replace('_', ' ').title()}:\")\n",
        "        for item in value:\n",
        "            print(f\"  ‚Ä¢ {item}\")\n",
        "    else:\n",
        "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 1: Crear tu Primer Notebook**\n",
        "\n",
        "#### **M√©todo 1: Desde Home (Recomendado para principiantes)**\n",
        "\n",
        "```markdown\n",
        "üìù CREAR NOTEBOOK DESDE HOME\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PASO 1: ACCEDER A HOME\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "1. Click en üè† Home (sidebar)\n",
        "\n",
        "2. Busca secci√≥n \"Quick actions\" o bot√≥n \"Create\"\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PASO 2: CREAR NOTEBOOK\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "3. Click en \"Create\" ‚Üí \"Notebook\"\n",
        "   O: Click en \"+ New\" ‚Üí \"Notebook\"\n",
        "\n",
        "4. Aparece di√°logo de creaci√≥n:\n",
        "\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ Create Notebook                                ‚îÇ\n",
        "   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "   ‚îÇ                                                ‚îÇ\n",
        "   ‚îÇ Name: [Mi_Primer_Notebook        ]             ‚îÇ\n",
        "   ‚îÇ                                                ‚îÇ\n",
        "   ‚îÇ Default Language: [Python    ‚ñº]                ‚îÇ\n",
        "   ‚îÇ                                                ‚îÇ\n",
        "   ‚îÇ Cluster: [mi-primer-cluster  ‚ñº]                ‚îÇ\n",
        "   ‚îÇ          (opcional)                            ‚îÇ\n",
        "   ‚îÇ                                                ‚îÇ\n",
        "   ‚îÇ [ Cancel ]              [ Create ]             ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "5. Completar campos:\n",
        "   \n",
        "   Name:\n",
        "   ‚Ä¢ Descriptivo\n",
        "   ‚Ä¢ Sin espacios (usa guiones bajos)\n",
        "   ‚Ä¢ Ejemplo: \"Mi_Primer_Notebook\"\n",
        "   \n",
        "   Default Language:\n",
        "   ‚Ä¢ Python (recomendado para principiantes)\n",
        "   ‚Ä¢ SQL (si prefieres queries)\n",
        "   ‚Ä¢ Scala\n",
        "   ‚Ä¢ R\n",
        "   \n",
        "   Cluster:\n",
        "   ‚Ä¢ Selecciona tu cluster si est√° corriendo\n",
        "   ‚Ä¢ Puedes dejarlo vac√≠o y asignar despu√©s\n",
        "   ‚Ä¢ Si el cluster no est√° corriendo, necesitar√°s iniciarlo\n",
        "\n",
        "6. Click \"Create\"\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PASO 3: NOTEBOOK CREADO\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "7. Se abre el notebook en una nueva pesta√±a del navegador\n",
        "\n",
        "8. Ubicaci√≥n por defecto:\n",
        "   Workspace ‚Üí Users ‚Üí tu-email ‚Üí [nombre_notebook]\n",
        "```\n",
        "\n",
        "#### **M√©todo 2: Desde Workspace (M√°s control)**\n",
        "\n",
        "```markdown\n",
        "üìù CREAR NOTEBOOK DESDE WORKSPACE\n",
        "\n",
        "1. Sidebar ‚Üí üìÅ Workspace\n",
        "\n",
        "2. Navega a la carpeta donde quieres crear el notebook\n",
        "   Ejemplo: Users ‚Üí tu-email ‚Üí 00_Curso_Databricks\n",
        "\n",
        "3. Click derecho en la carpeta\n",
        "\n",
        "4. Create ‚Üí Notebook\n",
        "\n",
        "5. Mismo di√°logo que M√©todo 1\n",
        "\n",
        "6. Ventaja: Control exacto de ubicaci√≥n\n",
        "```\n",
        "\n",
        "#### **M√©todo 3: Con Atajo de Teclado**\n",
        "\n",
        "```markdown\n",
        "‚å®Ô∏è ATAJO R√ÅPIDO\n",
        "\n",
        "1. Presiona Ctrl+N (Cmd+N en Mac)\n",
        "\n",
        "2. Se abre di√°logo de creaci√≥n directamente\n",
        "\n",
        "3. Ubicaci√≥n: Carpeta actual donde est√©s navegando\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 2: Anatom√≠a de un Notebook**\n",
        "\n",
        "```markdown\n",
        "üîç COMPONENTES DEL NOTEBOOK\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ BARRA SUPERIOR DEL NOTEBOOK                             ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ [Nombre] | üêçPython | üîó[Cluster] | ‚ö°Run All | ‚ãÆ More  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "       ‚ñ≤         ‚ñ≤           ‚ñ≤            ‚ñ≤          ‚ñ≤\n",
        "    Editable  Lenguaje   Asignar     Ejecutar    Men√∫\n",
        "    (click)   default    cluster     todo        opciones\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ √ÅREA DE CELDAS                                          ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                         ‚îÇ\n",
        "‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
        "‚îÇ ‚îÇ Celda 1 [Cmd 1]              | ‚ñ∂ Run | ‚ãÆ |     ‚îÇ     ‚îÇ\n",
        "‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§     ‚îÇ\n",
        "‚îÇ ‚îÇ print(\"Hola Databricks\")                        ‚îÇ     ‚îÇ\n",
        "‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
        "‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
        "‚îÇ ‚îÇ Resultado:                                      ‚îÇ     ‚îÇ\n",
        "‚îÇ ‚îÇ Hola Databricks                                 ‚îÇ     ‚îÇ\n",
        "‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
        "‚îÇ                                                         ‚îÇ\n",
        "‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
        "‚îÇ ‚îÇ Celda 2 [Cmd 2]              | ‚ñ∂ Run | ‚ãÆ |     ‚îÇ     ‚îÇ\n",
        "‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§     ‚îÇ\n",
        "‚îÇ ‚îÇ                                                 ‚îÇ     ‚îÇ\n",
        "‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
        "‚îÇ                                                         ‚îÇ\n",
        "‚îÇ [ + Code ] [ + Markdown ]                               ‚îÇ\n",
        "‚îÇ                                                         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ SIDEBAR DERECHO (Opcional)                              ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ ‚Ä¢ Table of Contents                                     ‚îÇ\n",
        "‚îÇ ‚Ä¢ Comments                                              ‚îÇ\n",
        "‚îÇ ‚Ä¢ Revision History                                      ‚îÇ\n",
        "‚îÇ ‚Ä¢ Git Status (si conectado a repo)                      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "#### **Elementos Clave**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ELEMENTOS DEL NOTEBOOK\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîë ELEMENTOS PRINCIPALES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "elementos = {\n",
        "    '1. Nombre del Notebook': {\n",
        "        'ubicaci√≥n': 'Esquina superior izquierda',\n",
        "        'editable': 'S√≠ - click para editar',\n",
        "        'formato': 'Sin extensi√≥n (.py se a√±ade autom√°ticamente)',\n",
        "        'uso': 'Identificar el notebook'\n",
        "    },\n",
        "    \n",
        "    '2. Lenguaje Default': {\n",
        "        'ubicaci√≥n': 'Junto al nombre',\n",
        "        'muestra': '√çcono (üêçPython, üìäSQL, etc.)',\n",
        "        'cambiable': 'S√≠ - click para cambiar',\n",
        "        'efecto': 'Lenguaje por defecto de nuevas celdas'\n",
        "    },\n",
        "    \n",
        "    '3. Cluster Attachment': {\n",
        "        'ubicaci√≥n': 'Centro superior',\n",
        "        'estados': [\n",
        "            'üîó Conectado (cluster name)',\n",
        "            '‚ö†Ô∏è Detached (sin cluster)',\n",
        "            '‚è≥ Starting... (arrancando)'\n",
        "        ],\n",
        "        'acci√≥n': 'Click para conectar/cambiar cluster',\n",
        "        'cr√≠tico': 'Sin cluster NO puedes ejecutar c√≥digo'\n",
        "    },\n",
        "    \n",
        "    '4. Run All': {\n",
        "        'ubicaci√≥n': 'Derecha del cluster',\n",
        "        'funci√≥n': 'Ejecutar todas las celdas en orden',\n",
        "        'uso': 'Cuando quieres ejecutar notebook completo',\n",
        "        'atajo': 'Ctrl+Shift+Enter (Cmd+Shift+Enter en Mac)'\n",
        "    },\n",
        "    \n",
        "    '5. Men√∫ More (‚ãÆ)': {\n",
        "        'ubicaci√≥n': 'Extremo derecho',\n",
        "        'opciones': [\n",
        "            'Clear Results',\n",
        "            'Export',\n",
        "            'Revision History',\n",
        "            'Clone',\n",
        "            'Move',\n",
        "            'Delete',\n",
        "            'Keyboard shortcuts'\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    '6. Celdas': {\n",
        "        'tipos': ['Code', 'Markdown'],\n",
        "        'numeraci√≥n': 'Cmd 1, Cmd 2, ... (autom√°tica)',\n",
        "        'orden': 'Se ejecutan en el orden que t√∫ elijas',\n",
        "        'estado': 'Muestra si fue ejecutada y cu√°ndo'\n",
        "    },\n",
        "    \n",
        "    '7. Botones de Acci√≥n de Celda': {\n",
        "        'run': '‚ñ∂ - Ejecutar esta celda',\n",
        "        'menu': '‚ãÆ - Opciones (cortar, copiar, mover, etc.)',\n",
        "        'ubicaci√≥n': 'Derecha de cada celda'\n",
        "    },\n",
        "    \n",
        "    '8. √Årea de Resultados': {\n",
        "        'ubicaci√≥n': 'Debajo de cada celda ejecutada',\n",
        "        'muestra': [\n",
        "            'Texto/print output',\n",
        "            'Tablas (DataFrames)',\n",
        "            'Gr√°ficos',\n",
        "            'Errores',\n",
        "            'Logs'\n",
        "        ],\n",
        "        'colapsable': 'S√≠ - click en borde',\n",
        "        'clearable': 'S√≠ - men√∫ de celda ‚Üí Clear'\n",
        "    }\n",
        "}\n",
        "\n",
        "for elemento, info in elementos.items():\n",
        "    print(f\"\\n{elemento}\")\n",
        "    for key, value in info.items():\n",
        "        if isinstance(value, list):\n",
        "            print(f\"  {key}:\")\n",
        "            for item in value:\n",
        "                print(f\"    ‚Ä¢ {item}\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 3: Conectar Notebook a Cluster**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CONECTAR NOTEBOOK A CLUSTER\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîó CONECTAR A CLUSTER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "conexion_info = \"\"\"\n",
        "¬øPOR QU√â ES NECESARIO?\n",
        "‚Ä¢ Notebooks contienen c√≥digo\n",
        "‚Ä¢ C√≥digo necesita ejecutarse en alg√∫n lugar\n",
        "‚Ä¢ Ese \"lugar\" es el cluster\n",
        "‚Ä¢ Sin cluster = Sin ejecuci√≥n\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "ESCENARIO 1: Cluster ya corriendo\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "1. Click en el dropdown de cluster (arriba del notebook)\n",
        "\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ ‚ö†Ô∏è Detached                       ‚ñº   ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "2. Aparece lista de clusters:\n",
        "\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ üü¢ mi-primer-cluster        (Running)  ‚îÇ\n",
        "   ‚îÇ ‚ö™ old-cluster           (Terminated)  ‚îÇ\n",
        "   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ\n",
        "   ‚îÇ Start New Cluster...                  ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "3. Click en el cluster corriendo (üü¢)\n",
        "\n",
        "4. Estado cambia a:\n",
        "\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ üü¢ mi-primer-cluster              ‚ñº    ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "5. ‚úÖ Ya puedes ejecutar c√≥digo\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "ESCENARIO 2: Cluster detenido\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "1. Click en dropdown de cluster\n",
        "\n",
        "2. Clusters disponibles est√°n en estado Terminated:\n",
        "\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ ‚ö™ mi-primer-cluster     (Terminated)  ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "3. Click en el cluster detenido\n",
        "\n",
        "4. Pregunta si quieres arrancarlo:\n",
        "\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ Start cluster \"mi-primer-cluster\"?     ‚îÇ\n",
        "   ‚îÇ                                        ‚îÇ\n",
        "   ‚îÇ This will take 2-5 minutes             ‚îÇ\n",
        "   ‚îÇ                                        ‚îÇ\n",
        "   ‚îÇ [ Cancel ]              [ Start ]      ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "5. Click \"Start\"\n",
        "\n",
        "6. Estado cambia a:\n",
        "\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ üü° mi-primer-cluster       (Starting)  ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "7. Espera 2-5 minutos...\n",
        "\n",
        "8. Estado final:\n",
        "\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ üü¢ mi-primer-cluster        (Running)  ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "9. ‚úÖ Ya puedes ejecutar c√≥digo\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "ESCENARIO 3: Crear nuevo cluster desde notebook\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "1. Click en dropdown de cluster\n",
        "\n",
        "2. Click en \"Start New Cluster...\" (al final de la lista)\n",
        "\n",
        "3. Se abre di√°logo de creaci√≥n (igual que en Compute)\n",
        "\n",
        "4. Crear cluster\n",
        "\n",
        "5. Esperar a que arranque\n",
        "\n",
        "6. Autom√°ticamente se conecta al notebook\n",
        "\n",
        "‚ö†Ô∏è NOTA: En Free Edition solo puedes tener 1 cluster a la vez\n",
        "\"\"\"\n",
        "\n",
        "print(conexion_info)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 4: Trabajar con Celdas**\n",
        "\n",
        "#### **Tipos de Celdas**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "TIPOS DE CELDAS EN NOTEBOOKS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüì¶ TIPOS DE CELDAS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "tipos_celdas = \"\"\"\n",
        "1Ô∏è‚É£ CELDA DE C√ìDIGO\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Apariencia:\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Cmd 1                                  | ‚ñ∂ Run | ‚ãÆ ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ # C√≥digo ejecutable                                 ‚îÇ\n",
        "‚îÇ print(\"Hola mundo\")                                 ‚îÇ\n",
        "‚îÇ                                                     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "Caracter√≠sticas:\n",
        "‚Ä¢ Fondo gris claro\n",
        "‚Ä¢ Editor de c√≥digo con syntax highlighting\n",
        "‚Ä¢ Bot√≥n Run visible\n",
        "‚Ä¢ N√∫mero de comando (Cmd X)\n",
        "‚Ä¢ Ejecutable con Shift+Enter\n",
        "\n",
        "Uso:\n",
        "‚Ä¢ Escribir c√≥digo Python/SQL/Scala/R\n",
        "‚Ä¢ Ejecutar transformaciones\n",
        "‚Ä¢ Crear visualizaciones\n",
        "‚Ä¢ Debugging\n",
        "\n",
        "2Ô∏è‚É£ CELDA MARKDOWN\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Apariencia (mientras editas):\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Cmd 2                                  | ‚ñ∂ Run | ‚ãÆ ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ # Mi T√≠tulo                                         ‚îÇ\n",
        "‚îÇ Este es texto de documentaci√≥n                      ‚îÇ\n",
        "‚îÇ                                                     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "Apariencia (renderizado):\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Mi T√≠tulo                                           ‚îÇ\n",
        "‚îÇ ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                                 ‚îÇ\n",
        "‚îÇ Este es texto de documentaci√≥n                      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "Caracter√≠sticas:\n",
        "‚Ä¢ Fondo blanco\n",
        "‚Ä¢ Formato Markdown\n",
        "‚Ä¢ Se renderiza al ejecutar\n",
        "‚Ä¢ No ejecuta c√≥digo\n",
        "\n",
        "Uso:\n",
        "‚Ä¢ Documentar tu an√°lisis\n",
        "‚Ä¢ Explicar c√≥digo\n",
        "‚Ä¢ T√≠tulos y secciones\n",
        "‚Ä¢ Notas y observaciones\n",
        "\n",
        "SINTAXIS MARKDOWN B√ÅSICA:\n",
        "\n",
        "# T√≠tulo 1\n",
        "## T√≠tulo 2\n",
        "### T√≠tulo 3\n",
        "\n",
        "**Negrita**\n",
        "*Cursiva*\n",
        "`c√≥digo inline`\n",
        "\n",
        "- Lista item 1\n",
        "- Lista item 2\n",
        "\n",
        "1. Lista numerada\n",
        "2. Item 2\n",
        "\n",
        "[Link](https://www.example.com)\n",
        "\n",
        "```python\n",
        "# Bloque de c√≥digo\n",
        "print(\"Ejemplo\")\n",
        "```\n",
        "\n",
        "> Cita o nota importante\n",
        "\n",
        "---\n",
        "(l√≠nea horizontal)\n",
        "\"\"\"\n",
        "\n",
        "print(tipos_celdas)\n",
        "```\n",
        "\n",
        "#### **Crear y Manipular Celdas**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "OPERACIONES CON CELDAS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîß OPERACIONES CON CELDAS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "operaciones_celdas = \"\"\"\n",
        "CREAR CELDA NUEVA\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "M√©todo 1: Botones inferiores\n",
        "‚Ä¢ Al final del notebook ver√°s:\n",
        "  [ + Code ] [ + Markdown ]\n",
        "‚Ä¢ Click en el bot√≥n apropiado\n",
        "\n",
        "M√©todo 2: Hover entre celdas\n",
        "‚Ä¢ Pasa mouse entre dos celdas\n",
        "‚Ä¢ Aparecen botones [ + Code ] [ + Markdown ]\n",
        "‚Ä¢ Click para insertar entre celdas\n",
        "\n",
        "M√©todo 3: Men√∫ de celda\n",
        "‚Ä¢ Click en ‚ãÆ (men√∫ de celda)\n",
        "‚Ä¢ Add Cell Above / Add Cell Below\n",
        "\n",
        "M√©todo 4: Atajos de teclado\n",
        "‚Ä¢ B - Insertar celda debajo (Below)\n",
        "‚Ä¢ A - Insertar celda arriba (Above)\n",
        "‚Ä¢ (Modo comando - presiona Esc primero)\n",
        "\n",
        "EJECUTAR CELDA\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "M√©todo 1: Bot√≥n Run\n",
        "‚Ä¢ Click en ‚ñ∂ Run\n",
        "\n",
        "M√©todo 2: Atajo de teclado (RECOMENDADO)\n",
        "‚Ä¢ Shift+Enter - Ejecutar y avanzar a siguiente celda\n",
        "‚Ä¢ Ctrl+Enter (Cmd+Enter) - Ejecutar sin avanzar\n",
        "‚Ä¢ Ctrl+Shift+Enter - Ejecutar todas las celdas\n",
        "\n",
        "EDITAR CELDA\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "‚Ä¢ Click en el c√≥digo para entrar en modo edici√≥n\n",
        "‚Ä¢ Escribe tu c√≥digo\n",
        "‚Ä¢ Syntax highlighting autom√°tico\n",
        "‚Ä¢ Autocompletado con Tab\n",
        "\n",
        "CAMBIAR TIPO DE CELDA\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "‚Ä¢ Click en ‚ãÆ (men√∫ celda)\n",
        "‚Ä¢ Convert to ‚Üí Code / Markdown\n",
        "‚Ä¢ O: Usa magic commands (%md para markdown)\n",
        "\n",
        "MOVER CELDA\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "M√©todo 1: Drag and drop\n",
        "‚Ä¢ Click y arrastra desde el n√∫mero de celda\n",
        "\n",
        "M√©todo 2: Men√∫\n",
        "‚Ä¢ ‚ãÆ ‚Üí Move Up / Move Down\n",
        "\n",
        "M√©todo 3: Atajos (modo comando)\n",
        "‚Ä¢ Ctrl+Alt+‚Üë - Mover arriba\n",
        "‚Ä¢ Ctrl+Alt+‚Üì - Mover abajo\n",
        "\n",
        "COPIAR/CORTAR/PEGAR CELDA\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "‚Ä¢ ‚ãÆ ‚Üí Cut / Copy / Paste\n",
        "‚Ä¢ O atajos:\n",
        "  - X - Cortar (modo comando)\n",
        "  - C - Copiar (modo comando)\n",
        "  - V - Pegar (modo comando)\n",
        "\n",
        "ELIMINAR CELDA\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "‚Ä¢ ‚ãÆ ‚Üí Delete Cell\n",
        "‚Ä¢ O atajo:\n",
        "  - D,D - Presionar D dos veces (modo comando)\n",
        "\n",
        "‚ö†Ô∏è CUIDADO: No hay \"undo\" f√°cil para eliminar\n",
        "\n",
        "LIMPIAR RESULTADOS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Una celda:\n",
        "‚Ä¢ ‚ãÆ ‚Üí Clear Result\n",
        "\n",
        "Todas las celdas:\n",
        "‚Ä¢ Men√∫ notebook (arriba) ‚Üí Clear ‚Üí All Results\n",
        "\n",
        "COLAPSAR/EXPANDIR CELDA\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "‚Ä¢ Click en la l√≠nea izquierda de la celda\n",
        "‚Ä¢ √ötil para ocultar c√≥digo largo temporalmente\n",
        "\"\"\"\n",
        "\n",
        "print(operaciones_celdas)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 5: Ejecutar C√≥digo**\n",
        "\n",
        "#### **Tu Primer C√≥digo**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "PRIMER C√ìDIGO EN DATABRICKS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüöÄ EJECUTA TU PRIMER C√ìDIGO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "primer_codigo = \"\"\"\n",
        "EJERCICIO PR√ÅCTICO:\n",
        "\n",
        "1. Aseg√∫rate de que tu cluster est√° conectado (üü¢)\n",
        "\n",
        "2. En la primera celda de c√≥digo, escribe:\n",
        "\n",
        "   print(\"¬°Hola Databricks!\")\n",
        "   print(\"Mi primer c√≥digo Spark\")\n",
        "\n",
        "3. Presiona Shift+Enter\n",
        "\n",
        "4. Deber√≠as ver:\n",
        "\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ ¬°Hola Databricks!                   ‚îÇ\n",
        "   ‚îÇ Mi primer c√≥digo Spark              ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚úÖ Si ves esto: ¬°Felicidades! Tu primer c√≥digo funcion√≥\n",
        "\n",
        "‚ùå Si ves error: Verifica que el cluster est√© conectado\n",
        "\"\"\"\n",
        "\n",
        "print(primer_codigo)\n",
        "```\n",
        "\n",
        "#### **Ejemplos de C√≥digo B√°sico**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "EJEMPLOS PARA PROBAR\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüíª C√ìDIGO DE EJEMPLO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ejemplos = \"\"\"\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "EJEMPLO 1: Operaciones B√°sicas Python\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Celda 1\n",
        "x = 10\n",
        "y = 20\n",
        "resultado = x + y\n",
        "print(f\"La suma de {x} + {y} = {resultado}\")\n",
        "\n",
        "# Resultado esperado:\n",
        "# La suma de 10 + 20 = 30\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "EJEMPLO 2: Verificar Spark\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Celda 2\n",
        "print(f\"Versi√≥n de Spark: {spark.version}\")\n",
        "print(f\"Spark funcionando: ‚úÖ\")\n",
        "\n",
        "# Resultado esperado:\n",
        "# Versi√≥n de Spark: 3.5.0\n",
        "# Spark funcionando: ‚úÖ\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "EJEMPLO 3: Crear DataFrame Simple\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Celda 3\n",
        "# Crear datos de ejemplo\n",
        "datos = [\n",
        "    (\"Alice\", 25, \"Madrid\"),\n",
        "    (\"Bob\", 30, \"Barcelona\"),\n",
        "    (\"Charlie\", 35, \"Valencia\")\n",
        "]\n",
        "\n",
        "# Crear DataFrame\n",
        "df = spark.createDataFrame(datos, [\"nombre\", \"edad\", \"ciudad\"])\n",
        "\n",
        "# Mostrar\n",
        "df.show()\n",
        "\n",
        "# Resultado esperado:\n",
        "# +-------+----+---------+\n",
        "# | nombre|edad|   ciudad|\n",
        "# +-------+----+---------+\n",
        "# |  Alice|  25|   Madrid|\n",
        "# |    Bob|  30|Barcelona|\n",
        "# |Charlie|  35| Valencia|\n",
        "# +-------+----+---------+\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "EJEMPLO 4: Operaciones con DataFrame\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Celda 4\n",
        "# Filtrar personas mayores de 28 a√±os\n",
        "df_filtrado = df.filter(df.edad > 28)\n",
        "df_filtrado.show()\n",
        "\n",
        "# Resultado esperado:\n",
        "# +-------+----+---------+\n",
        "# | nombre|edad|   ciudad|\n",
        "# +-------+----+---------+\n",
        "# |    Bob|  30|Barcelona|\n",
        "# |Charlie|  35| Valencia|\n",
        "# +-------+----+---------+\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "EJEMPLO 5: Agregaciones\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Celda 5\n",
        "from pyspark.sql.functions import avg, max, min, count\n",
        "\n",
        "# Estad√≠sticas\n",
        "df.agg(\n",
        "    count(\"*\").alias(\"total_personas\"),\n",
        "    avg(\"edad\").alias(\"edad_promedio\"),\n",
        "    max(\"edad\").alias(\"edad_maxima\"),\n",
        "    min(\"edad\").alias(\"edad_minima\")\n",
        ").show()\n",
        "\n",
        "# Resultado esperado:\n",
        "# +--------------+--------------+------------+------------+\n",
        "# |total_personas|edad_promedio|edad_maxima|edad_minima|\n",
        "# +--------------+--------------+------------+------------+\n",
        "# |             3|          30.0|          35|          25|\n",
        "# +--------------+--------------+------------+------------+\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplos)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 6: Estados de Ejecuci√≥n**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ESTADOS DE EJECUCI√ìN DE CELDAS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n‚è±Ô∏è ESTADOS DE EJECUCI√ìN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "estados = \"\"\"\n",
        "UNA CELDA PUEDE ESTAR EN:\n",
        "\n",
        "1Ô∏è‚É£ NO EJECUTADA\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Cmd 1                                  | ‚ñ∂ Run | ‚ãÆ ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ print(\"Hola\")                                       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚Ä¢ Sin resultado debajo\n",
        "‚Ä¢ Nunca se ha ejecutado desde que abriste el notebook\n",
        "\n",
        "2Ô∏è‚É£ EJECUT√ÅNDOSE\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Cmd 1  ‚è≥ Running...                   | ‚è∏ | ‚ãÆ       ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ print(\"Hola\")                                       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚Ä¢ Spinner animado\n",
        "‚Ä¢ Bot√≥n Run cambia a ‚è∏ (Cancel)\n",
        "‚Ä¢ Puedes cancelar si tarda mucho\n",
        "\n",
        "3Ô∏è‚É£ EJECUTADA EXITOSAMENTE\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Cmd 1  ‚úÖ Ran 2m ago                   | ‚ñ∂ Run | ‚ãÆ ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ print(\"Hola\")                                       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Hola                                                ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚Ä¢ ‚úÖ Check verde\n",
        "‚Ä¢ Timestamp de cu√°ndo se ejecut√≥\n",
        "‚Ä¢ Resultado visible debajo\n",
        "\n",
        "4Ô∏è‚É£ ERROR\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Cmd 1  ‚ùå Failed                       | ‚ñ∂ Run | ‚ãÆ ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ print(variable_inexistente)                         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ NameError: name 'variable_inexistente' is not       ‚îÇ\n",
        "‚îÇ defined                                             ‚îÇ\n",
        "‚îÇ                                                     ‚îÇ\n",
        "‚îÇ [Stack trace completo...]                           ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚Ä¢ ‚ùå X roja\n",
        "‚Ä¢ Mensaje de error en rojo\n",
        "‚Ä¢ Stack trace para debugging\n",
        "\n",
        "ORDEN DE EJECUCI√ìN\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "‚ö†Ô∏è IMPORTANTE:\n",
        "Las celdas se ejecutan en el ORDEN que t√∫ las ejecutas,\n",
        "NO en el orden que aparecen en el notebook.\n",
        "\n",
        "Ejemplo:\n",
        "\n",
        "Celda 1: x = 10\n",
        "Celda 2: print(x)\n",
        "Celda 3: x = 20\n",
        "\n",
        "Si ejecutas: Celda 1 ‚Üí Celda 3 ‚Üí Celda 2\n",
        "Resultado: print(x) mostrar√° 20, no 10\n",
        "\n",
        "üí° MEJOR PR√ÅCTICA:\n",
        "‚Ä¢ Ejecuta celdas de arriba hacia abajo\n",
        "‚Ä¢ O usa \"Run All\" para ejecutar en orden\n",
        "‚Ä¢ Evita ejecutar celdas fuera de orden (confuso)\n",
        "\n",
        "REINICIAR ESTADO\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Si necesitas \"empezar de cero\":\n",
        "\n",
        "Opci√≥n 1: Restart Python (men√∫ notebook)\n",
        "‚Ä¢ Limpia todas las variables\n",
        "‚Ä¢ Mantiene los resultados visibles\n",
        "‚Ä¢ No detiene el cluster\n",
        "\n",
        "Opci√≥n 2: Detach & Reattach cluster\n",
        "‚Ä¢ Limpia TODO\n",
        "‚Ä¢ Como empezar de nuevo\n",
        "\n",
        "Opci√≥n 3: Clear ‚Üí All Results\n",
        "‚Ä¢ Solo limpia resultados visuales\n",
        "‚Ä¢ Variables siguen en memoria\n",
        "\"\"\"\n",
        "\n",
        "print(estados)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica Guiada - Tu Primer Notebook Completo**\n",
        "\n",
        "```markdown\n",
        "üß™ EJERCICIO: CREAR Y EJECUTAR NOTEBOOK COMPLETO\n",
        "\n",
        "Vamos a crear un notebook desde cero con varios tipos de celdas.\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 1: CREAR NOTEBOOK (3 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "1. **Crear notebook**:\n",
        "   - [ ] Home ‚Üí Create ‚Üí Notebook\n",
        "   - [ ] Name: \"Practica_4_1_Mi_Primer_Notebook\"\n",
        "   - [ ] Language: Python\n",
        "   - [ ] Cluster: (selecciona tu cluster)\n",
        "   - [ ] Create\n",
        "\n",
        "2. **Verificar conexi√≥n**:\n",
        "   - [ ] Arriba debe mostrar: üü¢ [nombre-cluster]\n",
        "   - [ ] Si no, conecta tu cluster\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 2: DOCUMENTACI√ìN (5 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "3. **Celda 1 - T√≠tulo (Markdown)**:\n",
        "   - [ ] En la primera celda, cambia a Markdown\n",
        "   - [ ] Escribe:\n",
        "   \n",
        "   ```markdown\n",
        "   # Mi Primer Notebook en Databricks\n",
        "   \n",
        "   **Autor**: [Tu Nombre]  \n",
        "   **Fecha**: 2025-02-02  \n",
        "   **Objetivo**: Practicar creaci√≥n y ejecuci√≥n de notebooks\n",
        "   \n",
        "   ## Secciones\n",
        "   1. Verificaci√≥n de Spark\n",
        "   2. Operaciones b√°sicas\n",
        "   3. DataFrames\n",
        "   4. An√°lisis simple\n",
        "   ```\n",
        "   \n",
        "   - [ ] Shift+Enter para ejecutar\n",
        "   - [ ] Verifica que se renderice correctamente\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 3: C√ìDIGO B√ÅSICO (10 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "4. **Celda 2 - Verificaci√≥n (Code)**:\n",
        "   - [ ] A√±ade celda de c√≥digo\n",
        "   - [ ] Escribe:\n",
        "   \n",
        "   ```python\n",
        "   # Verificar que Spark funciona\n",
        "   print(\"‚úÖ Spark est√° funcionando\")\n",
        "   print(f\"Versi√≥n: {spark.version}\")\n",
        "   print(f\"Aplicaci√≥n: {spark.sparkContext.appName}\")\n",
        "   ```\n",
        "   \n",
        "   - [ ] Shift+Enter\n",
        "   - [ ] Verifica resultado\n",
        "\n",
        "5. **Celda 3 - T√≠tulo secci√≥n (Markdown)**:\n",
        "   - [ ] A√±ade celda Markdown\n",
        "   - [ ] Escribe:\n",
        "   \n",
        "   ```markdown\n",
        "   ## 2. Operaciones B√°sicas\n",
        "   \n",
        "   Vamos a realizar algunas operaciones simples con Python.\n",
        "   ```\n",
        "   \n",
        "   - [ ] Shift+Enter\n",
        "\n",
        "6. **Celda 4 - Operaciones (Code)**:\n",
        "   - [ ] A√±ade celda de c√≥digo\n",
        "   - [ ] Escribe:\n",
        "   \n",
        "   ```python\n",
        "   # Variables y operaciones\n",
        "   nombre = \"Databricks\"\n",
        "   a√±o = 2025\n",
        "   version = 4.5\n",
        "   \n",
        "   print(f\"Estoy aprendiendo {nombre}\")\n",
        "   print(f\"A√±o: {a√±o}\")\n",
        "   print(f\"Versi√≥n del curso: {version}\")\n",
        "   \n",
        "   # Operaci√≥n matem√°tica\n",
        "   resultado = a√±o + version\n",
        "   print(f\"Suma curiosa: {resultado}\")\n",
        "   ```\n",
        "   \n",
        "   - [ ] Shift+Enter\n",
        "\n",
        "7. **Celda 5 - T√≠tulo secci√≥n (Markdown)**:\n",
        "   - [ ] A√±ade celda Markdown\n",
        "   - [ ] Escribe:\n",
        "   \n",
        "   ```markdown\n",
        "   ## 3. Mi Primer DataFrame\n",
        "   \n",
        "   Ahora crearemos un DataFrame con datos de ejemplo.\n",
        "   ```\n",
        "   \n",
        "   - [ ] Shift+Enter\n",
        "\n",
        "8. **Celda 6 - Crear DataFrame (Code)**:\n",
        "   - [ ] A√±ade celda de c√≥digo\n",
        "   - [ ] Escribe:\n",
        "   \n",
        "   ```python\n",
        "   # Crear DataFrame de ejemplo\n",
        "   datos = [\n",
        "       (\"Python\", 95, \"Programaci√≥n\"),\n",
        "       (\"SQL\", 88, \"Bases de Datos\"),\n",
        "       (\"Spark\", 92, \"Big Data\"),\n",
        "       (\"Machine Learning\", 85, \"IA\"),\n",
        "       (\"Databricks\", 90, \"Plataforma\")\n",
        "   ]\n",
        "   \n",
        "   # Crear DataFrame\n",
        "   df = spark.createDataFrame(\n",
        "       datos,\n",
        "       [\"tecnologia\", \"puntuacion\", \"categoria\"]\n",
        "   )\n",
        "   \n",
        "   # Mostrar\n",
        "   print(\"üìä Tecnolog√≠as que estoy aprendiendo:\")\n",
        "   df.show()\n",
        "   ```\n",
        "   \n",
        "   - [ ] Shift+Enter\n",
        "   - [ ] Verifica que veas una tabla\n",
        "\n",
        "9. **Celda 7 - An√°lisis (Code)**:\n",
        "   - [ ] A√±ade celda de c√≥digo\n",
        "   - [ ] Escribe:\n",
        "   \n",
        "   ```python\n",
        "   from pyspark.sql.functions import avg, max, min, count\n",
        "   \n",
        "   # An√°lisis del DataFrame\n",
        "   print(\"üìà Estad√≠sticas:\")\n",
        "   df.agg(\n",
        "       count(\"*\").alias(\"total_tecnologias\"),\n",
        "       avg(\"puntuacion\").alias(\"puntuacion_promedio\"),\n",
        "       max(\"puntuacion\").alias(\"puntuacion_maxima\"),\n",
        "       min(\"puntuacion\").alias(\"puntuacion_minima\")\n",
        "   ).show()\n",
        "   \n",
        "   # Filtrar tecnolog√≠as con alta puntuaci√≥n\n",
        "   print(\"\\nüåü Tecnolog√≠as con puntuaci√≥n >= 90:\")\n",
        "   df.filter(df.puntuacion >= 90).show()\n",
        "   ```\n",
        "   \n",
        "   - [ ] Shift+Enter\n",
        "\n",
        "10. **Celda 8 - Conclusi√≥n (Markdown)**:\n",
        "    - [ ] A√±ade celda Markdown\n",
        "    - [ ] Escribe:\n",
        "    \n",
        "    ```markdown\n",
        "    ## Conclusiones\n",
        "    \n",
        "    ‚úÖ He aprendido a:\n",
        "    - Crear un notebook\n",
        "    - Ejecutar c√≥digo Python\n",
        "    - Crear DataFrames en Spark\n",
        "    - Realizar an√°lisis b√°sico\n",
        "    \n",
        "    ### Pr√≥ximos pasos\n",
        "    - Aprender magic commands\n",
        "    - Trabajar con widgets\n",
        "    - Explorar m√°s funciones de Spark\n",
        "    ```\n",
        "    \n",
        "    - [ ] Shift+Enter\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 4: VERIFICACI√ìN (5 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "11. **Ejecutar todo de nuevo**:\n",
        "    - [ ] Men√∫ notebook ‚Üí Clear ‚Üí All Results\n",
        "    - [ ] Run All (bot√≥n arriba)\n",
        "    - [ ] Verifica que TODO se ejecute sin errores\n",
        "\n",
        "12. **Explorar opciones**:\n",
        "    - [ ] Click en ‚ãÆ del notebook (arriba derecha)\n",
        "    - [ ] Explora: Export, Clone, Revision History\n",
        "    - [ ] NO hagas cambios, solo explora\n",
        "\n",
        "13. **Guardar**:\n",
        "    - [ ] Ctrl+S (Cmd+S en Mac)\n",
        "    - [ ] O simplemente espera (autosave autom√°tico)\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 5: REFLEXI√ìN (2 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Responde en una nueva celda Markdown:\n",
        "\n",
        "```markdown\n",
        "## Mi Reflexi√≥n\n",
        "\n",
        "1. ¬øQu√© fue lo m√°s f√°cil?\n",
        "   _________________________________\n",
        "\n",
        "2. ¬øQu√© fue lo m√°s dif√≠cil?\n",
        "   _________________________________\n",
        "\n",
        "3. ¬øQu√© me sorprendi√≥?\n",
        "   _________________________________\n",
        "```\n",
        "\n",
        "**TIEMPO TOTAL**: ~25 minutos\n",
        "\n",
        "‚úÖ ¬°Has creado tu primer notebook completo en Databricks!\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 4.1 - Desaf√≠o**\n",
        "\n",
        "```markdown\n",
        "### Desaf√≠o: An√°lisis de Ventas Simple\n",
        "\n",
        "**Objetivo**: Crear un notebook que analice datos de ventas ficticias\n",
        "\n",
        "**Instrucciones**:\n",
        "\n",
        "1. Crea nuevo notebook: \"Desafio_4_1_Analisis_Ventas\"\n",
        "\n",
        "2. Estructura (m√≠nimo 8 celdas):\n",
        "   - T√≠tulo y descripci√≥n (Markdown)\n",
        "   - Crear datos de ventas (Code)\n",
        "   - An√°lisis 1: Ventas totales (Code)\n",
        "   - An√°lisis 2: Ventas por producto (Code)\n",
        "   - An√°lisis 3: Top 3 productos (Code)\n",
        "   - Visualizaci√≥n simple (Code)\n",
        "   - Conclusiones (Markdown)\n",
        "\n",
        "3. Datos de ejemplo:\n",
        "   ```python\n",
        "   ventas = [\n",
        "       (\"Laptop\", 1200, 5),\n",
        "       (\"Mouse\", 25, 50),\n",
        "       (\"Teclado\", 75, 30),\n",
        "       (\"Monitor\", 300, 15),\n",
        "       (\"Webcam\", 80, 25),\n",
        "       (\"Auriculares\", 50, 40)\n",
        "   ]\n",
        "   # Columnas: producto, precio, cantidad\n",
        "   ```\n",
        "\n",
        "4. Incluye:\n",
        "   - C√°lculo de ingresos (precio * cantidad)\n",
        "   - Producto m√°s vendido\n",
        "   - Ingresos totales\n",
        "   - Producto con mayores ingresos\n",
        "\n",
        "5. Bonus:\n",
        "   - Ordena resultados\n",
        "   - A√±ade formato a los n√∫meros\n",
        "   - Crea un gr√°fico simple (display(df))\n",
        "\n",
        "**Tiempo estimado**: 20-30 minutos\n",
        "\n",
        "**Resultado esperado**:\n",
        "Notebook funcionando con an√°lisis completo y bien documentado\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **Notebooks** = Documentos interactivos con c√≥digo y texto\n",
        "2. **Necesitas cluster** conectado para ejecutar c√≥digo\n",
        "3. **Shift+Enter** = Ejecutar celda y avanzar\n",
        "4. **Dos tipos de celdas**: Code y Markdown\n",
        "5. **Las celdas se ejecutan** en el orden que t√∫ eliges\n",
        "6. **Autosave** est√° activado (pero Ctrl+S es buena pr√°ctica)\n",
        "7. **Run All** ejecuta todas las celdas en orden\n",
        "8. **Clear Results** limpia visuales, no variables\n",
        "9. **Estado de celda** muestra si fue ejecutada y cu√°ndo\n",
        "10. **Documenta** siempre con Markdown entre c√≥digo\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **Documenta mientras codificas**\n",
        "```markdown\n",
        "No dejes la documentaci√≥n para el final\n",
        "A√±ade celdas Markdown explicando QU√â y POR QU√â\n",
        "Tu yo futuro te lo agradecer√°\n",
        "```\n",
        "\n",
        "‚úÖ **Nombra notebooks descriptivamente**\n",
        "```markdown\n",
        "‚ùå \"Notebook1\", \"test\", \"untitled\"\n",
        "‚úÖ \"Analisis_Ventas_Q1_2025\", \"ETL_Clientes\"\n",
        "```\n",
        "\n",
        "‚úÖ **Usa \"Run All\" antes de compartir**\n",
        "```markdown\n",
        "Asegura que todo funciona en orden\n",
        "Detecta dependencias rotas\n",
        "```\n",
        "\n",
        "‚úÖ **Divide notebooks largos**\n",
        "```markdown\n",
        "> 50 celdas ‚Üí Considera dividir\n",
        "Un notebook = Un prop√≥sito/an√°lisis\n",
        "```\n",
        "\n",
        "‚úÖ **Aprovecha autocompletado**\n",
        "```markdown\n",
        "Tab para autocompletar\n",
        "Shift+Tab para ver documentaci√≥n\n",
        "```\n",
        "\n",
        "‚úÖ **Limpia resultados innecesarios**\n",
        "```markdown\n",
        "Resultados grandes (tablas de 1M filas)\n",
        "Hacen el notebook lento\n",
        "Clear result en celdas con mucha salida\n",
        "```\n",
        "\n",
        "‚úÖ **Guarda versiones importantes**\n",
        "```markdown\n",
        "Men√∫ ‚Üí Clone antes de cambios grandes\n",
        "O usa Git (Repos) para versionado profesional\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Has completado la introducci√≥n a notebooks! üéâ**\n",
        "\n",
        "Ahora sabes:\n",
        "- Crear notebooks ‚úÖ\n",
        "- Conectar a clusters ‚úÖ\n",
        "- Trabajar con celdas ‚úÖ\n",
        "- Ejecutar c√≥digo ‚úÖ\n",
        "- Documentar con Markdown ‚úÖ\n",
        "- Estados de ejecuci√≥n ‚úÖ\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el siguiente punto **4.2 Lenguajes disponibles y cambio entre ellos**? üöÄ"
      ],
      "metadata": {
        "id": "GUeeHvAxiaQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.2 Lenguajes disponibles y cambio entre ellos**\n",
        "\n",
        "#### **Introducci√≥n: Databricks Multilenguaje**\n",
        "\n",
        "Una de las caracter√≠sticas m√°s poderosas de Databricks es su soporte para m√∫ltiples lenguajes en un mismo notebook. Puedes escribir Python en una celda, SQL en la siguiente, y Scala en otra, todo trabajando con los mismos datos.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "LENGUAJES EN DATABRICKS\n",
        "\"\"\"\n",
        "\n",
        "lenguajes_info = {\n",
        "    'soportados': ['Python', 'SQL', 'Scala', 'R'],\n",
        "    'lenguaje_default': 'El que elijas al crear el notebook',\n",
        "    'cambio_de_lenguaje': 'Magic commands en cada celda',\n",
        "    'ventaja': 'Usa el mejor lenguaje para cada tarea',\n",
        "    'interoperabilidad': 'Variables compartidas entre lenguajes'\n",
        "}\n",
        "\n",
        "print(\"üó£Ô∏è LENGUAJES EN DATABRICKS\")\n",
        "print(\"=\"*60)\n",
        "for key, value in lenguajes_info.items():\n",
        "    if isinstance(value, list):\n",
        "        print(f\"\\n{key.replace('_', ' ').title()}:\")\n",
        "        for item in value:\n",
        "            print(f\"  ‚Ä¢ {item}\")\n",
        "    else:\n",
        "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 1: Los Cuatro Lenguajes**\n",
        "\n",
        "#### **1. Python (PySpark)**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "PYTHON / PYSPARK\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüêç PYTHON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "python_info = \"\"\"\n",
        "EL M√ÅS POPULAR (70% de usuarios lo usan)\n",
        "\n",
        "VENTAJAS:\n",
        "‚úÖ F√°cil de aprender\n",
        "‚úÖ Sintaxis intuitiva y legible\n",
        "‚úÖ Ecosistema enorme de librer√≠as\n",
        "‚úÖ Ideal para Data Science y ML\n",
        "‚úÖ Pandas-like API (familiar)\n",
        "‚úÖ Gran comunidad y recursos\n",
        "\n",
        "IDEAL PARA:\n",
        "‚Ä¢ Data Science\n",
        "‚Ä¢ Machine Learning\n",
        "‚Ä¢ An√°lisis exploratorio\n",
        "‚Ä¢ ETL/Transformaciones\n",
        "‚Ä¢ Automatizaci√≥n\n",
        "‚Ä¢ Scripts generales\n",
        "\n",
        "LIBRER√çAS DISPONIBLES:\n",
        "‚Ä¢ PySpark (Spark nativo)\n",
        "‚Ä¢ Pandas (datasets peque√±os)\n",
        "‚Ä¢ NumPy, SciPy (matem√°ticas)\n",
        "‚Ä¢ Matplotlib, Seaborn (visualizaci√≥n)\n",
        "‚Ä¢ Scikit-learn (ML)\n",
        "‚Ä¢ TensorFlow, PyTorch (Deep Learning - en ML Runtime)\n",
        "\n",
        "EJEMPLO:\n",
        "\"\"\"\n",
        "\n",
        "print(python_info)\n",
        "\n",
        "ejemplo_python = \"\"\"\n",
        "# Crear DataFrame\n",
        "from pyspark.sql.functions import col, avg\n",
        "\n",
        "datos = [\n",
        "    (\"Alice\", 85, \"Math\"),\n",
        "    (\"Bob\", 92, \"Math\"),\n",
        "    (\"Charlie\", 78, \"Science\")\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(datos, [\"nombre\", \"nota\", \"materia\"])\n",
        "\n",
        "# Operaciones con API de Python\n",
        "df_filtrado = df.filter(col(\"nota\") > 80)\n",
        "promedio = df.agg(avg(\"nota\")).collect()[0][0]\n",
        "\n",
        "print(f\"Promedio: {promedio:.2f}\")\n",
        "df_filtrado.show()\n",
        "\n",
        "# Output:\n",
        "# Promedio: 85.00\n",
        "# +-----+----+-------+\n",
        "# |nombre|nota|materia|\n",
        "# +-----+----+-------+\n",
        "# |Alice|  85|   Math|\n",
        "# |  Bob|  92|   Math|\n",
        "# +-----+----+-------+\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_python)\n",
        "```\n",
        "\n",
        "#### **2. SQL**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "SQL\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìä SQL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sql_info = \"\"\"\n",
        "EL M√ÅS INTUITIVO PARA AN√ÅLISIS\n",
        "\n",
        "VENTAJAS:\n",
        "‚úÖ Familiar para analistas\n",
        "‚úÖ Sintaxis declarativa (qu√©, no c√≥mo)\n",
        "‚úÖ Optimizado autom√°ticamente por Catalyst\n",
        "‚úÖ Perfecto para queries y reportes\n",
        "‚úÖ F√°cil de leer y mantener\n",
        "‚úÖ No requiere programaci√≥n\n",
        "\n",
        "IDEAL PARA:\n",
        "‚Ä¢ An√°lisis de datos\n",
        "‚Ä¢ Reportes y dashboards\n",
        "‚Ä¢ Queries ad-hoc\n",
        "‚Ä¢ ETL simple\n",
        "‚Ä¢ Business Intelligence\n",
        "‚Ä¢ Cualquier cosa con SELECT/WHERE/JOIN\n",
        "\n",
        "CARACTER√çSTICAS ESPECIALES:\n",
        "‚Ä¢ Soporte SQL ANSI completo\n",
        "‚Ä¢ Funciones de ventana (window functions)\n",
        "‚Ä¢ CTEs (Common Table Expressions)\n",
        "‚Ä¢ Funciones agregadas avanzadas\n",
        "‚Ä¢ Delta Lake operations (MERGE, UPDATE, DELETE)\n",
        "\n",
        "EJEMPLO:\n",
        "\"\"\"\n",
        "\n",
        "print(sql_info)\n",
        "\n",
        "ejemplo_sql = \"\"\"\n",
        "-- En una celda con %sql\n",
        "\n",
        "-- Crear tabla temporal\n",
        "CREATE OR REPLACE TEMP VIEW estudiantes AS\n",
        "SELECT * FROM VALUES\n",
        "  (\"Alice\", 85, \"Math\"),\n",
        "  (\"Bob\", 92, \"Math\"),\n",
        "  (\"Charlie\", 78, \"Science\"),\n",
        "  (\"Diana\", 88, \"Math\")\n",
        "AS estudiantes(nombre, nota, materia);\n",
        "\n",
        "-- Query con agregaciones\n",
        "SELECT\n",
        "  materia,\n",
        "  COUNT(*) as num_estudiantes,\n",
        "  ROUND(AVG(nota), 2) as promedio,\n",
        "  MAX(nota) as nota_maxima\n",
        "FROM estudiantes\n",
        "GROUP BY materia\n",
        "ORDER BY promedio DESC;\n",
        "\n",
        "-- Output:\n",
        "-- +-------+----------------+---------+------------+\n",
        "-- |materia|num_estudiantes |promedio |nota_maxima |\n",
        "-- +-------+----------------+---------+------------+\n",
        "-- |Math   |              3 |    88.33|          92|\n",
        "-- |Science|              1 |    78.00|          78|\n",
        "-- +-------+----------------+---------+------------+\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_sql)\n",
        "```\n",
        "\n",
        "#### **3. Scala**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "SCALA\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n‚ö° SCALA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "scala_info = \"\"\"\n",
        "EL LENGUAJE NATIVO DE SPARK\n",
        "\n",
        "VENTAJAS:\n",
        "‚úÖ Performance m√°ximo (JVM nativo)\n",
        "‚úÖ Type-safe (errores en compilaci√≥n)\n",
        "‚úÖ Acceso a todas las features de Spark\n",
        "‚úÖ Menos overhead que Python\n",
        "‚úÖ Ideal para librer√≠as y frameworks\n",
        "\n",
        "DESVENTAJAS:\n",
        "‚ùå Curva de aprendizaje m√°s alta\n",
        "‚ùå Sintaxis m√°s compleja\n",
        "‚ùå Menos librer√≠as de Data Science\n",
        "‚ùå Comunidad m√°s peque√±a que Python\n",
        "\n",
        "IDEAL PARA:\n",
        "‚Ä¢ Producci√≥n de alto rendimiento\n",
        "‚Ä¢ Librer√≠as y frameworks\n",
        "‚Ä¢ Cuando performance es cr√≠tico\n",
        "‚Ä¢ Sistemas distribuidos complejos\n",
        "‚Ä¢ Desarrollo de Spark mismo\n",
        "\n",
        "CU√ÅNDO USAR:\n",
        "‚Ä¢ Eres desarrollador Scala\n",
        "‚Ä¢ Performance cr√≠tico (microsegundos importan)\n",
        "‚Ä¢ Necesitas features avanzadas de Spark\n",
        "‚Ä¢ Trabajas en infraestructura\n",
        "\n",
        "CU√ÅNDO NO USAR:\n",
        "‚Ä¢ Eres principiante\n",
        "‚Ä¢ Data Science/ML (usa Python)\n",
        "‚Ä¢ An√°lisis r√°pido (usa SQL)\n",
        "‚Ä¢ Prototipado (usa Python)\n",
        "\n",
        "EJEMPLO:\n",
        "\"\"\"\n",
        "\n",
        "print(scala_info)\n",
        "\n",
        "ejemplo_scala = \"\"\"\n",
        "// En una celda con %scala\n",
        "\n",
        "// Crear DataFrame\n",
        "val datos = Seq(\n",
        "  (\"Alice\", 85, \"Math\"),\n",
        "  (\"Bob\", 92, \"Math\"),\n",
        "  (\"Charlie\", 78, \"Science\")\n",
        ")\n",
        "\n",
        "val df = datos.toDF(\"nombre\", \"nota\", \"materia\")\n",
        "\n",
        "// Operaciones type-safe\n",
        "val dfFiltrado = df.filter($\"nota\" > 80)\n",
        "val promedio = df.agg(avg(\"nota\")).first().getDouble(0)\n",
        "\n",
        "println(f\"Promedio: $promedio%.2f\")\n",
        "dfFiltrado.show()\n",
        "\n",
        "// Output similar a Python\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_scala)\n",
        "```\n",
        "\n",
        "#### **4. R**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "R\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìà R\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "r_info = \"\"\"\n",
        "EL LENGUAJE DE ESTAD√çSTICA\n",
        "\n",
        "VENTAJAS:\n",
        "‚úÖ Potente para estad√≠stica\n",
        "‚úÖ Visualizaciones avanzadas (ggplot2)\n",
        "‚úÖ Comunidad acad√©mica fuerte\n",
        "‚úÖ Paquetes especializados en estad√≠stica\n",
        "‚úÖ Notebooks estilo RMarkdown\n",
        "\n",
        "DESVENTAJAS:\n",
        "‚ùå Menos usado que Python en industria\n",
        "‚ùå Performance inferior\n",
        "‚ùå Menos integraci√≥n con Spark\n",
        "‚ùå Sintaxis puede ser confusa\n",
        "\n",
        "IDEAL PARA:\n",
        "‚Ä¢ An√°lisis estad√≠stico avanzado\n",
        "‚Ä¢ Investigaci√≥n acad√©mica\n",
        "‚Ä¢ Visualizaciones complejas\n",
        "‚Ä¢ Si ya sabes R\n",
        "‚Ä¢ Migraci√≥n de c√≥digo R existente\n",
        "\n",
        "CU√ÅNDO USAR:\n",
        "‚Ä¢ Background en estad√≠stica/academia\n",
        "‚Ä¢ An√°lisis estad√≠stico espec√≠fico\n",
        "‚Ä¢ Necesitas paquetes R espec√≠ficos\n",
        "‚Ä¢ Equipo usa R\n",
        "\n",
        "CU√ÅNDO NO USAR:\n",
        "‚Ä¢ Eres principiante (aprende Python)\n",
        "‚Ä¢ Necesitas performance\n",
        "‚Ä¢ Proyecto de ingenier√≠a\n",
        "\n",
        "DISPONIBILIDAD:\n",
        "‚ö†Ô∏è En Free Edition: Soporte limitado\n",
        "‚úÖ En versiones premium: Soporte completo\n",
        "\n",
        "EJEMPLO:\n",
        "\"\"\"\n",
        "\n",
        "print(r_info)\n",
        "\n",
        "ejemplo_r = \"\"\"\n",
        "# En una celda con %r\n",
        "\n",
        "# Crear DataFrame (usa SparkR)\n",
        "library(SparkR)\n",
        "\n",
        "datos <- data.frame(\n",
        "  nombre = c(\"Alice\", \"Bob\", \"Charlie\"),\n",
        "  nota = c(85, 92, 78),\n",
        "  materia = c(\"Math\", \"Math\", \"Science\")\n",
        ")\n",
        "\n",
        "df <- createDataFrame(datos)\n",
        "\n",
        "# Operaciones\n",
        "df_filtrado <- filter(df, df$nota > 80)\n",
        "promedio <- mean(df$nota)\n",
        "\n",
        "print(paste(\"Promedio:\", round(promedio, 2)))\n",
        "showDF(df_filtrado)\n",
        "\n",
        "# Visualizaci√≥n con ggplot2\n",
        "library(ggplot2)\n",
        "ggplot(datos, aes(x=nombre, y=nota)) +\n",
        "  geom_bar(stat=\"identity\")\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_r)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 2: Lenguaje Default del Notebook**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "LENGUAJE DEFAULT\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüéØ LENGUAJE DEFAULT DEL NOTEBOOK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "default_info = \"\"\"\n",
        "¬øQU√â ES EL LENGUAJE DEFAULT?\n",
        "\n",
        "El lenguaje que se usa por defecto en todas las celdas nuevas\n",
        "hasta que especifiques otro con magic commands.\n",
        "\n",
        "SE DEFINE AL CREAR EL NOTEBOOK:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Create Notebook                                ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Name: Mi_Notebook                              ‚îÇ\n",
        "‚îÇ                                                ‚îÇ\n",
        "‚îÇ Default Language: [Python    ‚ñº]                ‚îÇ\n",
        "‚îÇ                   ‚Ä¢ Python                     ‚îÇ\n",
        "‚îÇ                   ‚Ä¢ SQL                        ‚îÇ\n",
        "‚îÇ                   ‚Ä¢ Scala                      ‚îÇ\n",
        "‚îÇ                   ‚Ä¢ R                          ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "SE MUESTRA EN LA BARRA SUPERIOR:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Mi_Notebook | üêç Python | üü¢ cluster | ...     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                    ‚ñ≤\n",
        "              Lenguaje default\n",
        "\n",
        "CAMBIAR EL DEFAULT (despu√©s de crear):\n",
        "\n",
        "1. Click en el √≠cono del lenguaje (üêç Python)\n",
        "\n",
        "2. Aparece men√∫:\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ ‚óã Python     ‚îÇ\n",
        "   ‚îÇ ‚óã SQL        ‚îÇ\n",
        "   ‚îÇ ‚óã Scala      ‚îÇ\n",
        "   ‚îÇ ‚óã R          ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "3. Selecciona nuevo lenguaje\n",
        "\n",
        "4. ‚ö†Ô∏è Esto solo afecta celdas NUEVAS\n",
        "   Las celdas existentes mantienen su lenguaje\n",
        "\n",
        "IMPLICACIONES:\n",
        "\n",
        "‚Ä¢ Celdas sin magic command = Default language\n",
        "‚Ä¢ Si default es Python:\n",
        "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "  ‚îÇ print(\"Hola\")  ‚îÇ  ‚Üí Python\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚Ä¢ Si default es SQL:\n",
        "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "  ‚îÇ SELECT * FROM tabla  ‚îÇ  ‚Üí SQL\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "RECOMENDACI√ìN:\n",
        "\n",
        "üéØ Elige seg√∫n el prop√≥sito principal del notebook:\n",
        "\n",
        "‚Ä¢ An√°lisis exploratorio ‚Üí Python\n",
        "‚Ä¢ Reportes y dashboards ‚Üí SQL\n",
        "‚Ä¢ Ingenier√≠a de alto rendimiento ‚Üí Scala\n",
        "‚Ä¢ An√°lisis estad√≠stico ‚Üí R\n",
        "\n",
        "Luego usa magic commands para celdas espec√≠ficas en otros lenguajes\n",
        "\"\"\"\n",
        "\n",
        "print(default_info)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 3: Magic Commands - Cambiar Lenguaje por Celda**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "MAGIC COMMANDS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n‚ú® MAGIC COMMANDS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "magic_info = \"\"\"\n",
        "¬øQU√â SON MAGIC COMMANDS?\n",
        "\n",
        "Comandos especiales que empiezan con % y controlan el\n",
        "comportamiento de la celda.\n",
        "\n",
        "PARA CAMBIAR LENGUAJE:\n",
        "\n",
        "%python  ‚Üí  Ejecuta celda en Python\n",
        "%sql     ‚Üí  Ejecuta celda en SQL\n",
        "%scala   ‚Üí  Ejecuta celda en Scala\n",
        "%r       ‚Üí  Ejecuta celda en R\n",
        "\n",
        "SINTAXIS:\n",
        "\n",
        "1. DEBE ser la PRIMERA L√çNEA de la celda\n",
        "2. Sin espacios antes del %\n",
        "3. Min√∫sculas\n",
        "4. Una sola l√≠nea\n",
        "\n",
        "CORRECTOS:\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ %sql                        ‚îÇ\n",
        "‚îÇ SELECT * FROM tabla         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ %python                     ‚îÇ\n",
        "‚îÇ print(\"Hola\")               ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "INCORRECTOS:\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   %sql    ‚Üê espacios antes  ‚îÇ\n",
        "‚îÇ SELECT * FROM tabla         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ # Comentario                ‚îÇ\n",
        "‚îÇ %sql      ‚Üê no es primera   ‚îÇ\n",
        "‚îÇ SELECT * FROM tabla         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ %SQL      ‚Üê may√∫sculas      ‚îÇ\n",
        "‚îÇ SELECT * FROM tabla         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\"\"\"\n",
        "\n",
        "print(magic_info)\n",
        "```\n",
        "\n",
        "#### **Ejemplos Pr√°cticos de Magic Commands**\n",
        "\n",
        "```markdown\n",
        "üíª EJEMPLOS DE MAGIC COMMANDS\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "EJEMPLO 1: Notebook Python con SQL\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Notebook default: Python\n",
        "\n",
        "# Celda 1 (Python - default, sin magic command)\n",
        "# Crear datos\n",
        "datos = [\n",
        "    (\"Alice\", 85),\n",
        "    (\"Bob\", 92),\n",
        "    (\"Charlie\", 78)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(datos, [\"nombre\", \"nota\"])\n",
        "\n",
        "# Registrar como tabla temporal para SQL\n",
        "df.createOrReplaceTempView(\"estudiantes\")\n",
        "\n",
        "print(\"‚úÖ Datos creados y registrados\")\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 2 (SQL - usando %sql)\n",
        "%sql\n",
        "SELECT\n",
        "  nombre,\n",
        "  nota,\n",
        "  CASE\n",
        "    WHEN nota >= 90 THEN 'A'\n",
        "    WHEN nota >= 80 THEN 'B'\n",
        "    ELSE 'C'\n",
        "  END as calificacion\n",
        "FROM estudiantes\n",
        "ORDER BY nota DESC\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 3 (Python de nuevo - default)\n",
        "# An√°lisis adicional en Python\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "promedio = df.agg(avg(\"nota\")).collect()[0][0]\n",
        "print(f\"Promedio general: {promedio:.2f}\")\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "EJEMPLO 2: Notebook SQL con Python\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Notebook default: SQL\n",
        "\n",
        "-- Celda 1 (SQL - default)\n",
        "CREATE OR REPLACE TEMP VIEW ventas AS\n",
        "SELECT * FROM VALUES\n",
        "  (1, 'Laptop', 1200, 5),\n",
        "  (2, 'Mouse', 25, 50),\n",
        "  (3, 'Teclado', 75, 30)\n",
        "AS ventas(id, producto, precio, cantidad);\n",
        "\n",
        "SELECT * FROM ventas;\n",
        "\n",
        "---\n",
        "\n",
        "-- Celda 2 (SQL - default)\n",
        "SELECT\n",
        "  producto,\n",
        "  precio * cantidad as ingresos\n",
        "FROM ventas\n",
        "ORDER BY ingresos DESC;\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 3 (Python - usando %python)\n",
        "%python\n",
        "# Cargar tabla en DataFrame Python\n",
        "df = spark.table(\"ventas\")\n",
        "\n",
        "# An√°lisis con Pandas (dataset peque√±o)\n",
        "pdf = df.toPandas()\n",
        "print(f\"Total productos: {len(pdf)}\")\n",
        "print(f\"Ingresos totales: ${pdf['precio'].mul(pdf['cantidad']).sum():,}\")\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "EJEMPLO 3: Mezclar m√∫ltiples lenguajes\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Notebook default: Python\n",
        "\n",
        "# Celda 1 (Python)\n",
        "# Crear datos\n",
        "spark.sql(\"\"\"\n",
        "  CREATE OR REPLACE TEMP VIEW datos AS\n",
        "  SELECT * FROM VALUES\n",
        "    ('2025-01-01', 100),\n",
        "    ('2025-01-02', 150),\n",
        "    ('2025-01-03', 120)\n",
        "  AS datos(fecha, ventas)\n",
        "\"\"\")\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 2 (SQL)\n",
        "%sql\n",
        "-- An√°lisis SQL\n",
        "SELECT\n",
        "  fecha,\n",
        "  ventas,\n",
        "  SUM(ventas) OVER (ORDER BY fecha) as ventas_acumuladas\n",
        "FROM datos;\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 3 (Scala - para performance)\n",
        "%scala\n",
        "// Operaci√≥n de alto rendimiento\n",
        "val df = spark.table(\"datos\")\n",
        "val total = df.agg(sum(\"ventas\")).first().getLong(0)\n",
        "println(s\"Total ventas: $total\")\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 4 (R - para visualizaci√≥n estad√≠stica)\n",
        "%r\n",
        "library(ggplot2)\n",
        "library(SparkR)\n",
        "\n",
        "df <- sql(\"SELECT * FROM datos\")\n",
        "df_r <- collect(df)\n",
        "\n",
        "ggplot(df_r, aes(x=fecha, y=ventas)) +\n",
        "  geom_line() +\n",
        "  theme_minimal() +\n",
        "  labs(title=\"Ventas Diarias\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 4: Compartir Variables Entre Lenguajes**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "INTEROPERABILIDAD ENTRE LENGUAJES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîÑ COMPARTIR DATOS ENTRE LENGUAJES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "interop_info = \"\"\"\n",
        "PRINCIPIO CLAVE:\n",
        "Las variables se comparten a trav√©s de la SESI√ìN DE SPARK,\n",
        "no directamente entre lenguajes.\n",
        "\n",
        "MECANISMO: TABLAS TEMPORALES\n",
        "\n",
        "Python/Scala/R ‚Üí Registrar DataFrame como tabla temporal\n",
        "‚Üì\n",
        "Tabla temporal en Spark\n",
        "‚Üì\n",
        "SQL/Python/Scala/R ‚Üí Leer tabla temporal\n",
        "\n",
        "M√âTODO 1: createOrReplaceTempView()\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Python:\n",
        "\"\"\"\n",
        "\n",
        "print(interop_info)\n",
        "\n",
        "ejemplo_interop1 = \"\"\"\n",
        "# Celda Python\n",
        "# Crear DataFrame\n",
        "df = spark.createDataFrame([\n",
        "    (1, \"Alice\"),\n",
        "    (2, \"Bob\")\n",
        "], [\"id\", \"nombre\"])\n",
        "\n",
        "# Registrar como tabla temporal\n",
        "df.createOrReplaceTempView(\"personas\")\n",
        "\n",
        "print(\"‚úÖ Tabla 'personas' disponible para SQL\")\n",
        "\n",
        "---\n",
        "\n",
        "# Celda SQL\n",
        "%sql\n",
        "-- Ahora SQL puede usar la tabla\n",
        "SELECT * FROM personas WHERE id = 1;\n",
        "\n",
        "---\n",
        "\n",
        "# Celda Scala\n",
        "%scala\n",
        "// Scala tambi√©n puede acceder\n",
        "val df = spark.table(\"personas\")\n",
        "df.show()\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_interop1)\n",
        "\n",
        "print(\"\\nM√âTODO 2: spark.table()\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "ejemplo_interop2 = \"\"\"\n",
        "-- Celda SQL\n",
        "%sql\n",
        "CREATE OR REPLACE TEMP VIEW ventas AS\n",
        "SELECT * FROM VALUES\n",
        "  (1, 'Producto A', 100),\n",
        "  (2, 'Producto B', 150)\n",
        "AS ventas(id, nombre, precio);\n",
        "\n",
        "---\n",
        "\n",
        "# Celda Python\n",
        "# Leer la tabla creada en SQL\n",
        "df = spark.table(\"ventas\")\n",
        "\n",
        "# Trabajar con el DataFrame\n",
        "df_filtrado = df.filter(df.precio > 100)\n",
        "df_filtrado.show()\n",
        "\n",
        "---\n",
        "\n",
        "# Celda Scala\n",
        "%scala\n",
        "// Scala tambi√©n puede leer\n",
        "val ventas = spark.table(\"ventas\")\n",
        "ventas.createOrReplaceTempView(\"ventas_scala\")\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_interop2)\n",
        "\n",
        "print(\"\\nM√âTODO 3: Variables temporales (solo para valores simples)\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "ejemplo_interop3 = \"\"\"\n",
        "‚ö†Ô∏è LIMITADO - Solo para configuraci√≥n, no DataFrames\n",
        "\n",
        "# Python\n",
        "spark.conf.set(\"mi.variable\", \"valor\")\n",
        "\n",
        "---\n",
        "\n",
        "%sql\n",
        "-- SQL puede leer\n",
        "SELECT '${mi.variable}' as mi_valor;\n",
        "\n",
        "‚ö†Ô∏è Esto NO funciona para DataFrames completos\n",
        "Solo para strings, n√∫meros, configuraciones\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_interop3)\n",
        "\n",
        "print(\"\\nQU√â NO SE COMPARTE DIRECTAMENTE:\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "no_compartido = \"\"\"\n",
        "‚ùå Variables Python locales\n",
        "# Celda Python\n",
        "x = 10  # Variable Python local\n",
        "\n",
        "---\n",
        "\n",
        "%sql\n",
        "-- Esto NO funciona\n",
        "SELECT ${x}  -- ‚ùå Error: x no existe en SQL\n",
        "\n",
        "‚úÖ SOLUCI√ìN: Usar tablas temporales o spark.conf.set()\n",
        "\n",
        "‚ùå Funciones definidas localmente\n",
        "# Celda Python\n",
        "def mi_funcion(x):\n",
        "    return x * 2\n",
        "\n",
        "---\n",
        "\n",
        "%scala\n",
        "// No puedes llamar mi_funcion() desde Scala\n",
        "// ‚ùå No existe en contexto Scala\n",
        "\n",
        "‚ùå Imports de librer√≠as\n",
        "# Celda Python\n",
        "import pandas as pd\n",
        "\n",
        "---\n",
        "\n",
        "%scala\n",
        "// pandas no est√° disponible en Scala\n",
        "// Debes importar equivalentes de Scala\n",
        "\"\"\"\n",
        "\n",
        "print(no_compartido)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 5: ¬øQu√© Lenguaje Elegir?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "GU√çA DE DECISI√ìN\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nü§î ¬øQU√â LENGUAJE USAR?\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "decision_guide = \"\"\"\n",
        "√ÅRBOL DE DECISI√ìN:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ ¬øQu√© necesitas hacer?                   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                ‚îÇ\n",
        "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "    ‚îÇ                       ‚îÇ\n",
        "    ‚ñº                       ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ An√°lisis‚îÇ           ‚îÇIngenier√≠a‚îÇ\n",
        "‚îÇ y Query ‚îÇ           ‚îÇy Pipeline‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "     ‚îÇ                      ‚îÇ\n",
        "     ‚ñº                      ‚ñº\n",
        "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "  ‚îÇ SQL  ‚îÇ              ‚îÇ¬øPerf.  ‚îÇ\n",
        "  ‚îÇ ‚úÖ   ‚îÇ              ‚îÇcr√≠tico?‚îÇ\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                            ‚îÇ\n",
        "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                    ‚îÇ                ‚îÇ\n",
        "                    ‚ñº                ‚ñº\n",
        "                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                 ‚îÇNO  ‚îÇ          ‚îÇ S√ç   ‚îÇ\n",
        "                 ‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò\n",
        "                   ‚îÇ                 ‚îÇ\n",
        "                   ‚ñº                 ‚ñº\n",
        "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "              ‚îÇPython  ‚îÇ        ‚îÇScala ‚îÇ\n",
        "              ‚îÇ   ‚úÖ   ‚îÇ        ‚îÇ  ‚úÖ  ‚îÇ\n",
        "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "CASOS DE USO POR LENGUAJE:\n",
        "\n",
        "üêç USA PYTHON CUANDO:\n",
        "‚îú‚îÄ Data Science / ML\n",
        "‚îú‚îÄ An√°lisis exploratorio\n",
        "‚îú‚îÄ ETL general\n",
        "‚îú‚îÄ Automatizaci√≥n\n",
        "‚îú‚îÄ Eres principiante\n",
        "‚îú‚îÄ Equipo usa Python\n",
        "‚îî‚îÄ Necesitas librer√≠as cient√≠ficas\n",
        "\n",
        "üìä USA SQL CUANDO:\n",
        "‚îú‚îÄ Reportes y dashboards\n",
        "‚îú‚îÄ Queries ad-hoc\n",
        "‚îú‚îÄ An√°lisis de datos simple\n",
        "‚îú‚îÄ Joins y agregaciones\n",
        "‚îú‚îÄ Business intelligence\n",
        "‚îú‚îÄ Equipo son analistas\n",
        "‚îî‚îÄ Ya tienes SQL escrito\n",
        "\n",
        "‚ö° USA SCALA CUANDO:\n",
        "‚îú‚îÄ Performance cr√≠tico\n",
        "‚îú‚îÄ Sistemas en producci√≥n de alto tr√°fico\n",
        "‚îú‚îÄ Desarrollo de librer√≠as Spark\n",
        "‚îú‚îÄ Equipos Java/Scala\n",
        "‚îú‚îÄ Necesitas type-safety\n",
        "‚îî‚îÄ Features avanzadas de Spark\n",
        "\n",
        "üìà USA R CUANDO:\n",
        "‚îú‚îÄ An√°lisis estad√≠stico avanzado\n",
        "‚îú‚îÄ Investigaci√≥n acad√©mica\n",
        "‚îú‚îÄ Necesitas paquetes R espec√≠ficos\n",
        "‚îú‚îÄ Equipo usa R\n",
        "‚îî‚îÄ Migraci√≥n de c√≥digo R existente\n",
        "\n",
        "COMBINACIONES COMUNES:\n",
        "\n",
        "üî• POPULAR: Python + SQL\n",
        "‚îú‚îÄ Python: ETL y transformaciones\n",
        "‚îî‚îÄ SQL: An√°lisis y reportes\n",
        "\n",
        "‚öôÔ∏è INGENIER√çA: Scala + SQL\n",
        "‚îú‚îÄ Scala: Pipeline de producci√≥n\n",
        "‚îî‚îÄ SQL: Verificaci√≥n y an√°lisis\n",
        "\n",
        "üî¨ CIENCIA: Python + R\n",
        "‚îú‚îÄ Python: Pipeline de datos\n",
        "‚îî‚îÄ R: An√°lisis estad√≠stico especializado\n",
        "\n",
        "RECOMENDACI√ìN PARA PRINCIPIANTES:\n",
        "\n",
        "1Ô∏è‚É£ Empieza con PYTHON\n",
        "   ‚Ä¢ M√°s f√°cil de aprender\n",
        "   ‚Ä¢ Mayor comunidad\n",
        "   ‚Ä¢ M√°s recursos\n",
        "\n",
        "2Ô∏è‚É£ A√±ade SQL gradualmente\n",
        "   ‚Ä¢ Para queries simples\n",
        "   ‚Ä¢ An√°lisis r√°pidos\n",
        "   ‚Ä¢ Reportes\n",
        "\n",
        "3Ô∏è‚É£ Scala/R solo si necesario\n",
        "   ‚Ä¢ Scala: Performance cr√≠tico\n",
        "   ‚Ä¢ R: Estad√≠stica avanzada\n",
        "\"\"\"\n",
        "\n",
        "print(decision_guide)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Notebook Multilenguaje**\n",
        "\n",
        "```markdown\n",
        "üß™ EJERCICIO: NOTEBOOK CON M√öLTIPLES LENGUAJES\n",
        "\n",
        "Vamos a crear un notebook que usa Python y SQL para analizar datos.\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "SETUP (2 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "1. **Crear notebook**:\n",
        "   - [ ] Name: \"Practica_4_2_Multilenguaje\"\n",
        "   - [ ] Default Language: Python\n",
        "   - [ ] Connect cluster\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 1: PREPARAR DATOS EN PYTHON (5 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "2. **Celda 1 - T√≠tulo (Markdown)**:\n",
        "```markdown\n",
        "# An√°lisis Multilenguaje: Ventas de Tienda\n",
        "\n",
        "**Objetivo**: Demostrar interoperabilidad entre Python y SQL\n",
        "\n",
        "## Dataset\n",
        "Ventas ficticias de una tienda online\n",
        "```\n",
        "\n",
        "3. **Celda 2 - Crear datos (Python)**:\n",
        "```python\n",
        "# Crear dataset de ventas\n",
        "from datetime import date\n",
        "\n",
        "ventas_data = [\n",
        "    (1, date(2025, 1, 1), \"Laptop\", 1200, 2, \"Madrid\"),\n",
        "    (2, date(2025, 1, 2), \"Mouse\", 25, 10, \"Barcelona\"),\n",
        "    (3, date(2025, 1, 2), \"Teclado\", 75, 5, \"Madrid\"),\n",
        "    (4, date(2025, 1, 3), \"Monitor\", 300, 3, \"Valencia\"),\n",
        "    (5, date(2025, 1, 3), \"Laptop\", 1200, 1, \"Barcelona\"),\n",
        "    (6, date(2025, 1, 4), \"Mouse\", 25, 15, \"Madrid\"),\n",
        "    (7, date(2025, 1, 4), \"Webcam\", 80, 8, \"Valencia\"),\n",
        "    (8, date(2025, 1, 5), \"Auriculares\", 50, 12, \"Madrid\")\n",
        "]\n",
        "\n",
        "# Crear DataFrame\n",
        "df_ventas = spark.createDataFrame(\n",
        "    ventas_data,\n",
        "    [\"id\", \"fecha\", \"producto\", \"precio\", \"cantidad\", \"ciudad\"]\n",
        ")\n",
        "\n",
        "# Registrar como tabla temporal para SQL\n",
        "df_ventas.createOrReplaceTempView(\"ventas\")\n",
        "\n",
        "print(\"‚úÖ Tabla 'ventas' creada y disponible\")\n",
        "print(f\"üìä Total registros: {df_ventas.count()}\")\n",
        "\n",
        "# Mostrar muestra\n",
        "df_ventas.show(5)\n",
        "```\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 2: AN√ÅLISIS EN SQL (10 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "4. **Celda 3 - T√≠tulo secci√≥n (Markdown)**:\n",
        "```markdown\n",
        "## An√°lisis SQL\n",
        "\n",
        "Usaremos SQL para queries anal√≠ticas.\n",
        "```\n",
        "\n",
        "5. **Celda 4 - Ingresos totales (SQL)**:\n",
        "```sql\n",
        "%sql\n",
        "-- Calcular ingresos por venta\n",
        "SELECT\n",
        "  id,\n",
        "  fecha,\n",
        "  producto,\n",
        "  precio,\n",
        "  cantidad,\n",
        "  precio * cantidad as ingresos,\n",
        "  ciudad\n",
        "FROM ventas\n",
        "ORDER BY ingresos DESC;\n",
        "```\n",
        "\n",
        "6. **Celda 5 - An√°lisis por producto (SQL)**:\n",
        "```sql\n",
        "%sql\n",
        "-- Ventas por producto\n",
        "SELECT\n",
        "  producto,\n",
        "  SUM(cantidad) as unidades_vendidas,\n",
        "  SUM(precio * cantidad) as ingresos_totales,\n",
        "  COUNT(*) as num_transacciones,\n",
        "  ROUND(AVG(precio * cantidad), 2) as ingreso_promedio_transaccion\n",
        "FROM ventas\n",
        "GROUP BY producto\n",
        "ORDER BY ingresos_totales DESC;\n",
        "```\n",
        "\n",
        "7. **Celda 6 - An√°lisis por ciudad (SQL)**:\n",
        "```sql\n",
        "%sql\n",
        "-- Ventas por ciudad\n",
        "SELECT\n",
        "  ciudad,\n",
        "  COUNT(*) as num_ventas,\n",
        "  SUM(precio * cantidad) as ingresos_totales,\n",
        "  ROUND(AVG(precio * cantidad), 2) as ticket_promedio\n",
        "FROM ventas\n",
        "GROUP BY ciudad\n",
        "ORDER BY ingresos_totales DESC;\n",
        "```\n",
        "\n",
        "8. **Celda 7 - Top 3 d√≠as (SQL)**:\n",
        "```sql\n",
        "%sql\n",
        "-- Top 3 d√≠as con mayores ingresos\n",
        "SELECT\n",
        "  fecha,\n",
        "  COUNT(*) as num_transacciones,\n",
        "  SUM(precio * cantidad) as ingresos_dia\n",
        "FROM ventas\n",
        "GROUP BY fecha\n",
        "ORDER BY ingresos_dia DESC\n",
        "LIMIT 3;\n",
        "```\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 3: VOLVER A PYTHON PARA AN√ÅLISIS AVANZADO (8 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "9. **Celda 8 - T√≠tulo secci√≥n (Markdown)**:\n",
        "```markdown\n",
        "## An√°lisis Avanzado en Python\n",
        "\n",
        "Retomamos Python para c√°lculos m√°s complejos.\n",
        "```\n",
        "\n",
        "10. **Celda 9 - Cargar resultados SQL (Python)**:\n",
        "```python\n",
        "# Cargar tabla de ventas (creada en SQL)\n",
        "df = spark.table(\"ventas\")\n",
        "\n",
        "# A√±adir columna de ingresos\n",
        "from pyspark.sql.functions import col, sum, avg, count\n",
        "\n",
        "df_con_ingresos = df.withColumn(\"ingresos\", col(\"precio\") * col(\"cantidad\"))\n",
        "\n",
        "# Estad√≠sticas generales\n",
        "stats = df_con_ingresos.agg(\n",
        "    count(\"*\").alias(\"total_ventas\"),\n",
        "    sum(\"ingresos\").alias(\"ingresos_totales\"),\n",
        "    avg(\"ingresos\").alias(\"ticket_promedio\"),\n",
        "    count(col(\"producto\").distinct()).alias(\"productos_unicos\")\n",
        ").collect()[0]\n",
        "\n",
        "print(\"üìä ESTAD√çSTICAS GENERALES\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total ventas: {stats['total_ventas']}\")\n",
        "print(f\"Ingresos totales: ${stats['ingresos_totales']:,.2f}\")\n",
        "print(f\"Ticket promedio: ${stats['ticket_promedio']:.2f}\")\n",
        "print(f\"Productos √∫nicos: {stats['productos_unicos']}\")\n",
        "```\n",
        "\n",
        "11. **Celda 10 - An√°lisis de concentraci√≥n (Python)**:\n",
        "```python\n",
        "# ¬øQu√© % de ingresos genera cada producto?\n",
        "from pyspark.sql.functions import round\n",
        "\n",
        "df_concentracion = df_con_ingresos.groupBy(\"producto\").agg(\n",
        "    sum(\"ingresos\").alias(\"ingresos\")\n",
        ")\n",
        "\n",
        "# Calcular porcentaje\n",
        "total_ingresos = df_con_ingresos.agg(sum(\"ingresos\")).collect()[0][0]\n",
        "\n",
        "df_concentracion = df_concentracion.withColumn(\n",
        "    \"porcentaje\",\n",
        "    round((col(\"ingresos\") / total_ingresos) * 100, 2)\n",
        ")\n",
        "\n",
        "print(\"üí∞ CONCENTRACI√ìN DE INGRESOS POR PRODUCTO\")\n",
        "df_concentracion.orderBy(col(\"porcentaje\").desc()).show()\n",
        "\n",
        "# Identificar top producto\n",
        "top_producto = df_concentracion.orderBy(col(\"porcentaje\").desc()).first()\n",
        "print(f\"\\nüèÜ Producto estrella: {top_producto['producto']}\")\n",
        "print(f\"   Genera: {top_producto['porcentaje']}% de ingresos totales\")\n",
        "```\n",
        "\n",
        "12. **Celda 11 - Crear vista para SQL (Python)**:\n",
        "```python\n",
        "# Guardar an√°lisis de concentraci√≥n para SQL\n",
        "df_concentracion.createOrReplaceTempView(\"concentracion_productos\")\n",
        "\n",
        "print(\"‚úÖ Tabla 'concentracion_productos' disponible para SQL\")\n",
        "```\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 4: VOLVER A SQL PARA VISUALIZACI√ìN (3 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "13. **Celda 12 - Gr√°fico en SQL**:\n",
        "```sql\n",
        "%sql\n",
        "-- Visualizar concentraci√≥n de productos\n",
        "SELECT\n",
        "  producto,\n",
        "  ingresos,\n",
        "  porcentaje\n",
        "FROM concentracion_productos\n",
        "ORDER BY porcentaje DESC;\n",
        "\n",
        "-- Databricks crear√° un gr√°fico de barras autom√°ticamente\n",
        "-- Click en el icono de gr√°fico debajo de la tabla\n",
        "```\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 5: CONCLUSIONES (2 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "14. **Celda 13 - Conclusiones (Markdown)**:\n",
        "```markdown\n",
        "## Conclusiones\n",
        "\n",
        "### Lo que aprendimos:\n",
        "‚úÖ Crear datos en Python\n",
        "‚úÖ Registrar como tabla temporal\n",
        "‚úÖ An√°lisis SQL sobre datos de Python\n",
        "‚úÖ Volver a Python para c√°lculos complejos\n",
        "‚úÖ Crear nuevas tablas desde Python\n",
        "‚úÖ Visualizar en SQL\n",
        "\n",
        "### Flujo de trabajo:\n",
        "1. Python: Preparaci√≥n de datos\n",
        "2. SQL: Queries anal√≠ticas\n",
        "3. Python: An√°lisis avanzado\n",
        "4. SQL: Visualizaci√≥n\n",
        "\n",
        "### Interoperabilidad:\n",
        "- Variables compartidas via tablas temporales\n",
        "- Cada lenguaje para lo que hace mejor\n",
        "- Flujo natural entre lenguajes\n",
        "```\n",
        "\n",
        "**TIEMPO TOTAL**: ~30 minutos\n",
        "\n",
        "‚úÖ Has creado un notebook multilenguaje completo!\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 4.2 - Desaf√≠o**\n",
        "\n",
        "```markdown\n",
        "### Desaf√≠o: Pipeline Multilenguaje\n",
        "\n",
        "**Objetivo**: Crear un an√°lisis completo usando Python, SQL y opcionalmente Scala\n",
        "\n",
        "**Escenario**:\n",
        "Eres analista en una empresa de delivery. Tienes datos de pedidos y necesitas:\n",
        "1. Calcular m√©tricas de rendimiento\n",
        "2. Identificar zonas problem√°ticas\n",
        "3. Recomendar acciones\n",
        "\n",
        "**Datos**:\n",
        "```python\n",
        "pedidos = [\n",
        "    (1, \"2025-01-01 10:30\", \"Zona A\", 25, 15, \"entregado\"),\n",
        "    (2, \"2025-01-01 11:00\", \"Zona B\", 30, 45, \"entregado\"),\n",
        "    (3, \"2025-01-01 11:30\", \"Zona A\", 20, 18, \"entregado\"),\n",
        "    (4, \"2025-01-01 12:00\", \"Zona C\", 35, 60, \"cancelado\"),\n",
        "    (5, \"2025-01-01 12:30\", \"Zona B\", 28, 40, \"entregado\"),\n",
        "    (6, \"2025-01-01 13:00\", \"Zona A\", 22, 20, \"entregado\"),\n",
        "    (7, \"2025-01-01 13:30\", \"Zona C\", 40, 70, \"entregado\"),\n",
        "    (8, \"2025-01-01 14:00\", \"Zona B\", 26, 35, \"entregado\")\n",
        "]\n",
        "# Columnas: id, timestamp, zona, tiempo_esperado_min, tiempo_real_min, estado\n",
        "```\n",
        "\n",
        "**Tareas**:\n",
        "\n",
        "1. **Python**: Crear DataFrame y tabla temporal\n",
        "2. **SQL**: Calcular:\n",
        "   - Tiempo promedio de entrega por zona\n",
        "   - Tasa de cancelaci√≥n por zona\n",
        "   - % pedidos entregados a tiempo\n",
        "3. **Python**: Identificar zonas problem√°ticas (>50% fuera de tiempo)\n",
        "4. **SQL**: Generar reporte final con recomendaciones\n",
        "5. **Markdown**: Documentar hallazgos y recomendaciones\n",
        "\n",
        "**Bonus**:\n",
        "- Usa %scala para calcular alguna m√©trica\n",
        "- Crea visualizaci√≥n (gr√°fico)\n",
        "- A√±ade an√°lisis de hora del d√≠a\n",
        "\n",
        "**Tiempo estimado**: 30-40 minutos\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **4 lenguajes**: Python, SQL, Scala, R\n",
        "2. **Python** es el m√°s popular y recomendado para principiantes\n",
        "3. **SQL** es perfecto para an√°lisis y queries\n",
        "4. **Magic commands** (%python, %sql, %scala, %r) cambian lenguaje por celda\n",
        "5. **Default language** del notebook afecta celdas sin magic command\n",
        "6. **Variables se comparten** v√≠a tablas temporales (createOrReplaceTempView)\n",
        "7. **spark.table()** carga tablas temporales en cualquier lenguaje\n",
        "8. **Cada lenguaje** tiene sus fortalezas - usa el adecuado para cada tarea\n",
        "9. **Magic command** debe ser primera l√≠nea de la celda\n",
        "10. **Interoperabilidad** es clave - combina lenguajes seg√∫n necesidad\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **Usa el lenguaje adecuado para cada tarea**\n",
        "```markdown\n",
        "Python: ETL complejo, ML\n",
        "SQL: Queries, reportes\n",
        "No fuerces un lenguaje donde otro es mejor\n",
        "```\n",
        "\n",
        "‚úÖ **Documenta qu√© lenguaje y por qu√©**\n",
        "```markdown\n",
        "%sql\n",
        "-- Usamos SQL porque es m√°s legible para joins complejos\n",
        "SELECT ...\n",
        "```\n",
        "\n",
        "‚úÖ **Mant√©n consistencia en notebooks**\n",
        "```markdown\n",
        "No alternes Python/SQL/Python/SQL sin raz√≥n\n",
        "Agrupa por lenguaje cuando sea posible\n",
        "```\n",
        "\n",
        "‚úÖ **Tablas temporales son baratas**\n",
        "```markdown\n",
        "Crea tantas como necesites\n",
        "Ayudan a la legibilidad\n",
        "df.createOrReplaceTempView(\"nombre_descriptivo\")\n",
        "```\n",
        "\n",
        "‚úÖ **SQL para stakeholders**\n",
        "```markdown\n",
        "Queries SQL son m√°s f√°ciles de entender para no-programadores\n",
        "Usa SQL en secciones que compartir√°s con negocio\n",
        "```\n",
        "\n",
        "‚úÖ **Python para automatizaci√≥n**\n",
        "```markdown\n",
        "Workflows complejos ‚Üí Python\n",
        "Condicionales y loops ‚Üí Python\n",
        "SQL es declarativo, no tiene l√≥gica de control\n",
        "```\n",
        "\n",
        "‚úÖ **Convenci√≥n de nombres para tablas**\n",
        "```markdown\n",
        "‚úÖ \"ventas_2025\", \"clientes_activos\"\n",
        "‚ùå \"temp\", \"tabla1\", \"df\"\n",
        "Nombres descriptivos ayudan en SQL\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Has dominado los lenguajes en Databricks! üéâ**\n",
        "\n",
        "Ahora sabes:\n",
        "- Los 4 lenguajes disponibles ‚úÖ\n",
        "- Cu√°ndo usar cada uno ‚úÖ\n",
        "- Magic commands ‚úÖ\n",
        "- Compartir datos entre lenguajes ‚úÖ\n",
        "- Crear notebooks multilenguaje ‚úÖ\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el siguiente punto **4.3 Magic commands (%python, %sql, %fs, %sh)** donde exploraremos magic commands adicionales m√°s all√° de los lenguajes? üöÄ"
      ],
      "metadata": {
        "id": "-LqHHFs9jgEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.3 Magic commands (%python, %sql, %fs, %sh)**\n",
        "\n",
        "#### **Introducci√≥n: M√°s All√° de los Lenguajes**\n",
        "\n",
        "Ya conoces los magic commands para cambiar de lenguaje (%python, %sql, %scala, %r). Pero Databricks ofrece **magic commands adicionales** que te dan superpoderes para interactuar con el sistema de archivos, ejecutar comandos shell, renderizar markdown y m√°s.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "MAGIC COMMANDS EN DATABRICKS\n",
        "\"\"\"\n",
        "\n",
        "magic_commands_intro = {\n",
        "    'definici√≥n': 'Comandos especiales que comienzan con %',\n",
        "    'categor√≠as': {\n",
        "        'Lenguajes': ['%python', '%sql', '%scala', '%r'],\n",
        "        'Sistema de archivos': ['%fs', '%sh'],\n",
        "        'Utilidades': ['%md', '%run', '%pip'],\n",
        "        'Otros': ['%lsmagic']\n",
        "    },\n",
        "    'caracter√≠sticas': [\n",
        "        'Deben ser la primera l√≠nea de la celda',\n",
        "        'Una sola l√≠nea (no multi-l√≠nea)',\n",
        "        'Min√∫sculas',\n",
        "        'Sin espacios antes del %'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"‚ú® MAGIC COMMANDS COMPLETOS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Definici√≥n: {magic_commands_intro['definici√≥n']}\")\n",
        "print(\"\\nCategor√≠as:\")\n",
        "for categoria, comandos in magic_commands_intro['categor√≠as'].items():\n",
        "    print(f\"  {categoria}:\")\n",
        "    for cmd in comandos:\n",
        "        print(f\"    ‚Ä¢ {cmd}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 1: %fs - File System Commands**\n",
        "\n",
        "#### **¬øQu√© es %fs?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "%fs - DATABRICKS FILE SYSTEM\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìÅ %fs - FILE SYSTEM MAGIC\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "fs_info = \"\"\"\n",
        "DEFINICI√ìN:\n",
        "%fs permite ejecutar comandos del sistema de archivos DBFS\n",
        "(Databricks File System) directamente desde notebooks.\n",
        "\n",
        "DBFS = Sistema de archivos distribuido de Databricks\n",
        "       Accesible desde todos los clusters\n",
        "       Persiste entre sesiones\n",
        "\n",
        "SINTAXIS:\n",
        "%fs <comando> <argumentos>\n",
        "\n",
        "COMANDOS DISPONIBLES:\n",
        "\"\"\"\n",
        "\n",
        "print(fs_info)\n",
        "```\n",
        "\n",
        "#### **Comandos %fs Principales**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "COMANDOS %fs\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìã COMANDOS %fs PRINCIPALES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comandos_fs = \"\"\"\n",
        "1Ô∏è‚É£ ls - LISTAR ARCHIVOS/DIRECTORIOS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Sintaxis:\n",
        "%fs ls <ruta>\n",
        "\n",
        "Ejemplos:\n",
        "%fs ls /                          # Ra√≠z de DBFS\n",
        "%fs ls /FileStore                 # Carpeta FileStore\n",
        "%fs ls /databricks-datasets       # Datasets de ejemplo\n",
        "%fs ls /user/hive/warehouse       # Tablas guardadas\n",
        "\n",
        "Salida:\n",
        "path                          name           size\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "dbfs:/FileStore/              FileStore/     0\n",
        "dbfs:/databricks-datasets/    databricks-... 0\n",
        "dbfs:/user/                   user/          0\n",
        "\n",
        "Equivalente Python:\n",
        "dbutils.fs.ls(\"/\")\n",
        "\n",
        "2Ô∏è‚É£ head - VER CONTENIDO DE ARCHIVO\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Sintaxis:\n",
        "%fs head <ruta> [--maxBytes N]\n",
        "\n",
        "Ejemplos:\n",
        "%fs head /FileStore/mi_archivo.txt\n",
        "%fs head /FileStore/data.csv --maxBytes 1000\n",
        "\n",
        "Muestra:\n",
        "‚Ä¢ Por defecto: primeros 65536 bytes\n",
        "‚Ä¢ Con --maxBytes: cantidad especificada\n",
        "\n",
        "‚ö†Ô∏è Solo para archivos de texto, no binarios\n",
        "\n",
        "Equivalente Python:\n",
        "dbutils.fs.head(\"/FileStore/mi_archivo.txt\")\n",
        "\n",
        "3Ô∏è‚É£ mkdirs - CREAR DIRECTORIOS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Sintaxis:\n",
        "%fs mkdirs <ruta>\n",
        "\n",
        "Ejemplos:\n",
        "%fs mkdirs /FileStore/mi_proyecto\n",
        "%fs mkdirs /FileStore/datos/input\n",
        "%fs mkdirs /tmp/resultados\n",
        "\n",
        "Caracter√≠sticas:\n",
        "‚Ä¢ Crea directorios padres autom√°ticamente (como mkdir -p)\n",
        "‚Ä¢ No da error si el directorio ya existe\n",
        "\n",
        "Equivalente Python:\n",
        "dbutils.fs.mkdirs(\"/FileStore/mi_proyecto\")\n",
        "\n",
        "4Ô∏è‚É£ rm - ELIMINAR ARCHIVOS/DIRECTORIOS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Sintaxis:\n",
        "%fs rm [-r] <ruta>\n",
        "\n",
        "Ejemplos:\n",
        "%fs rm /FileStore/temp.txt          # Eliminar archivo\n",
        "%fs rm -r /FileStore/old_data       # Eliminar directorio recursivo\n",
        "\n",
        "Opciones:\n",
        "-r : Recursivo (necesario para directorios)\n",
        "\n",
        "‚ö†Ô∏è PELIGRO: No hay papelera, eliminaci√≥n permanente\n",
        "\n",
        "Equivalente Python:\n",
        "dbutils.fs.rm(\"/FileStore/temp.txt\")\n",
        "dbutils.fs.rm(\"/FileStore/old_data\", True)  # recursivo\n",
        "\n",
        "5Ô∏è‚É£ cp - COPIAR ARCHIVOS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Sintaxis:\n",
        "%fs cp [-r] <origen> <destino>\n",
        "\n",
        "Ejemplos:\n",
        "%fs cp /FileStore/data.csv /FileStore/backup/data.csv\n",
        "%fs cp -r /FileStore/datos /FileStore/backup/datos\n",
        "\n",
        "Opciones:\n",
        "-r : Recursivo (para directorios)\n",
        "\n",
        "Equivalente Python:\n",
        "dbutils.fs.cp(\"/FileStore/data.csv\", \"/FileStore/backup/data.csv\")\n",
        "\n",
        "6Ô∏è‚É£ mv - MOVER/RENOMBRAR ARCHIVOS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Sintaxis:\n",
        "%fs mv <origen> <destino>\n",
        "\n",
        "Ejemplos:\n",
        "%fs mv /FileStore/old_name.csv /FileStore/new_name.csv\n",
        "%fs mv /FileStore/temp /FileStore/archive\n",
        "\n",
        "Equivalente Python:\n",
        "dbutils.fs.mv(\"/FileStore/old_name.csv\", \"/FileStore/new_name.csv\")\n",
        "\n",
        "7Ô∏è‚É£ put - CREAR ARCHIVO CON CONTENIDO\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Sintaxis:\n",
        "%fs put [-f] <ruta> \"<contenido>\"\n",
        "\n",
        "Ejemplos:\n",
        "%fs put /FileStore/test.txt \"Hola Mundo\"\n",
        "%fs put -f /FileStore/config.json '{\"key\": \"value\"}'\n",
        "\n",
        "Opciones:\n",
        "-f : Force (sobrescribe si existe)\n",
        "\n",
        "‚ö†Ô∏è Solo para archivos peque√±os (texto)\n",
        "\n",
        "Equivalente Python:\n",
        "dbutils.fs.put(\"/FileStore/test.txt\", \"Hola Mundo\", True)\n",
        "\n",
        "8Ô∏è‚É£ mount - MONTAR ALMACENAMIENTO EXTERNO\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Sintaxis:\n",
        "%fs mount <url> <punto_montaje>\n",
        "\n",
        "Ejemplo:\n",
        "# Montar bucket S3 (requiere configuraci√≥n)\n",
        "%fs mount s3a://mi-bucket /mnt/mi-bucket\n",
        "\n",
        "‚ö†Ô∏è Requiere configuraci√≥n de credenciales\n",
        "‚ö†Ô∏è En Free Edition: funcionalidad limitada\n",
        "\n",
        "9Ô∏è‚É£ unmount - DESMONTAR\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Sintaxis:\n",
        "%fs unmount <punto_montaje>\n",
        "\n",
        "Ejemplo:\n",
        "%fs unmount /mnt/mi-bucket\n",
        "\n",
        "üîü mounts - LISTAR MONTAJES\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Sintaxis:\n",
        "%fs mounts\n",
        "\n",
        "Muestra todos los puntos de montaje activos\n",
        "\n",
        "Equivalente Python:\n",
        "dbutils.fs.mounts()\n",
        "\"\"\"\n",
        "\n",
        "print(comandos_fs)\n",
        "```\n",
        "\n",
        "#### **Rutas Importantes en DBFS**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "RUTAS CLAVE EN DBFS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüóÇÔ∏è RUTAS IMPORTANTES EN DBFS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rutas_dbfs = \"\"\"\n",
        "ESTRUCTURA DE DBFS:\n",
        "\n",
        "dbfs:/\n",
        "‚îú‚îÄ‚îÄ FileStore/                  ‚Üê TU √ÅREA DE TRABAJO\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ tables/                 (Datos subidos via UI)\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ jars/                   (Librer√≠as Java)\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ [tus carpetas]          (Lo que t√∫ creas)\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ databricks-datasets/        ‚Üê DATASETS DE EJEMPLO\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ samples/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ learning-spark-v2/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ user/                       ‚Üê DATOS DE USUARIO\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ hive/\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ warehouse/          (Tablas managed)\n",
        "‚îÇ           ‚îú‚îÄ‚îÄ mi_tabla/\n",
        "‚îÇ           ‚îî‚îÄ‚îÄ ...\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ tmp/                        ‚Üê TEMPORAL\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ [archivos temporales]\n",
        "‚îÇ\n",
        "‚îî‚îÄ‚îÄ mnt/                        ‚Üê PUNTOS DE MONTAJE\n",
        "    ‚îî‚îÄ‚îÄ [montajes externos]\n",
        "\n",
        "RUTAS √öTILES:\n",
        "\n",
        "/FileStore/\n",
        "‚îú‚îÄ Prop√≥sito: Tu √°rea de trabajo personal\n",
        "‚îú‚îÄ Acceso: Completo (lectura/escritura)\n",
        "‚îú‚îÄ Persistencia: S√≠\n",
        "‚îî‚îÄ Uso: Guardar datos, notebooks exportados, resultados\n",
        "\n",
        "/databricks-datasets/\n",
        "‚îú‚îÄ Prop√≥sito: Datasets de ejemplo de Databricks\n",
        "‚îú‚îÄ Acceso: Solo lectura\n",
        "‚îú‚îÄ Contenido: CSV, JSON, Parquet de ejemplo\n",
        "‚îî‚îÄ Uso: Pr√°ctica, aprendizaje, demos\n",
        "\n",
        "/user/hive/warehouse/\n",
        "‚îú‚îÄ Prop√≥sito: Tablas managed (por defecto)\n",
        "‚îú‚îÄ Acceso: V√≠a Spark SQL\n",
        "‚îú‚îÄ Formato: Parquet/Delta\n",
        "‚îî‚îÄ Uso: Tablas creadas sin especificar LOCATION\n",
        "\n",
        "/tmp/\n",
        "‚îú‚îÄ Prop√≥sito: Archivos temporales\n",
        "‚îú‚îÄ Acceso: Lectura/escritura\n",
        "‚îú‚îÄ Persistencia: Puede limpiarse autom√°ticamente\n",
        "‚îî‚îÄ Uso: Resultados intermedios, cache\n",
        "\n",
        "NOTACI√ìN DE RUTAS:\n",
        "\n",
        "Tres formas equivalentes:\n",
        "1. %fs ls /FileStore                  ‚Üê Magic command\n",
        "2. dbutils.fs.ls(\"dbfs:/FileStore\")   ‚Üê Python con dbfs:\n",
        "3. dbutils.fs.ls(\"/FileStore\")        ‚Üê Python sin prefijo\n",
        "\n",
        "‚ö†Ô∏è Dentro de %fs: NO uses prefijo \"dbfs:\"\n",
        "‚úÖ %fs ls /FileStore           (correcto)\n",
        "‚ùå %fs ls dbfs:/FileStore      (incorrecto)\n",
        "\n",
        "ACCESO DESDE SPARK:\n",
        "\n",
        "# Leer archivo\n",
        "df = spark.read.csv(\"dbfs:/FileStore/data.csv\")\n",
        "df = spark.read.csv(\"/FileStore/data.csv\")  # Equivalente\n",
        "\n",
        "# Escribir archivo\n",
        "df.write.parquet(\"dbfs:/FileStore/output\")\n",
        "df.write.parquet(\"/FileStore/output\")  # Equivalente\n",
        "\"\"\"\n",
        "\n",
        "print(rutas_dbfs)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 2: %sh - Shell Commands**\n",
        "\n",
        "#### **¬øQu√© es %sh?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "%sh - SHELL COMMANDS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüíª %sh - SHELL MAGIC\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sh_info = \"\"\"\n",
        "DEFINICI√ìN:\n",
        "%sh permite ejecutar comandos de shell Linux directamente\n",
        "en el nodo DRIVER del cluster.\n",
        "\n",
        "‚ö†Ô∏è IMPORTANTE:\n",
        "‚Ä¢ Se ejecuta EN EL DRIVER, no en los workers\n",
        "‚Ä¢ Acceso al sistema de archivos LOCAL del driver\n",
        "‚Ä¢ NO es distribuido\n",
        "‚Ä¢ √ötil para comandos del sistema\n",
        "\n",
        "SINTAXIS:\n",
        "%sh <comando>\n",
        "\n",
        "SHELL:\n",
        "‚Ä¢ Bash shell est√°ndar de Linux\n",
        "‚Ä¢ Ubuntu (normalmente)\n",
        "\"\"\"\n",
        "\n",
        "print(sh_info)\n",
        "```\n",
        "\n",
        "#### **Comandos %sh Comunes**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "COMANDOS %sh √öTILES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìã COMANDOS %sh COMUNES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comandos_sh = \"\"\"\n",
        "1Ô∏è‚É£ INFORMACI√ìN DEL SISTEMA\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Ver informaci√≥n del sistema operativo\n",
        "%sh uname -a\n",
        "\n",
        "# Ver distribuci√≥n Linux\n",
        "%sh cat /etc/os-release\n",
        "\n",
        "# Ver uso de CPU y memoria\n",
        "%sh top -bn1 | head -20\n",
        "\n",
        "# Ver memoria disponible\n",
        "%sh free -h\n",
        "\n",
        "# Ver espacio en disco\n",
        "%sh df -h\n",
        "\n",
        "2Ô∏è‚É£ NAVEGACI√ìN Y LISTADO\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Directorio actual\n",
        "%sh pwd\n",
        "\n",
        "# Listar archivos\n",
        "%sh ls -lh\n",
        "\n",
        "# Listar con detalles\n",
        "%sh ls -lah /tmp\n",
        "\n",
        "# Ver √°rbol de directorios\n",
        "%sh tree /dbfs/FileStore -L 2\n",
        "\n",
        "3Ô∏è‚É£ MANIPULACI√ìN DE ARCHIVOS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Ver contenido de archivo\n",
        "%sh cat /dbfs/FileStore/test.txt\n",
        "\n",
        "# Ver primeras l√≠neas\n",
        "%sh head -n 10 /dbfs/FileStore/data.csv\n",
        "\n",
        "# Ver √∫ltimas l√≠neas\n",
        "%sh tail -n 10 /dbfs/FileStore/log.txt\n",
        "\n",
        "# Buscar en archivo\n",
        "%sh grep \"error\" /dbfs/FileStore/log.txt\n",
        "\n",
        "# Contar l√≠neas\n",
        "%sh wc -l /dbfs/FileStore/data.csv\n",
        "\n",
        "4Ô∏è‚É£ CREAR/MODIFICAR ARCHIVOS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Crear archivo\n",
        "%sh echo \"Hola Mundo\" > /dbfs/FileStore/hello.txt\n",
        "\n",
        "# A√±adir a archivo\n",
        "%sh echo \"Nueva l√≠nea\" >> /dbfs/FileStore/hello.txt\n",
        "\n",
        "# Copiar archivo\n",
        "%sh cp /dbfs/FileStore/data.csv /dbfs/FileStore/backup.csv\n",
        "\n",
        "# Mover/renombrar\n",
        "%sh mv /dbfs/FileStore/old.txt /dbfs/FileStore/new.txt\n",
        "\n",
        "# Eliminar archivo\n",
        "%sh rm /dbfs/FileStore/temp.txt\n",
        "\n",
        "# Eliminar directorio\n",
        "%sh rm -rf /dbfs/FileStore/temp_dir\n",
        "\n",
        "5Ô∏è‚É£ COMPRESI√ìN Y DESCOMPRESI√ìN\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Comprimir con gzip\n",
        "%sh gzip /dbfs/FileStore/large_file.csv\n",
        "\n",
        "# Descomprimir gzip\n",
        "%sh gunzip /dbfs/FileStore/large_file.csv.gz\n",
        "\n",
        "# Crear tar.gz\n",
        "%sh tar -czf /dbfs/FileStore/backup.tar.gz /dbfs/FileStore/data/\n",
        "\n",
        "# Extraer tar.gz\n",
        "%sh tar -xzf /dbfs/FileStore/backup.tar.gz -C /dbfs/FileStore/\n",
        "\n",
        "# Zip\n",
        "%sh zip -r /dbfs/FileStore/archive.zip /dbfs/FileStore/data/\n",
        "\n",
        "# Unzip\n",
        "%sh unzip /dbfs/FileStore/archive.zip -d /dbfs/FileStore/extracted/\n",
        "\n",
        "6Ô∏è‚É£ INFORMACI√ìN DE ARCHIVOS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Tama√±o de archivo\n",
        "%sh du -sh /dbfs/FileStore/data.csv\n",
        "\n",
        "# Tama√±o de directorio\n",
        "%sh du -sh /dbfs/FileStore/\n",
        "\n",
        "# Informaci√≥n detallada\n",
        "%sh stat /dbfs/FileStore/data.csv\n",
        "\n",
        "# Tipo de archivo\n",
        "%sh file /dbfs/FileStore/data.csv\n",
        "\n",
        "7Ô∏è‚É£ PROCESOS Y RED\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Ver procesos activos\n",
        "%sh ps aux | head -20\n",
        "\n",
        "# Ver conexiones de red\n",
        "%sh netstat -tuln\n",
        "\n",
        "# Ping (si est√° permitido)\n",
        "%sh ping -c 4 google.com\n",
        "\n",
        "# Descargar archivo\n",
        "%sh wget https://example.com/file.csv -O /dbfs/FileStore/file.csv\n",
        "\n",
        "# cURL\n",
        "%sh curl https://api.example.com/data\n",
        "\n",
        "8Ô∏è‚É£ PYTHON/PIP DEL SISTEMA\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Versi√≥n de Python\n",
        "%sh python --version\n",
        "\n",
        "# Pip instalado\n",
        "%sh pip list\n",
        "\n",
        "# Librer√≠as Python\n",
        "%sh pip show pandas\n",
        "\n",
        "‚ö†Ô∏è NOTA: Para instalar librer√≠as, mejor usa %pip (veremos luego)\n",
        "\n",
        "9Ô∏è‚É£ VARIABLES DE ENTORNO\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Ver todas las variables\n",
        "%sh env\n",
        "\n",
        "# Ver variable espec√≠fica\n",
        "%sh echo $HOME\n",
        "%sh echo $PATH\n",
        "%sh echo $SPARK_HOME\n",
        "\n",
        "# Establecer variable (solo para esa celda)\n",
        "%sh export MY_VAR=\"valor\" && echo $MY_VAR\n",
        "\n",
        "üîü COMBINAR COMANDOS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Pipe\n",
        "%sh ls -l /dbfs/FileStore | grep \".csv\"\n",
        "\n",
        "# AND l√≥gico\n",
        "%sh cd /dbfs/FileStore && ls -l\n",
        "\n",
        "# Redirecci√≥n\n",
        "%sh ls -l /dbfs/FileStore > /dbfs/FileStore/listado.txt\n",
        "\n",
        "# M√∫ltiples comandos\n",
        "%sh echo \"Empezando\" && sleep 2 && echo \"Terminado\"\n",
        "\"\"\"\n",
        "\n",
        "print(comandos_sh)\n",
        "```\n",
        "\n",
        "#### **Acceso a DBFS desde %sh**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DBFS DESDE SHELL\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîó ACCEDER A DBFS DESDE %sh\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "dbfs_sh = \"\"\"\n",
        "IMPORTANTE: DBFS est√° montado en /dbfs\n",
        "\n",
        "RUTAS:\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ DBFS Path        ‚Üî  Local Shell Path             ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ /FileStore       ‚Üî  /dbfs/FileStore              ‚îÇ\n",
        "‚îÇ /tmp             ‚Üî  /dbfs/tmp                    ‚îÇ\n",
        "‚îÇ /databricks-datasets ‚Üî /dbfs/databricks-datasets ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "EJEMPLOS:\n",
        "\n",
        "# INCORRECTO ‚ùå\n",
        "%sh ls /FileStore\n",
        "# Error: No existe (no est√°s en DBFS)\n",
        "\n",
        "# CORRECTO ‚úÖ\n",
        "%sh ls /dbfs/FileStore\n",
        "\n",
        "# Leer archivo DBFS\n",
        "%sh cat /dbfs/FileStore/data.txt\n",
        "\n",
        "# Crear archivo en DBFS desde shell\n",
        "%sh echo \"Datos\" > /dbfs/FileStore/nuevo.txt\n",
        "\n",
        "# Copiar de local a DBFS\n",
        "%sh cp /tmp/archivo.csv /dbfs/FileStore/\n",
        "\n",
        "LIMITACIONES:\n",
        "\n",
        "‚ö†Ô∏è /dbfs es una capa FUSE\n",
        "   ‚Ä¢ Puede ser m√°s lento que DBFS nativo\n",
        "   ‚Ä¢ No recomendado para operaciones grandes\n",
        "   ‚Ä¢ Usa Spark APIs para datasets grandes\n",
        "\n",
        "‚ö†Ô∏è Solo en el driver\n",
        "   ‚Ä¢ Workers no tienen /dbfs montado\n",
        "   ‚Ä¢ Para operaciones distribuidas, usa Spark\n",
        "\n",
        "RECOMENDACI√ìN:\n",
        "\n",
        "Para archivos peque√±os (<1 GB): %sh est√° bien\n",
        "Para archivos grandes: Usa Spark o %fs\n",
        "\"\"\"\n",
        "\n",
        "print(dbfs_sh)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 3: %md - Markdown**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "%md - MARKDOWN\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìù %md - MARKDOWN MAGIC\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "md_info = \"\"\"\n",
        "DEFINICI√ìN:\n",
        "%md convierte la celda en Markdown renderizado\n",
        "\n",
        "SINTAXIS:\n",
        "%md\n",
        "# Tu contenido markdown aqu√≠\n",
        "\n",
        "EQUIVALENTE:\n",
        "Cambiar tipo de celda a \"Markdown\" en la UI\n",
        "\n",
        "¬øCU√ÅNDO USAR %md vs Celda Markdown?\n",
        "\n",
        "%md:\n",
        "‚Ä¢ Cuando quieres documentaci√≥n inline en celda de c√≥digo\n",
        "‚Ä¢ Para documentaci√≥n program√°tica\n",
        "\n",
        "Celda Markdown:\n",
        "‚Ä¢ M√°s com√∫n y recomendado\n",
        "‚Ä¢ Mejor experiencia de edici√≥n\n",
        "‚Ä¢ M√°s f√°cil de mantener\n",
        "\n",
        "EJEMPLO:\n",
        "\"\"\"\n",
        "\n",
        "print(md_info)\n",
        "\n",
        "ejemplo_md = \"\"\"\n",
        "%md\n",
        "# Mi An√°lisis de Datos\n",
        "\n",
        "## Objetivos\n",
        "1. Cargar datos\n",
        "2. Limpiar\n",
        "3. Analizar\n",
        "\n",
        "**Fecha**: 2025-02-02  \n",
        "**Autor**: Tu Nombre\n",
        "\n",
        "---\n",
        "\n",
        "### M√©tricas Clave\n",
        "- Ventas: $1,000,000\n",
        "- Clientes: 5,000\n",
        "- Ticket promedio: $200\n",
        "\n",
        "> üí° Nota importante sobre los datos\n",
        "\n",
        "```python\n",
        "# C√≥digo de ejemplo\n",
        "df = spark.read.csv(\"data.csv\")\n",
        "```\n",
        "\n",
        "[Documentaci√≥n](https://docs.databricks.com)\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_md)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 4: %run - Ejecutar Notebooks**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "%run - EJECUTAR OTROS NOTEBOOKS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüèÉ %run - EJECUTAR NOTEBOOKS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "run_info = \"\"\"\n",
        "DEFINICI√ìN:\n",
        "%run ejecuta otro notebook e importa todas sus variables\n",
        "al notebook actual\n",
        "\n",
        "SINTAXIS:\n",
        "%run /path/to/notebook\n",
        "\n",
        "CASOS DE USO:\n",
        "\n",
        "1Ô∏è‚É£ C√ìDIGO REUTILIZABLE\n",
        "Tienes funciones comunes en un notebook\n",
        "M√∫ltiples notebooks las necesitan\n",
        "\n",
        "2Ô∏è‚É£ CONFIGURACI√ìN\n",
        "Notebook con par√°metros y configuraci√≥n\n",
        "Importar en todos los notebooks del proyecto\n",
        "\n",
        "3Ô∏è‚É£ MODULARIZACI√ìN\n",
        "Dividir proyecto grande en notebooks peque√±os\n",
        "Notebook principal orquesta los dem√°s\n",
        "\n",
        "EJEMPLO:\n",
        "\n",
        "# Notebook: /Shared/utils\n",
        "# Contiene:\n",
        "def limpiar_datos(df):\n",
        "    return df.dropna()\n",
        "\n",
        "def calcular_metricas(df):\n",
        "    return df.describe()\n",
        "\n",
        "CONFIG = {\n",
        "    \"database\": \"prod\",\n",
        "    \"table\": \"ventas\"\n",
        "}\n",
        "\n",
        "---\n",
        "\n",
        "# Notebook principal\n",
        "%run /Shared/utils\n",
        "\n",
        "# Ahora puedes usar las funciones\n",
        "df = spark.read.table(\"datos\")\n",
        "df_limpio = limpiar_datos(df)  # ‚úÖ Funci√≥n disponible\n",
        "metricas = calcular_metricas(df_limpio)  # ‚úÖ Funci√≥n disponible\n",
        "\n",
        "print(CONFIG)  # ‚úÖ Variable disponible\n",
        "\n",
        "CARACTER√çSTICAS:\n",
        "\n",
        "‚úÖ Importa TODO: funciones, variables, imports\n",
        "‚úÖ Ejecuta en el mismo contexto (mismo SparkSession)\n",
        "‚úÖ S√≠ncrono (espera a que termine)\n",
        "‚úÖ Puede anidar (%run puede llamar otro %run)\n",
        "\n",
        "‚ö†Ô∏è LIMITACIONES:\n",
        "\n",
        "‚Ä¢ Solo notebooks en el mismo workspace\n",
        "‚Ä¢ No puede pasar argumentos directamente\n",
        "  (usa widgets para parametrizaci√≥n)\n",
        "‚Ä¢ Puede causar conflictos de nombres\n",
        "‚Ä¢ Dificulta debugging\n",
        "\n",
        "ALTERNATIVA MEJOR (para c√≥digo reutilizable):\n",
        "\n",
        "En lugar de %run, considera:\n",
        "‚Ä¢ Python modules (.py files)\n",
        "‚Ä¢ Librer√≠as compartidas\n",
        "‚Ä¢ Repos con Git\n",
        "\n",
        "EJEMPLO COMPLETO:\n",
        "\"\"\"\n",
        "\n",
        "print(run_info)\n",
        "\n",
        "ejemplo_run_completo = \"\"\"\n",
        "ESTRUCTURA:\n",
        "\n",
        "/Users/tu_email/\n",
        "‚îú‚îÄ‚îÄ 00_Config\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ config_notebook          ‚Üê Configuraci√≥n\n",
        "‚îú‚îÄ‚îÄ 01_Utils\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ funciones_comunes        ‚Üê Funciones reutilizables\n",
        "‚îî‚îÄ‚îÄ 02_Analysis\n",
        "    ‚îî‚îÄ‚îÄ analisis_ventas          ‚Üê Notebook principal\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "NOTEBOOK: config_notebook\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Celda 1\n",
        "# Configuraci√≥n del proyecto\n",
        "CONFIG = {\n",
        "    \"env\": \"production\",\n",
        "    \"database\": \"ventas_db\",\n",
        "    \"fecha_inicio\": \"2025-01-01\",\n",
        "    \"fecha_fin\": \"2025-12-31\"\n",
        "}\n",
        "\n",
        "RUTAS = {\n",
        "    \"input\": \"/FileStore/input/\",\n",
        "    \"output\": \"/FileStore/output/\",\n",
        "    \"logs\": \"/FileStore/logs/\"\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Configuraci√≥n cargada\")\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "NOTEBOOK: funciones_comunes\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Celda 1\n",
        "from pyspark.sql.functions import col, sum, avg, count\n",
        "\n",
        "def limpiar_ventas(df):\n",
        "    \"\"\"Limpia DataFrame de ventas\"\"\"\n",
        "    return df.filter(col(\"monto\") > 0) \\\\\n",
        "             .dropna(subset=[\"cliente_id\", \"producto\"])\n",
        "\n",
        "def calcular_metricas_ventas(df):\n",
        "    \"\"\"Calcula m√©tricas agregadas\"\"\"\n",
        "    return df.agg(\n",
        "        sum(\"monto\").alias(\"total\"),\n",
        "        avg(\"monto\").alias(\"promedio\"),\n",
        "        count(\"*\").alias(\"num_ventas\")\n",
        "    )\n",
        "\n",
        "def guardar_con_timestamp(df, ruta_base):\n",
        "    \"\"\"Guarda DataFrame con timestamp\"\"\"\n",
        "    from datetime import datetime\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    ruta = f\"{ruta_base}_{timestamp}\"\n",
        "    df.write.parquet(ruta)\n",
        "    print(f\"‚úÖ Guardado en: {ruta}\")\n",
        "\n",
        "print(\"‚úÖ Funciones cargadas\")\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "NOTEBOOK: analisis_ventas (PRINCIPAL)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Celda 1 - Cargar dependencias\n",
        "%run ./00_Config/config_notebook\n",
        "%run ./01_Utils/funciones_comunes\n",
        "\n",
        "print(\"üöÄ Iniciando an√°lisis...\")\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 2 - Cargar datos\n",
        "# CONFIG est√° disponible (viene de config_notebook)\n",
        "df_raw = spark.read.parquet(f\"{CONFIG['database']}.ventas\")\n",
        "print(f\"üìä Registros cargados: {df_raw.count()}\")\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 3 - Limpiar\n",
        "# limpiar_ventas() est√° disponible (viene de funciones_comunes)\n",
        "df_clean = limpiar_ventas(df_raw)\n",
        "print(f\"‚úÖ Registros despu√©s de limpieza: {df_clean.count()}\")\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 4 - Analizar\n",
        "# calcular_metricas_ventas() est√° disponible\n",
        "metricas = calcular_metricas_ventas(df_clean)\n",
        "metricas.show()\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 5 - Guardar\n",
        "# guardar_con_timestamp() y RUTAS est√°n disponibles\n",
        "guardar_con_timestamp(df_clean, RUTAS['output'])\n",
        "\n",
        "print(\"‚úÖ An√°lisis completado\")\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_run_completo)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 5: %pip - Gesti√≥n de Paquetes Python**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "%pip - GESTI√ìN DE PAQUETES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüì¶ %pip - PACKAGE MANAGEMENT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pip_info = \"\"\"\n",
        "DEFINICI√ìN:\n",
        "%pip permite instalar, actualizar y gestionar paquetes Python\n",
        "en el cluster desde el notebook\n",
        "\n",
        "VENTAJAS sobre %sh pip:\n",
        "‚úÖ Instala en todos los nodos (driver + workers)\n",
        "‚úÖ Reinicia autom√°ticamente el int√©rprete Python\n",
        "‚úÖ M√°s seguro y recomendado\n",
        "\n",
        "COMANDOS PRINCIPALES:\n",
        "\n",
        "1Ô∏è‚É£ INSTALL - Instalar paquete\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "%pip install nombre_paquete\n",
        "\n",
        "Ejemplos:\n",
        "%pip install pandas==1.5.3          # Versi√≥n espec√≠fica\n",
        "%pip install matplotlib             # √öltima versi√≥n\n",
        "%pip install numpy scikit-learn     # M√∫ltiples paquetes\n",
        "\n",
        "2Ô∏è‚É£ INSTALL desde requirements.txt\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "%pip install -r /dbfs/FileStore/requirements.txt\n",
        "\n",
        "3Ô∏è‚É£ UPGRADE - Actualizar paquete\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "%pip install --upgrade nombre_paquete\n",
        "\n",
        "Ejemplo:\n",
        "%pip install --upgrade pandas\n",
        "\n",
        "4Ô∏è‚É£ UNINSTALL - Desinstalar paquete\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "%pip uninstall nombre_paquete -y\n",
        "\n",
        "Ejemplo:\n",
        "%pip uninstall old_package -y    # -y para no pedir confirmaci√≥n\n",
        "\n",
        "5Ô∏è‚É£ LIST - Listar paquetes instalados\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "%pip list\n",
        "\n",
        "Muestra todos los paquetes con versiones\n",
        "\n",
        "6Ô∏è‚É£ SHOW - Info de paquete espec√≠fico\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "%pip show nombre_paquete\n",
        "\n",
        "Ejemplo:\n",
        "%pip show pandas\n",
        "\n",
        "CARACTER√çSTICAS IMPORTANTES:\n",
        "\n",
        "‚ö†Ô∏è REINICIO AUTOM√ÅTICO:\n",
        "Despu√©s de %pip install, Python se reinicia\n",
        "‚Ä¢ Variables en memoria SE PIERDEN\n",
        "‚Ä¢ Necesitas re-ejecutar celdas anteriores\n",
        "‚Ä¢ O usa %pip al inicio del notebook\n",
        "\n",
        "üí° MEJOR PR√ÅCTICA:\n",
        "Todas las instalaciones %pip al INICIO del notebook\n",
        "En las primeras celdas, antes de crear variables\n",
        "\n",
        "‚ö†Ô∏è PERSISTENCIA:\n",
        "‚Ä¢ Las instalaciones %pip NO persisten al reiniciar cluster\n",
        "‚Ä¢ Cada vez que inicies cluster, debes reinstalar\n",
        "‚Ä¢ Para permanente: Usar librer√≠as de cluster (Admin)\n",
        "\n",
        "EJEMPLO COMPLETO:\n",
        "\"\"\"\n",
        "\n",
        "print(pip_info)\n",
        "\n",
        "ejemplo_pip = \"\"\"\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# NOTEBOOK: An√°lisis con librer√≠as externas\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Celda 1 - Instalar dependencias (SIEMPRE PRIMERO)\n",
        "%pip install plotly\n",
        "%pip install seaborn\n",
        "\n",
        "# ‚ö†Ô∏è Python se reinicia autom√°ticamente aqu√≠\n",
        "# ‚ö†Ô∏è Contin√∫a en la siguiente celda\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 2 - Verificar instalaci√≥n\n",
        "import plotly\n",
        "import seaborn as sns\n",
        "print(f\"‚úÖ Plotly version: {plotly.__version__}\")\n",
        "print(f\"‚úÖ Seaborn version: {sns.__version__}\")\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 3 - Usar las librer√≠as\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Crear datos\n",
        "df = pd.DataFrame({\n",
        "    'x': [1, 2, 3, 4, 5],\n",
        "    'y': [10, 11, 12, 13, 14]\n",
        "})\n",
        "\n",
        "# Crear gr√°fico con Plotly\n",
        "fig = px.line(df, x='x', y='y', title='Mi Gr√°fico')\n",
        "fig.show()\n",
        "\n",
        "---\n",
        "\n",
        "# ALTERNATIVA: requirements.txt\n",
        "\n",
        "# Si tienes muchas dependencias, crea requirements.txt:\n",
        "\n",
        "# Celda 1 - Crear requirements.txt\n",
        "%sh cat > /dbfs/FileStore/requirements.txt << EOF\n",
        "plotly==5.18.0\n",
        "seaborn==0.12.2\n",
        "scikit-learn==1.3.2\n",
        "xgboost==2.0.2\n",
        "EOF\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 2 - Instalar desde archivo\n",
        "%pip install -r /dbfs/FileStore/requirements.txt\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 3 - Usar librer√≠as\n",
        "import plotly\n",
        "import seaborn\n",
        "import sklearn\n",
        "import xgboost\n",
        "\n",
        "print(\"‚úÖ Todas las librer√≠as instaladas\")\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_pip)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 6: Otros Magic Commands √ötiles**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "OTROS MAGIC COMMANDS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüéÅ OTROS MAGIC COMMANDS √öTILES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "otros_magic = \"\"\"\n",
        "1Ô∏è‚É£ %lsmagic - LISTAR TODOS LOS MAGIC COMMANDS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "%lsmagic\n",
        "\n",
        "Muestra todos los magic commands disponibles en tu sesi√≥n\n",
        "\n",
        "2Ô∏è‚É£ %time - MEDIR TIEMPO DE EJECUCI√ìN\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "‚ö†Ô∏è NO disponible directamente en Databricks\n",
        "Alternativa: Usar time de Python\n",
        "\n",
        "import time\n",
        "inicio = time.time()\n",
        "# Tu c√≥digo aqu√≠\n",
        "fin = time.time()\n",
        "print(f\"Tiempo: {fin - inicio:.2f} segundos\")\n",
        "\n",
        "3Ô∏è‚É£ DISPLAY - VISUALIZACI√ìN MEJORADA\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "No es magic command, pero muy √∫til:\n",
        "\n",
        "display(df)  # Tabla interactiva con gr√°ficos\n",
        "\n",
        "Caracter√≠sticas:\n",
        "‚Ä¢ Tablas interactivas\n",
        "‚Ä¢ Botones para crear gr√°ficos\n",
        "‚Ä¢ Filtrado inline\n",
        "‚Ä¢ Exportar a CSV\n",
        "\n",
        "4Ô∏è‚É£ displayHTML - HTML CUSTOM\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "displayHTML(\"<h1>Mi HTML</h1>\")\n",
        "\n",
        "displayHTML('''\n",
        "<div style=\"background-color: lightblue; padding: 20px;\">\n",
        "  <h2>T√≠tulo Personalizado</h2>\n",
        "  <p>Contenido HTML</p>\n",
        "</div>\n",
        "''')\n",
        "\n",
        "5Ô∏è‚É£ dbutils - UTILIDADES DE DATABRICKS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "No es magic, pero fundamental:\n",
        "\n",
        "# File system\n",
        "dbutils.fs.ls(\"/\")\n",
        "dbutils.fs.mkdirs(\"/FileStore/nuevo\")\n",
        "\n",
        "# Widgets\n",
        "dbutils.widgets.text(\"param\", \"default\")\n",
        "value = dbutils.widgets.get(\"param\")\n",
        "\n",
        "# Notebook\n",
        "dbutils.notebook.run(\"/path/to/notebook\", 60)\n",
        "\n",
        "# Secrets\n",
        "password = dbutils.secrets.get(scope=\"my-scope\", key=\"password\")\n",
        "\n",
        "Veremos dbutils.widgets en la siguiente secci√≥n\n",
        "\"\"\"\n",
        "\n",
        "print(otros_magic)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Dominando Magic Commands**\n",
        "\n",
        "```markdown\n",
        "üß™ EJERCICIO: EXPLORAR MAGIC COMMANDS\n",
        "\n",
        "Vamos a practicar todos los magic commands aprendidos.\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "SETUP (2 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "1. **Crear notebook**:\n",
        "   - [ ] Name: \"Practica_4_3_Magic_Commands\"\n",
        "   - [ ] Default Language: Python\n",
        "   - [ ] Connect cluster\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 1: FILE SYSTEM (%fs) (10 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "2. **Celda 1 - Explorar DBFS**:\n",
        "```\n",
        "%fs ls /\n",
        "```\n",
        "   - [ ] Ejecutar y anotar qu√© directorios ves\n",
        "\n",
        "3. **Celda 2 - Ver datasets de ejemplo**:\n",
        "```\n",
        "%fs ls /databricks-datasets\n",
        "```\n",
        "\n",
        "4. **Celda 3 - Crear tu directorio**:\n",
        "```\n",
        "%fs mkdirs /FileStore/mi_practica\n",
        "```\n",
        "\n",
        "5. **Celda 4 - Crear archivo de prueba**:\n",
        "```\n",
        "%fs put /FileStore/mi_practica/test.txt \"Hola desde DBFS\"\n",
        "```\n",
        "\n",
        "6. **Celda 5 - Leer el archivo**:\n",
        "```\n",
        "%fs head /FileStore/mi_practica/test.txt\n",
        "```\n",
        "\n",
        "7. **Celda 6 - Listar tu directorio**:\n",
        "```\n",
        "%fs ls /FileStore/mi_practica\n",
        "```\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 2: SHELL COMMANDS (%sh) (10 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "8. **Celda 7 - Info del sistema**:\n",
        "```\n",
        "%sh uname -a\n",
        "```\n",
        "\n",
        "9. **Celda 8 - Ver Python version**:\n",
        "```\n",
        "%sh python --version\n",
        "```\n",
        "\n",
        "10. **Celda 9 - Acceder a DBFS desde shell**:\n",
        "```\n",
        "%sh ls -lh /dbfs/FileStore/mi_practica\n",
        "```\n",
        "\n",
        "11. **Celda 10 - Ver contenido con cat**:\n",
        "```\n",
        "%sh cat /dbfs/FileStore/mi_practica/test.txt\n",
        "```\n",
        "\n",
        "12. **Celda 11 - Crear archivo desde shell**:\n",
        "```\n",
        "%sh echo \"Creado desde shell\" > /dbfs/FileStore/mi_practica/shell.txt\n",
        "```\n",
        "\n",
        "13. **Celda 12 - Verificar con %fs**:\n",
        "```\n",
        "%fs ls /FileStore/mi_practica\n",
        "```\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 3: CREAR DATOS Y USAR %run (15 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "14. **Crear notebook auxiliar**:\n",
        "    - [ ] Name: \"Utils_Funciones\"\n",
        "    - [ ] En misma carpeta que notebook principal\n",
        "    - [ ] A√±adir este c√≥digo:\n",
        "    \n",
        "```python\n",
        "# Funciones √∫tiles\n",
        "from pyspark.sql.functions import col, sum, avg\n",
        "\n",
        "def crear_datos_ejemplo():\n",
        "    \"\"\"Crea DataFrame de ejemplo\"\"\"\n",
        "    datos = [\n",
        "        (\"Producto A\", 100, 5),\n",
        "        (\"Producto B\", 50, 10),\n",
        "        (\"Producto C\", 200, 3)\n",
        "    ]\n",
        "    return spark.createDataFrame(datos, [\"producto\", \"precio\", \"cantidad\"])\n",
        "\n",
        "def calcular_total(df):\n",
        "    \"\"\"Calcula ingresos totales\"\"\"\n",
        "    return df.withColumn(\"total\", col(\"precio\") * col(\"cantidad\"))\n",
        "\n",
        "print(\"‚úÖ Funciones cargadas desde Utils_Funciones\")\n",
        "```\n",
        "\n",
        "15. **Volver al notebook principal - Celda 13**:\n",
        "```python\n",
        "# Importar funciones del otro notebook\n",
        "%run ./Utils_Funciones\n",
        "```\n",
        "\n",
        "16. **Celda 14 - Usar funciones importadas**:\n",
        "```python\n",
        "# Crear datos usando funci√≥n importada\n",
        "df = crear_datos_ejemplo()\n",
        "df.show()\n",
        "\n",
        "# Calcular totales usando funci√≥n importada\n",
        "df_totales = calcular_total(df)\n",
        "df_totales.show()\n",
        "\n",
        "print(\"‚úÖ Funciones de otro notebook funcionan!\")\n",
        "```\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 4: INSTALAR PAQUETE (%pip) (5 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "17. **Celda 15 - Instalar librer√≠a**:\n",
        "```python\n",
        "%pip install emoji\n",
        "```\n",
        "    - [ ] Esperar a que se reinicie Python\n",
        "\n",
        "18. **Celda 16 - Usar librer√≠a instalada**:\n",
        "```python\n",
        "import emoji\n",
        "\n",
        "mensaje = emoji.emojize(\"Python es :thumbs_up: :fire:\")\n",
        "print(mensaje)\n",
        "\n",
        "# Mostrar algunos emojis\n",
        "print(emoji.emojize(\":snake: PySpark\"))\n",
        "print(emoji.emojize(\":rocket: Databricks\"))\n",
        "print(emoji.emojize(\":chart_increasing: Analytics\"))\n",
        "```\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 5: COMBINAR TODO (10 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "19. **Celda 17 - Markdown documentaci√≥n**:\n",
        "```\n",
        "%md\n",
        "## Resumen de Pr√°ctica\n",
        "\n",
        "He practicado:\n",
        "- ‚úÖ %fs para manipular archivos\n",
        "- ‚úÖ %sh para comandos shell\n",
        "- ‚úÖ %run para reutilizar c√≥digo\n",
        "- ‚úÖ %pip para instalar librer√≠as\n",
        "- ‚úÖ Interacci√≥n entre magic commands\n",
        "```\n",
        "\n",
        "20. **Celda 18 - Crear reporte y guardarlo**:\n",
        "```python\n",
        "# Crear reporte con datos\n",
        "df_totales_python = df_totales.toPandas()\n",
        "\n",
        "# Crear contenido del reporte\n",
        "reporte = f\"\"\"\n",
        "REPORTE DE VENTAS\n",
        "==================\n",
        "\n",
        "Fecha: 2025-02-02\n",
        "Generado con: Databricks + Magic Commands\n",
        "\n",
        "Productos analizados: {len(df_totales_python)}\n",
        "\n",
        "Detalles:\n",
        "{df_totales_python.to_string()}\n",
        "\n",
        "Total general: ${df_totales_python['total'].sum():,.2f}\n",
        "\n",
        "---\n",
        "Generado autom√°ticamente\n",
        "\"\"\"\n",
        "\n",
        "print(reporte)\n",
        "\n",
        "# Guardar reporte usando %fs\n",
        "dbutils.fs.put(\n",
        "    \"/FileStore/mi_practica/reporte.txt\",\n",
        "    reporte,\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Reporte guardado en /FileStore/mi_practica/reporte.txt\")\n",
        "```\n",
        "\n",
        "21. **Celda 19 - Verificar con %fs**:\n",
        "```\n",
        "%fs head /FileStore/mi_practica/reporte.txt\n",
        "```\n",
        "\n",
        "22. **Celda 20 - Limpiar (opcional)**:\n",
        "```\n",
        "%fs rm -r /FileStore/mi_practica\n",
        "```\n",
        "\n",
        "**TIEMPO TOTAL**: ~50 minutos\n",
        "\n",
        "‚úÖ ¬°Has dominado los magic commands!\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 4.3 - Desaf√≠o**\n",
        "\n",
        "```markdown\n",
        "### Desaf√≠o: Pipeline Completo con Magic Commands\n",
        "\n",
        "**Objetivo**: Crear un pipeline que usa todos los magic commands\n",
        "\n",
        "**Escenario**:\n",
        "Eres data engineer y necesitas:\n",
        "1. Descargar datos de una API\n",
        "2. Procesarlos con Spark\n",
        "3. Generar reporte\n",
        "4. Guardarlo en DBFS\n",
        "\n",
        "**Tareas**:\n",
        "\n",
        "1. **Usar %sh para descargar datos**:\n",
        "   ```\n",
        "   %sh curl \"https://api.github.com/repos/databricks/koalas/contributors\" -o /dbfs/FileStore/contributors.json\n",
        "   ```\n",
        "\n",
        "2. **Usar Python para procesar**:\n",
        "   - Leer JSON\n",
        "   - Extraer top 5 contributors\n",
        "   - Calcular estad√≠sticas\n",
        "\n",
        "3. **Crear notebook con funciones auxiliares**:\n",
        "   - Funci√≥n para limpiar datos\n",
        "   - Funci√≥n para formatear output\n",
        "   - Importar con %run\n",
        "\n",
        "4. **Usar %fs para gestionar archivos**:\n",
        "   - Crear directorio de salida\n",
        "   - Guardar resultados\n",
        "   - Verificar tama√±o\n",
        "\n",
        "5. **Bonus con %pip**:\n",
        "   - Instalar `requests`\n",
        "   - Hacer request a otra API\n",
        "   - Combinar datasets\n",
        "\n",
        "6. **Generar reporte final**:\n",
        "   - Usar %md para documentaci√≥n\n",
        "   - Usar %sh para crear tar.gz del resultado\n",
        "   - Usar %fs para verificar\n",
        "\n",
        "**Entregable**:\n",
        "Notebook funcionando que ejecuta todo el pipeline\n",
        "con magic commands apropiados en cada paso\n",
        "\n",
        "**Tiempo estimado**: 45-60 minutos\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **%fs** = File system commands (DBFS)\n",
        "2. **%sh** = Shell commands (Linux en driver)\n",
        "3. **%md** = Markdown renderizado\n",
        "4. **%run** = Ejecutar otro notebook\n",
        "5. **%pip** = Gestionar paquetes Python\n",
        "6. **/dbfs/** = Ruta para acceder DBFS desde shell\n",
        "7. **%pip al inicio** del notebook (antes de variables)\n",
        "8. **%fs NO necesita** prefijo \"dbfs:\"\n",
        "9. **%sh se ejecuta** solo en driver, no distribuido\n",
        "10. **%run importa todo**: variables, funciones, imports\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **%fs vs %sh para archivos**\n",
        "```markdown\n",
        "Archivos en DBFS ‚Üí Preferir %fs\n",
        "Archivos locales del sistema ‚Üí %sh\n",
        "```\n",
        "\n",
        "‚úÖ **Organiza instalaciones**\n",
        "```markdown\n",
        "Todas las instalaciones %pip en celdas iniciales\n",
        "Documenta versiones espec√≠ficas\n",
        "```\n",
        "\n",
        "‚úÖ **%run para modularizaci√≥n**\n",
        "```markdown\n",
        "Notebook de configuraci√≥n ‚Üí Importar en todos\n",
        "Notebook de funciones ‚Üí Reutilizar c√≥digo\n",
        "```\n",
        "\n",
        "‚úÖ **Documenta magic commands**\n",
        "```markdown\n",
        "# %fs para acceder datos en FileStore\n",
        "%fs ls /FileStore/datos\n",
        "```\n",
        "\n",
        "‚úÖ **Combina magic commands**\n",
        "```markdown\n",
        "%sh para descargar\n",
        "%fs para verificar\n",
        "Python para procesar\n",
        "```\n",
        "\n",
        "‚úÖ **Evita %sh para operaciones grandes**\n",
        "```markdown\n",
        "Solo en driver, no distribuido\n",
        "Para datasets grandes ‚Üí Usa Spark\n",
        "```\n",
        "\n",
        "‚úÖ **DBFS paths**\n",
        "```markdown\n",
        "Dentro de %fs: /FileStore\n",
        "Dentro de %sh: /dbfs/FileStore\n",
        "Dentro de Spark: dbfs:/FileStore o /FileStore\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Has dominado los magic commands! üéâ**\n",
        "\n",
        "Ahora sabes:\n",
        "- Manipular archivos con %fs ‚úÖ\n",
        "- Ejecutar comandos shell con %sh ‚úÖ\n",
        "- Crear notebooks modulares con %run ‚úÖ\n",
        "- Gestionar paquetes con %pip ‚úÖ\n",
        "- Usar todos los magic commands apropiadamente ‚úÖ\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el punto final del Tema 4: **4.4 Widgets para parametrizaci√≥n**? üöÄ"
      ],
      "metadata": {
        "id": "P9GRHvkGkS7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.4 Widgets para parametrizaci√≥n**\n",
        "\n",
        "#### **Introducci√≥n: Notebooks Din√°micos e Interactivos**\n",
        "\n",
        "Los **widgets** son controles de interfaz (inputs, dropdowns, etc.) que permiten que los usuarios pasen par√°metros a los notebooks sin modificar el c√≥digo. Son fundamentales para hacer notebooks reutilizables, interactivos y profesionales.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "WIDGETS EN DATABRICKS\n",
        "\"\"\"\n",
        "\n",
        "widgets_intro = {\n",
        "    'definici√≥n': 'Controles UI para pasar par√°metros a notebooks',\n",
        "    'ubicaci√≥n': 'Parte superior del notebook, debajo del nombre',\n",
        "    'api': 'dbutils.widgets',\n",
        "    'casos_uso': [\n",
        "        'Parametrizar consultas',\n",
        "        'Hacer notebooks reutilizables',\n",
        "        'Crear dashboards interactivos',\n",
        "        'Workflows con par√°metros',\n",
        "        'Testing con diferentes valores'\n",
        "    ],\n",
        "    'tipos': ['text', 'dropdown', 'combobox', 'multiselect'],\n",
        "    'ventajas': [\n",
        "        'No necesitas modificar c√≥digo',\n",
        "        'Usuarios no t√©cnicos pueden usarlos',\n",
        "        'F√°cil testing de escenarios',\n",
        "        'Notebooks m√°s profesionales'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"üéõÔ∏è WIDGETS EN DATABRICKS\")\n",
        "print(\"=\"*60)\n",
        "for key, value in widgets_intro.items():\n",
        "    if isinstance(value, list):\n",
        "        print(f\"\\n{key.replace('_', ' ').title()}:\")\n",
        "        for item in value:\n",
        "            print(f\"  ‚Ä¢ {item}\")\n",
        "    else:\n",
        "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 1: Tipos de Widgets**\n",
        "\n",
        "#### **1. Text Widget - Entrada de Texto**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "TEXT WIDGET\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìù TEXT WIDGET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "text_widget_info = \"\"\"\n",
        "DEFINICI√ìN:\n",
        "Widget de entrada de texto libre\n",
        "\n",
        "SINTAXIS:\n",
        "dbutils.widgets.text(name, defaultValue, label)\n",
        "\n",
        "PAR√ÅMETROS:\n",
        "‚Ä¢ name: Nombre interno del widget (identificador √∫nico)\n",
        "‚Ä¢ defaultValue: Valor por defecto\n",
        "‚Ä¢ label: Etiqueta visible (opcional, por defecto = name)\n",
        "\n",
        "CREAR:\n",
        "\"\"\"\n",
        "\n",
        "print(text_widget_info)\n",
        "\n",
        "ejemplo_text = \"\"\"\n",
        "# Crear widget de texto\n",
        "dbutils.widgets.text(\"nombre_usuario\", \"Juan\", \"Nombre del Usuario\")\n",
        "\n",
        "# Obtener valor\n",
        "nombre = dbutils.widgets.get(\"nombre_usuario\")\n",
        "print(f\"Hola, {nombre}!\")\n",
        "\n",
        "# Output (si dejas default):\n",
        "# Hola, Juan!\n",
        "\n",
        "# Output (si escribes \"Mar√≠a\" en el widget):\n",
        "# Hola, Mar√≠a!\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "APARIENCIA EN EL NOTEBOOK:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Mi Notebook | Python | Cluster | Run All               ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Nombre del Usuario: [Juan                    ] [‚ü≥]     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "           ‚ñ≤                ‚ñ≤                      ‚ñ≤\n",
        "        Label         Input box            Reset button\n",
        "\n",
        "CARACTER√çSTICAS:\n",
        "‚Ä¢ Texto libre (cualquier string)\n",
        "‚Ä¢ Validaci√≥n manual (si necesitas)\n",
        "‚Ä¢ √ötil para nombres, IDs, fechas en string\n",
        "\n",
        "CASOS DE USO:\n",
        "‚Ä¢ Nombre de tabla a consultar\n",
        "‚Ä¢ Ruta de archivo\n",
        "‚Ä¢ Fecha espec√≠fica\n",
        "‚Ä¢ ID de usuario/cliente\n",
        "‚Ä¢ Query parameter\n",
        "\n",
        "EJEMPLO PR√ÅCTICO:\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_text)\n",
        "\n",
        "ejemplo_text_practico = \"\"\"\n",
        "# Celda 1 - Crear widget\n",
        "dbutils.widgets.text(\"tabla\", \"ventas\", \"Nombre de Tabla\")\n",
        "\n",
        "# Celda 2 - Usar widget en query\n",
        "tabla_nombre = dbutils.widgets.get(\"tabla\")\n",
        "\n",
        "# Leer tabla parametrizada\n",
        "df = spark.read.table(tabla_nombre)\n",
        "print(f\"üìä Tabla '{tabla_nombre}' cargada: {df.count()} registros\")\n",
        "\n",
        "# Ahora puedes cambiar \"ventas\" a \"clientes\" en el widget\n",
        "# y re-ejecutar la celda 2 ‚Üí Lee tabla diferente sin cambiar c√≥digo\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_text_practico)\n",
        "```\n",
        "\n",
        "#### **2. Dropdown Widget - Lista Desplegable**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DROPDOWN WIDGET\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìã DROPDOWN WIDGET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "dropdown_info = \"\"\"\n",
        "DEFINICI√ìN:\n",
        "Widget con opciones predefinidas (selecci√≥n √∫nica)\n",
        "\n",
        "SINTAXIS:\n",
        "dbutils.widgets.dropdown(name, defaultValue, choices, label)\n",
        "\n",
        "PAR√ÅMETROS:\n",
        "‚Ä¢ name: Nombre interno\n",
        "‚Ä¢ defaultValue: Valor por defecto (debe estar en choices)\n",
        "‚Ä¢ choices: Lista de opciones disponibles\n",
        "‚Ä¢ label: Etiqueta visible (opcional)\n",
        "\n",
        "CREAR:\n",
        "\"\"\"\n",
        "\n",
        "print(dropdown_info)\n",
        "\n",
        "ejemplo_dropdown = \"\"\"\n",
        "# Crear dropdown\n",
        "dbutils.widgets.dropdown(\n",
        "    \"ciudad\",\n",
        "    \"Madrid\",\n",
        "    [\"Madrid\", \"Barcelona\", \"Valencia\", \"Sevilla\"],\n",
        "    \"Selecciona Ciudad\"\n",
        ")\n",
        "\n",
        "# Obtener valor\n",
        "ciudad_seleccionada = dbutils.widgets.get(\"ciudad\")\n",
        "print(f\"Ciudad seleccionada: {ciudad_seleccionada}\")\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "APARIENCIA EN EL NOTEBOOK:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Selecciona Ciudad: [Madrid        ‚ñº] [‚ü≥]               ‚îÇ\n",
        "‚îÇ                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ\n",
        "‚îÇ                     ‚îÇ Madrid       ‚îÇ ‚Üê Selected        ‚îÇ\n",
        "‚îÇ                     ‚îÇ Barcelona    ‚îÇ                   ‚îÇ\n",
        "‚îÇ                     ‚îÇ Valencia     ‚îÇ                   ‚îÇ\n",
        "‚îÇ                     ‚îÇ Sevilla      ‚îÇ                   ‚îÇ\n",
        "‚îÇ                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "CARACTER√çSTICAS:\n",
        "‚Ä¢ Selecci√≥n √∫nica\n",
        "‚Ä¢ Opciones fijas y controladas\n",
        "‚Ä¢ Previene errores de typo\n",
        "‚Ä¢ UI clara y profesional\n",
        "\n",
        "CASOS DE USO:\n",
        "‚Ä¢ Seleccionar entorno (dev/staging/prod)\n",
        "‚Ä¢ Elegir regi√≥n/pa√≠s\n",
        "‚Ä¢ Seleccionar tipo de an√°lisis\n",
        "‚Ä¢ Elegir per√≠odo (d√≠a/semana/mes)\n",
        "‚Ä¢ Seleccionar categor√≠a\n",
        "\n",
        "EJEMPLO PR√ÅCTICO:\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_dropdown)\n",
        "\n",
        "ejemplo_dropdown_practico = \"\"\"\n",
        "# Celda 1 - Crear widget de per√≠odo\n",
        "dbutils.widgets.dropdown(\n",
        "    \"periodo\",\n",
        "    \"mes\",\n",
        "    [\"dia\", \"semana\", \"mes\", \"a√±o\"],\n",
        "    \"Per√≠odo de An√°lisis\"\n",
        ")\n",
        "\n",
        "# Celda 2 - L√≥gica basada en selecci√≥n\n",
        "from pyspark.sql.functions import current_date, date_sub\n",
        "\n",
        "periodo = dbutils.widgets.get(\"periodo\")\n",
        "\n",
        "# Calcular fecha de inicio seg√∫n per√≠odo\n",
        "if periodo == \"dia\":\n",
        "    fecha_inicio = date_sub(current_date(), 1)\n",
        "    label = \"√∫ltimo d√≠a\"\n",
        "elif periodo == \"semana\":\n",
        "    fecha_inicio = date_sub(current_date(), 7)\n",
        "    label = \"√∫ltima semana\"\n",
        "elif periodo == \"mes\":\n",
        "    fecha_inicio = date_sub(current_date(), 30)\n",
        "    label = \"√∫ltimo mes\"\n",
        "else:  # a√±o\n",
        "    fecha_inicio = date_sub(current_date(), 365)\n",
        "    label = \"√∫ltimo a√±o\"\n",
        "\n",
        "print(f\"üìÖ Analizando datos del {label}\")\n",
        "\n",
        "# Filtrar datos\n",
        "df = spark.table(\"ventas\")\n",
        "df_filtrado = df.filter(df.fecha >= fecha_inicio)\n",
        "df_filtrado.show()\n",
        "\n",
        "# Cambiar dropdown y re-ejecutar ‚Üí Diferentes per√≠odos autom√°ticamente\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_dropdown_practico)\n",
        "```\n",
        "\n",
        "#### **3. Combobox Widget - Dropdown con Texto Libre**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "COMBOBOX WIDGET\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîΩ COMBOBOX WIDGET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "combobox_info = \"\"\"\n",
        "DEFINICI√ìN:\n",
        "Combinaci√≥n de dropdown + text input\n",
        "Puedes seleccionar de lista O escribir valor libre\n",
        "\n",
        "SINTAXIS:\n",
        "dbutils.widgets.combobox(name, defaultValue, choices, label)\n",
        "\n",
        "PAR√ÅMETROS:\n",
        "Mismos que dropdown\n",
        "\n",
        "DIFERENCIA CON DROPDOWN:\n",
        "‚Ä¢ Dropdown: SOLO opciones predefinidas\n",
        "‚Ä¢ Combobox: Opciones sugeridas + texto libre\n",
        "\n",
        "CREAR:\n",
        "\"\"\"\n",
        "\n",
        "print(combobox_info)\n",
        "\n",
        "ejemplo_combobox = \"\"\"\n",
        "# Crear combobox\n",
        "dbutils.widgets.combobox(\n",
        "    \"producto\",\n",
        "    \"Laptop\",\n",
        "    [\"Laptop\", \"Mouse\", \"Teclado\", \"Monitor\"],\n",
        "    \"Producto\"\n",
        ")\n",
        "\n",
        "# Usuario puede:\n",
        "# 1. Seleccionar de la lista: \"Laptop\", \"Mouse\", etc.\n",
        "# 2. O escribir cualquier texto: \"Tablet\", \"Impresora\", etc.\n",
        "\n",
        "producto = dbutils.widgets.get(\"producto\")\n",
        "print(f\"Producto: {producto}\")\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "APARIENCIA:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Producto: [Laptop        ‚ñº] [‚ü≥]                        ‚îÇ\n",
        "‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îÇ\n",
        "‚îÇ            ‚îÇ Laptop       ‚îÇ ‚Üê Opciones sugeridas       ‚îÇ\n",
        "‚îÇ            ‚îÇ Mouse        ‚îÇ                            ‚îÇ\n",
        "‚îÇ            ‚îÇ Teclado      ‚îÇ                            ‚îÇ\n",
        "‚îÇ            ‚îÇ Monitor      ‚îÇ                            ‚îÇ\n",
        "‚îÇ            ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                            ‚îÇ\n",
        "‚îÇ            ‚îÇ [Escribir aqu√≠...] ‚Üê O texto libre        ‚îÇ\n",
        "‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "CASOS DE USO:\n",
        "‚Ä¢ Lista de opciones comunes + otros posibles\n",
        "‚Ä¢ Sugerencias sin restricci√≥n\n",
        "‚Ä¢ Facilitar input pero permitir flexibilidad\n",
        "\n",
        "EJEMPLO PR√ÅCTICO:\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_combobox)\n",
        "\n",
        "ejemplo_combobox_practico = \"\"\"\n",
        "# Celda 1 - Combobox con categor√≠as comunes\n",
        "dbutils.widgets.combobox(\n",
        "    \"categoria\",\n",
        "    \"Electr√≥nica\",\n",
        "    [\"Electr√≥nica\", \"Ropa\", \"Alimentos\", \"Hogar\"],\n",
        "    \"Categor√≠a\"\n",
        ")\n",
        "\n",
        "# Celda 2 - Usar valor (validar si es necesario)\n",
        "categoria = dbutils.widgets.get(\"categoria\")\n",
        "\n",
        "# Buscar en datos\n",
        "df = spark.table(\"productos\")\n",
        "\n",
        "# Filtrar por categor√≠a\n",
        "df_categoria = df.filter(df.categoria == categoria)\n",
        "\n",
        "if df_categoria.count() > 0:\n",
        "    print(f\"‚úÖ Categor√≠a '{categoria}': {df_categoria.count()} productos\")\n",
        "    df_categoria.show(5)\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è No se encontraron productos en '{categoria}'\")\n",
        "    print(\"Categor√≠as disponibles:\")\n",
        "    df.select(\"categoria\").distinct().show()\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_combobox_practico)\n",
        "```\n",
        "\n",
        "#### **4. Multiselect Widget - Selecci√≥n M√∫ltiple**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "MULTISELECT WIDGET\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n‚òëÔ∏è MULTISELECT WIDGET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "multiselect_info = \"\"\"\n",
        "DEFINICI√ìN:\n",
        "Permite seleccionar M√öLTIPLES opciones de una lista\n",
        "\n",
        "SINTAXIS:\n",
        "dbutils.widgets.multiselect(name, defaultValue, choices, label)\n",
        "\n",
        "PAR√ÅMETROS:\n",
        "‚Ä¢ defaultValue: Valor inicial (debe estar en choices)\n",
        "‚Ä¢ choices: Lista de opciones\n",
        "‚Ä¢ M√∫ltiples selecciones separadas por coma en el valor retornado\n",
        "\n",
        "CREAR:\n",
        "\"\"\"\n",
        "\n",
        "print(multiselect_info)\n",
        "\n",
        "ejemplo_multiselect = \"\"\"\n",
        "# Crear multiselect\n",
        "dbutils.widgets.multiselect(\n",
        "    \"ciudades\",\n",
        "    \"Madrid\",\n",
        "    [\"Madrid\", \"Barcelona\", \"Valencia\", \"Sevilla\", \"Bilbao\"],\n",
        "    \"Ciudades a Analizar\"\n",
        ")\n",
        "\n",
        "# Obtener valores (string separado por comas)\n",
        "ciudades_str = dbutils.widgets.get(\"ciudades\")\n",
        "print(f\"String raw: {ciudades_str}\")\n",
        "\n",
        "# Convertir a lista\n",
        "ciudades_lista = [c.strip() for c in ciudades_str.split(\",\")]\n",
        "print(f\"Lista: {ciudades_lista}\")\n",
        "\n",
        "# Output si seleccionas Madrid y Barcelona:\n",
        "# String raw: Madrid,Barcelona\n",
        "# Lista: ['Madrid', 'Barcelona']\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "APARIENCIA:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Ciudades: [Madrid, Barcelona    ‚ñº] [‚ü≥]                 ‚îÇ\n",
        "‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ\n",
        "‚îÇ            ‚îÇ ‚òë Madrid             ‚îÇ ‚Üê Checked          ‚îÇ\n",
        "‚îÇ            ‚îÇ ‚òë Barcelona          ‚îÇ ‚Üê Checked          ‚îÇ\n",
        "‚îÇ            ‚îÇ ‚òê Valencia           ‚îÇ                    ‚îÇ\n",
        "‚îÇ            ‚îÇ ‚òê Sevilla            ‚îÇ                    ‚îÇ\n",
        "‚îÇ            ‚îÇ ‚òê Bilbao             ‚îÇ                    ‚îÇ\n",
        "‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "CARACTER√çSTICAS:\n",
        "‚Ä¢ M√∫ltiples selecciones\n",
        "‚Ä¢ Valor retornado: string con comas\n",
        "‚Ä¢ Necesitas parsear a lista\n",
        "\n",
        "CASOS DE USO:\n",
        "‚Ä¢ Seleccionar m√∫ltiples regiones\n",
        "‚Ä¢ Elegir varios productos\n",
        "‚Ä¢ Filtrar por m√∫ltiples categor√≠as\n",
        "‚Ä¢ Seleccionar dimensiones de an√°lisis\n",
        "\n",
        "EJEMPLO PR√ÅCTICO:\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_multiselect)\n",
        "\n",
        "ejemplo_multiselect_practico = \"\"\"\n",
        "# Celda 1 - Crear multiselect\n",
        "dbutils.widgets.multiselect(\n",
        "    \"categorias\",\n",
        "    \"Electr√≥nica\",\n",
        "    [\"Electr√≥nica\", \"Ropa\", \"Alimentos\", \"Hogar\", \"Deportes\"],\n",
        "    \"Categor√≠as a Incluir\"\n",
        ")\n",
        "\n",
        "# Celda 2 - Usar m√∫ltiples selecciones\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Obtener y parsear\n",
        "categorias_str = dbutils.widgets.get(\"categorias\")\n",
        "categorias_lista = [c.strip() for c in categorias_str.split(\",\")]\n",
        "\n",
        "print(f\"üìä Analizando categor√≠as: {', '.join(categorias_lista)}\")\n",
        "\n",
        "# Filtrar DataFrame por m√∫ltiples categor√≠as\n",
        "df = spark.table(\"productos\")\n",
        "df_filtrado = df.filter(col(\"categoria\").isin(categorias_lista))\n",
        "\n",
        "print(f\"‚úÖ {df_filtrado.count()} productos encontrados\")\n",
        "\n",
        "# An√°lisis por categor√≠a\n",
        "df_filtrado.groupBy(\"categoria\").count().show()\n",
        "\n",
        "# Si seleccionas \"Electr√≥nica\" y \"Ropa\":\n",
        "# Output:\n",
        "# üìä Analizando categor√≠as: Electr√≥nica, Ropa\n",
        "# ‚úÖ 150 productos encontrados\n",
        "# +------------+-----+\n",
        "# |  categoria |count|\n",
        "# +------------+-----+\n",
        "# |Electr√≥nica |   85|\n",
        "# |       Ropa |   65|\n",
        "# +------------+-----+\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_multiselect_practico)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 2: Operaciones con Widgets**\n",
        "\n",
        "#### **Crear, Obtener, Eliminar**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "OPERACIONES CON WIDGETS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîß OPERACIONES CON WIDGETS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "operaciones_widgets = \"\"\"\n",
        "API COMPLETA: dbutils.widgets\n",
        "\n",
        "1Ô∏è‚É£ CREAR WIDGETS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "dbutils.widgets.text(name, defaultValue, label)\n",
        "dbutils.widgets.dropdown(name, defaultValue, choices, label)\n",
        "dbutils.widgets.combobox(name, defaultValue, choices, label)\n",
        "dbutils.widgets.multiselect(name, defaultValue, choices, label)\n",
        "\n",
        "‚ö†Ô∏è Si el widget ya existe, se sobrescribe\n",
        "\n",
        "2Ô∏è‚É£ OBTENER VALOR\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "value = dbutils.widgets.get(name)\n",
        "\n",
        "‚Ä¢ Retorna string siempre\n",
        "‚Ä¢ Si multiselect: string con valores separados por coma\n",
        "‚Ä¢ Si no existe el widget: Error\n",
        "\n",
        "Ejemplo:\n",
        "nombre = dbutils.widgets.get(\"nombre_usuario\")\n",
        "\n",
        "3Ô∏è‚É£ OBTENER TODOS LOS WIDGETS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "all_widgets = dbutils.widgets.getAll()\n",
        "\n",
        "‚Ä¢ Retorna diccionario {name: value}\n",
        "\n",
        "Ejemplo:\n",
        "widgets = dbutils.widgets.getAll()\n",
        "print(widgets)\n",
        "# {'nombre_usuario': 'Juan', 'ciudad': 'Madrid'}\n",
        "\n",
        "4Ô∏è‚É£ ELIMINAR UN WIDGET\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "dbutils.widgets.remove(name)\n",
        "\n",
        "Ejemplo:\n",
        "dbutils.widgets.remove(\"nombre_usuario\")\n",
        "\n",
        "5Ô∏è‚É£ ELIMINAR TODOS LOS WIDGETS\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "dbutils.widgets.removeAll()\n",
        "\n",
        "‚ö†Ô∏è Elimina TODOS los widgets del notebook\n",
        "\n",
        "6Ô∏è‚É£ VERIFICAR SI EXISTE\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "No hay m√©todo directo, pero puedes:\n",
        "\n",
        "try:\n",
        "    value = dbutils.widgets.get(\"mi_widget\")\n",
        "    print(\"Widget existe\")\n",
        "except:\n",
        "    print(\"Widget no existe\")\n",
        "\n",
        "# O con getAll()\n",
        "if \"mi_widget\" in dbutils.widgets.getAll():\n",
        "    print(\"Widget existe\")\n",
        "\n",
        "EJEMPLO COMPLETO:\n",
        "\"\"\"\n",
        "\n",
        "print(operaciones_widgets)\n",
        "\n",
        "ejemplo_operaciones = \"\"\"\n",
        "# Celda 1 - Setup de widgets\n",
        "# Crear m√∫ltiples widgets\n",
        "dbutils.widgets.text(\"usuario\", \"admin\", \"Usuario\")\n",
        "dbutils.widgets.dropdown(\"env\", \"dev\", [\"dev\", \"staging\", \"prod\"], \"Entorno\")\n",
        "dbutils.widgets.multiselect(\"modulos\", \"ventas\", [\"ventas\", \"compras\", \"inventario\"], \"M√≥dulos\")\n",
        "\n",
        "print(\"‚úÖ Widgets creados\")\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 2 - Obtener valores\n",
        "# Obtener todos\n",
        "all_params = dbutils.widgets.getAll()\n",
        "print(\"üìã Todos los par√°metros:\")\n",
        "for name, value in all_params.items():\n",
        "    print(f\"  {name}: {value}\")\n",
        "\n",
        "# Obtener individuales\n",
        "usuario = dbutils.widgets.get(\"usuario\")\n",
        "env = dbutils.widgets.get(\"env\")\n",
        "modulos = dbutils.widgets.get(\"modulos\")\n",
        "\n",
        "print(f\"\\nüîß Configuraci√≥n actual:\")\n",
        "print(f\"  Usuario: {usuario}\")\n",
        "print(f\"  Entorno: {env}\")\n",
        "print(f\"  M√≥dulos: {modulos}\")\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 3 - Usar en l√≥gica\n",
        "env = dbutils.widgets.get(\"env\")\n",
        "\n",
        "# L√≥gica condicional basada en widget\n",
        "if env == \"prod\":\n",
        "    database = \"prod_db\"\n",
        "    log_level = \"ERROR\"\n",
        "elif env == \"staging\":\n",
        "    database = \"staging_db\"\n",
        "    log_level = \"WARN\"\n",
        "else:  # dev\n",
        "    database = \"dev_db\"\n",
        "    log_level = \"DEBUG\"\n",
        "\n",
        "print(f\"üóÑÔ∏è  Base de datos: {database}\")\n",
        "print(f\"üìù Log level: {log_level}\")\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 4 - Limpiar (al final)\n",
        "# dbutils.widgets.removeAll()\n",
        "# print(\"üóëÔ∏è Widgets eliminados\")\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_operaciones)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 3: Validaci√≥n y Manejo de Errores**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "VALIDACI√ìN DE WIDGETS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n‚úÖ VALIDACI√ìN Y MANEJO DE ERRORES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "validacion_info = \"\"\"\n",
        "Los widgets NO tienen validaci√≥n integrada.\n",
        "Debes validar los valores manualmente.\n",
        "\n",
        "ESTRATEGIAS DE VALIDACI√ìN:\n",
        "\n",
        "1Ô∏è‚É£ VALIDACI√ìN DE EXISTENCIA\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def get_widget_safe(name, default=None):\n",
        "    '''Obtener widget con valor por defecto si no existe'''\n",
        "    try:\n",
        "        return dbutils.widgets.get(name)\n",
        "    except:\n",
        "        return default\n",
        "\n",
        "# Uso\n",
        "usuario = get_widget_safe(\"usuario\", \"guest\")\n",
        "\n",
        "2Ô∏è‚É£ VALIDACI√ìN DE FORMATO\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Fechas\n",
        "fecha_str = dbutils.widgets.get(\"fecha\")\n",
        "try:\n",
        "    from datetime import datetime\n",
        "    fecha = datetime.strptime(fecha_str, \"%Y-%m-%d\")\n",
        "    print(f\"‚úÖ Fecha v√°lida: {fecha}\")\n",
        "except ValueError:\n",
        "    print(f\"‚ùå Fecha inv√°lida: {fecha_str}\")\n",
        "    print(\"   Formato esperado: YYYY-MM-DD\")\n",
        "    dbutils.notebook.exit(\"Error: Fecha inv√°lida\")\n",
        "\n",
        "# N√∫meros\n",
        "cantidad_str = dbutils.widgets.get(\"cantidad\")\n",
        "try:\n",
        "    cantidad = int(cantidad_str)\n",
        "    if cantidad <= 0:\n",
        "        raise ValueError(\"Cantidad debe ser positiva\")\n",
        "    print(f\"‚úÖ Cantidad v√°lida: {cantidad}\")\n",
        "except ValueError as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    dbutils.notebook.exit(f\"Error: {e}\")\n",
        "\n",
        "3Ô∏è‚É£ VALIDACI√ìN DE RANGO\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "limite = int(dbutils.widgets.get(\"limite\"))\n",
        "\n",
        "if not (1 <= limite <= 1000):\n",
        "    print(\"‚ùå L√≠mite debe estar entre 1 y 1000\")\n",
        "    dbutils.notebook.exit(\"Error: L√≠mite fuera de rango\")\n",
        "else:\n",
        "    print(f\"‚úÖ L√≠mite v√°lido: {limite}\")\n",
        "\n",
        "4Ô∏è‚É£ VALIDACI√ìN DE OPCIONES\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Para text/combobox donde usuario puede escribir libremente\n",
        "categoria = dbutils.widgets.get(\"categoria\")\n",
        "categorias_validas = [\"A\", \"B\", \"C\", \"D\"]\n",
        "\n",
        "if categoria not in categorias_validas:\n",
        "    print(f\"‚ùå Categor√≠a '{categoria}' no v√°lida\")\n",
        "    print(f\"   Opciones v√°lidas: {', '.join(categorias_validas)}\")\n",
        "    dbutils.notebook.exit(\"Error: Categor√≠a inv√°lida\")\n",
        "\n",
        "5Ô∏è‚É£ VALIDACI√ìN DE MULTISELECT\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "regiones_str = dbutils.widgets.get(\"regiones\")\n",
        "regiones = [r.strip() for r in regiones_str.split(\",\")]\n",
        "\n",
        "# Verificar que hay al menos una selecci√≥n\n",
        "if not regiones or regiones == ['']:\n",
        "    print(\"‚ùå Debes seleccionar al menos una regi√≥n\")\n",
        "    dbutils.notebook.exit(\"Error: Sin regiones seleccionadas\")\n",
        "\n",
        "# Verificar que todas son v√°lidas\n",
        "regiones_validas = [\"Norte\", \"Sur\", \"Este\", \"Oeste\"]\n",
        "regiones_invalidas = [r for r in regiones if r not in regiones_validas]\n",
        "\n",
        "if regiones_invalidas:\n",
        "    print(f\"‚ùå Regiones inv√°lidas: {', '.join(regiones_invalidas)}\")\n",
        "    dbutils.notebook.exit(\"Error: Regiones inv√°lidas\")\n",
        "\n",
        "print(f\"‚úÖ Regiones v√°lidas: {', '.join(regiones)}\")\n",
        "\n",
        "EJEMPLO COMPLETO CON VALIDACI√ìN:\n",
        "\"\"\"\n",
        "\n",
        "print(validacion_info)\n",
        "\n",
        "ejemplo_validacion = \"\"\"\n",
        "# Celda 1 - Crear widgets\n",
        "dbutils.widgets.text(\"fecha_inicio\", \"2025-01-01\", \"Fecha Inicio (YYYY-MM-DD)\")\n",
        "dbutils.widgets.text(\"fecha_fin\", \"2025-12-31\", \"Fecha Fin (YYYY-MM-DD)\")\n",
        "dbutils.widgets.dropdown(\"tipo\", \"ventas\", [\"ventas\", \"compras\", \"inventario\"], \"Tipo An√°lisis\")\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 2 - Validaci√≥n completa\n",
        "from datetime import datetime\n",
        "\n",
        "def validar_parametros():\n",
        "    '''Valida todos los par√°metros del widget'''\n",
        "    errores = []\n",
        "    \n",
        "    # Validar fecha inicio\n",
        "    try:\n",
        "        fecha_inicio_str = dbutils.widgets.get(\"fecha_inicio\")\n",
        "        fecha_inicio = datetime.strptime(fecha_inicio_str, \"%Y-%m-%d\")\n",
        "    except ValueError:\n",
        "        errores.append(f\"Fecha inicio inv√°lida: {fecha_inicio_str}\")\n",
        "        fecha_inicio = None\n",
        "    \n",
        "    # Validar fecha fin\n",
        "    try:\n",
        "        fecha_fin_str = dbutils.widgets.get(\"fecha_fin\")\n",
        "        fecha_fin = datetime.strptime(fecha_fin_str, \"%Y-%m-%d\")\n",
        "    except ValueError:\n",
        "        errores.append(f\"Fecha fin inv√°lida: {fecha_fin_str}\")\n",
        "        fecha_fin = None\n",
        "    \n",
        "    # Validar que fecha fin > fecha inicio\n",
        "    if fecha_inicio and fecha_fin and fecha_fin <= fecha_inicio:\n",
        "        errores.append(\"Fecha fin debe ser posterior a fecha inicio\")\n",
        "    \n",
        "    # Validar tipo\n",
        "    tipo = dbutils.widgets.get(\"tipo\")\n",
        "    tipos_validos = [\"ventas\", \"compras\", \"inventario\"]\n",
        "    if tipo not in tipos_validos:\n",
        "        errores.append(f\"Tipo inv√°lido: {tipo}\")\n",
        "    \n",
        "    # Reportar errores\n",
        "    if errores:\n",
        "        print(\"‚ùå ERRORES DE VALIDACI√ìN:\")\n",
        "        for error in errores:\n",
        "            print(f\"   ‚Ä¢ {error}\")\n",
        "        dbutils.notebook.exit(\"Validaci√≥n fallida\")\n",
        "    else:\n",
        "        print(\"‚úÖ Todos los par√°metros son v√°lidos\")\n",
        "        return {\n",
        "            \"fecha_inicio\": fecha_inicio,\n",
        "            \"fecha_fin\": fecha_fin,\n",
        "            \"tipo\": tipo\n",
        "        }\n",
        "\n",
        "# Ejecutar validaci√≥n\n",
        "params = validar_parametros()\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 3 - Usar par√°metros validados\n",
        "if params:\n",
        "    print(f\"üìä An√°lisis de {params['tipo']}\")\n",
        "    print(f\"üìÖ Desde: {params['fecha_inicio'].strftime('%Y-%m-%d')}\")\n",
        "    print(f\"üìÖ Hasta: {params['fecha_fin'].strftime('%Y-%m-%d')}\")\n",
        "    \n",
        "    # Aqu√≠ va tu l√≥gica de an√°lisis\n",
        "    # Los par√°metros est√°n garantizados como v√°lidos\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_validacion)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 4: Widgets en Workflows y Jobs**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "WIDGETS EN WORKFLOWS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n‚öôÔ∏è WIDGETS EN WORKFLOWS Y JOBS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "workflows_info = \"\"\"\n",
        "Los widgets son ESENCIALES para Jobs programados\n",
        "\n",
        "FLUJO T√çPICO:\n",
        "\n",
        "1. Desarrollas notebook con widgets\n",
        "2. Lo programas como Job\n",
        "3. Job pasa valores a los widgets\n",
        "4. Notebook ejecuta con esos par√°metros\n",
        "\n",
        "CONFIGURAR JOB CON PAR√ÅMETROS:\n",
        "\n",
        "En Databricks UI (Workflows):\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Job Configuration                                      ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Notebook: /Users/email/mi_notebook                     ‚îÇ\n",
        "‚îÇ                                                        ‚îÇ\n",
        "‚îÇ Parameters:                                            ‚îÇ\n",
        "‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
        "‚îÇ ‚îÇ Key                Value                           ‚îÇ ‚îÇ\n",
        "‚îÇ ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ ‚îÇ\n",
        "‚îÇ ‚îÇ fecha_inicio       2025-01-01                      ‚îÇ ‚îÇ\n",
        "‚îÇ ‚îÇ fecha_fin          2025-01-31                      ‚îÇ ‚îÇ\n",
        "‚îÇ ‚îÇ tipo               ventas                          ‚îÇ ‚îÇ\n",
        "‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "NOTEBOOK RECIBE PAR√ÅMETROS:\n",
        "\n",
        "# Tu notebook\n",
        "dbutils.widgets.text(\"fecha_inicio\", \"2025-01-01\")\n",
        "dbutils.widgets.text(\"fecha_fin\", \"2025-12-31\")\n",
        "dbutils.widgets.text(\"tipo\", \"ventas\")\n",
        "\n",
        "# Cuando corre como Job:\n",
        "# - Los valores del Job sobrescriben los defaults\n",
        "# - El notebook ejecuta con esos valores\n",
        "\n",
        "VENTAJAS:\n",
        "\n",
        "‚úÖ Mismo notebook para diferentes per√≠odos\n",
        "‚úÖ Reutilizaci√≥n de c√≥digo\n",
        "‚úÖ No duplicar notebooks\n",
        "‚úÖ Parametrizaci√≥n flexible\n",
        "\n",
        "EJEMPLO PR√ÅCTICO:\n",
        "\"\"\"\n",
        "\n",
        "print(workflows_info)\n",
        "\n",
        "ejemplo_job = \"\"\"\n",
        "CASO: Reporte de ventas mensual automatizado\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "NOTEBOOK: reporte_ventas\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Celda 1 - Widgets\n",
        "dbutils.widgets.text(\"mes\", \"2025-01\", \"Mes (YYYY-MM)\")\n",
        "dbutils.widgets.text(\"email_destino\", \"manager@empresa.com\", \"Email\")\n",
        "\n",
        "---\n",
        "\n",
        "# Celda 2 - L√≥gica\n",
        "mes = dbutils.widgets.get(\"mes\")\n",
        "email = dbutils.widgets.get(\"email_destino\")\n",
        "\n",
        "print(f\"üìä Generando reporte de {mes}\")\n",
        "\n",
        "# Generar reporte\n",
        "df = spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        producto,\n",
        "        SUM(cantidad) as unidades,\n",
        "        SUM(monto) as ingresos\n",
        "    FROM ventas\n",
        "    WHERE DATE_FORMAT(fecha, 'yyyy-MM') = '{mes}'\n",
        "    GROUP BY producto\n",
        "    ORDER BY ingresos DESC\n",
        "\"\"\")\n",
        "\n",
        "# Guardar\n",
        "output_path = f\"/FileStore/reportes/ventas_{mes}.csv\"\n",
        "df.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "print(f\"‚úÖ Reporte guardado: {output_path}\")\n",
        "\n",
        "# Enviar email (simulado)\n",
        "print(f\"üìß Enviando reporte a {email}\")\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "JOB CONFIGURATION\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Job 1: Reporte Enero\n",
        "- Notebook: reporte_ventas\n",
        "- Schedule: Primer d√≠a del mes\n",
        "- Parameters:\n",
        "  ‚Ä¢ mes: 2025-01\n",
        "  ‚Ä¢ email_destino: manager@empresa.com\n",
        "\n",
        "Job 2: Reporte Febrero\n",
        "- Notebook: reporte_ventas  ‚Üê MISMO NOTEBOOK\n",
        "- Schedule: Primer d√≠a del mes\n",
        "- Parameters:\n",
        "  ‚Ä¢ mes: 2025-02\n",
        "  ‚Ä¢ email_destino: manager@empresa.com\n",
        "\n",
        "Job 3: Reporte especial para CEO\n",
        "- Notebook: reporte_ventas  ‚Üê MISMO NOTEBOOK\n",
        "- Schedule: Manual\n",
        "- Parameters:\n",
        "  ‚Ä¢ mes: 2024-12\n",
        "  ‚Ä¢ email_destino: ceo@empresa.com\n",
        "\n",
        "‚úÖ 1 notebook ‚Üí M√∫ltiples jobs con diferentes par√°metros\n",
        "‚úÖ Mantenimiento simple (cambias 1 notebook, afecta todos los jobs)\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_job)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica Completa - Widgets Avanzados**\n",
        "\n",
        "```markdown\n",
        "üß™ EJERCICIO: DASHBOARD INTERACTIVO CON WIDGETS\n",
        "\n",
        "Vamos a crear un dashboard de an√°lisis de ventas completamente parametrizado.\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "SETUP (3 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "1. **Crear notebook**:\n",
        "   - [ ] Name: \"Practica_4_4_Dashboard_Ventas\"\n",
        "   - [ ] Default Language: Python\n",
        "   - [ ] Connect cluster\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 1: CREAR DATOS DE EJEMPLO (5 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "2. **Celda 1 - T√≠tulo (Markdown)**:\n",
        "```markdown\n",
        "# Dashboard de Ventas Interactivo\n",
        "\n",
        "Dashboard parametrizado con widgets para an√°lisis din√°mico.\n",
        "\n",
        "**Caracter√≠sticas**:\n",
        "- Filtro por per√≠odo\n",
        "- Filtro por ciudades\n",
        "- Filtro por categor√≠as\n",
        "- M√©tricas en tiempo real\n",
        "```\n",
        "\n",
        "3. **Celda 2 - Generar datos**:\n",
        "```python\n",
        "from datetime import date, timedelta\n",
        "from random import randint, choice\n",
        "\n",
        "# Generar datos de ventas\n",
        "def generar_ventas(num_dias=90):\n",
        "    '''Genera datos ficticios de ventas'''\n",
        "    productos = [\"Laptop\", \"Mouse\", \"Teclado\", \"Monitor\", \"Webcam\", \"Auriculares\"]\n",
        "    categorias = {\"Laptop\": \"Electr√≥nica\", \"Mouse\": \"Accesorios\",\n",
        "                  \"Teclado\": \"Accesorios\", \"Monitor\": \"Electr√≥nica\",\n",
        "                  \"Webcam\": \"Electr√≥nica\", \"Auriculares\": \"Accesorios\"}\n",
        "    ciudades = [\"Madrid\", \"Barcelona\", \"Valencia\", \"Sevilla\"]\n",
        "    \n",
        "    ventas = []\n",
        "    fecha_base = date(2025, 1, 1)\n",
        "    \n",
        "    for dia in range(num_dias):\n",
        "        fecha_actual = fecha_base + timedelta(days=dia)\n",
        "        # Generar 5-15 ventas por d√≠a\n",
        "        for _ in range(randint(5, 15)):\n",
        "            producto = choice(productos)\n",
        "            ventas.append((\n",
        "                fecha_actual,\n",
        "                producto,\n",
        "                categorias[producto],\n",
        "                randint(1, 5),  # cantidad\n",
        "                randint(20, 1200),  # precio\n",
        "                choice(ciudades)\n",
        "            ))\n",
        "    \n",
        "    return ventas\n",
        "\n",
        "# Generar y crear DataFrame\n",
        "datos_ventas = generar_ventas(90)\n",
        "df_ventas = spark.createDataFrame(\n",
        "    datos_ventas,\n",
        "    [\"fecha\", \"producto\", \"categoria\", \"cantidad\", \"precio\", \"ciudad\"]\n",
        ")\n",
        "\n",
        "# Calcular ingresos\n",
        "from pyspark.sql.functions import col\n",
        "df_ventas = df_ventas.withColumn(\"ingresos\", col(\"precio\") * col(\"cantidad\"))\n",
        "\n",
        "# Registrar como tabla temporal\n",
        "df_ventas.createOrReplaceTempView(\"ventas\")\n",
        "\n",
        "print(f\"‚úÖ Generadas {df_ventas.count()} ventas\")\n",
        "print(f\"üìÖ Rango: {df_ventas.agg({'fecha': 'min'}).collect()[0][0]} a {df_ventas.agg({'fecha': 'max'}).collect()[0][0]}\")\n",
        "df_ventas.show(5)\n",
        "```\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 2: CREAR WIDGETS (10 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "4. **Celda 3 - Secci√≥n t√≠tulo (Markdown)**:\n",
        "```markdown\n",
        "## Par√°metros de An√°lisis\n",
        "\n",
        "Ajusta los widgets para filtrar los datos:\n",
        "```\n",
        "\n",
        "5. **Celda 4 - Crear todos los widgets**:\n",
        "```python\n",
        "# Widget 1: Per√≠odo de an√°lisis\n",
        "dbutils.widgets.dropdown(\n",
        "    \"periodo\",\n",
        "    \"mes\",\n",
        "    [\"semana\", \"mes\", \"trimestre\", \"todo\"],\n",
        "    \"üìÖ Per√≠odo\"\n",
        ")\n",
        "\n",
        "# Widget 2: Ciudades (multiselect)\n",
        "dbutils.widgets.multiselect(\n",
        "    \"ciudades\",\n",
        "    \"Madrid\",\n",
        "    [\"Madrid\", \"Barcelona\", \"Valencia\", \"Sevilla\"],\n",
        "    \"üåç Ciudades\"\n",
        ")\n",
        "\n",
        "# Widget 3: Categor√≠as (multiselect)\n",
        "dbutils.widgets.multiselect(\n",
        "    \"categorias\",\n",
        "    \"Electr√≥nica\",\n",
        "    [\"Electr√≥nica\", \"Accesorios\"],\n",
        "    \"üì¶ Categor√≠as\"\n",
        ")\n",
        "\n",
        "# Widget 4: L√≠mite de productos top\n",
        "dbutils.widgets.text(\n",
        "    \"top_n\",\n",
        "    \"5\",\n",
        "    \"üèÜ Top N Productos\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Widgets creados\")\n",
        "print(\"\\nüí° Ajusta los widgets arriba y re-ejecuta las celdas siguientes\")\n",
        "```\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 3: VALIDACI√ìN (5 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "6. **Celda 5 - Validar par√°metros**:\n",
        "```python\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def validar_y_obtener_parametros():\n",
        "    '''Valida widgets y retorna par√°metros procesados'''\n",
        "    \n",
        "    # Per√≠odo\n",
        "    periodo = dbutils.widgets.get(\"periodo\")\n",
        "    fecha_max = datetime(2025, 3, 31).date()\n",
        "    \n",
        "    if periodo == \"semana\":\n",
        "        fecha_inicio = fecha_max - timedelta(days=7)\n",
        "    elif periodo == \"mes\":\n",
        "        fecha_inicio = fecha_max - timedelta(days=30)\n",
        "    elif periodo == \"trimestre\":\n",
        "        fecha_inicio = fecha_max - timedelta(days=90)\n",
        "    else:  # todo\n",
        "        fecha_inicio = datetime(2025, 1, 1).date()\n",
        "    \n",
        "    # Ciudades\n",
        "    ciudades_str = dbutils.widgets.get(\"ciudades\")\n",
        "    ciudades = [c.strip() for c in ciudades_str.split(\",\") if c.strip()]\n",
        "    \n",
        "    if not ciudades:\n",
        "        print(\"‚ö†Ô∏è Aviso: No hay ciudades seleccionadas, usando todas\")\n",
        "        ciudades = [\"Madrid\", \"Barcelona\", \"Valencia\", \"Sevilla\"]\n",
        "    \n",
        "    # Categor√≠as\n",
        "    categorias_str = dbutils.widgets.get(\"categorias\")\n",
        "    categorias = [c.strip() for c in categorias_str.split(\",\") if c.strip()]\n",
        "    \n",
        "    if not categorias:\n",
        "        print(\"‚ö†Ô∏è Aviso: No hay categor√≠as seleccionadas, usando todas\")\n",
        "        categorias = [\"Electr√≥nica\", \"Accesorios\"]\n",
        "    \n",
        "    # Top N\n",
        "    try:\n",
        "        top_n = int(dbutils.widgets.get(\"top_n\"))\n",
        "        if top_n <= 0:\n",
        "            top_n = 5\n",
        "            print(\"‚ö†Ô∏è Top N debe ser positivo, usando 5\")\n",
        "    except ValueError:\n",
        "        top_n = 5\n",
        "        print(\"‚ö†Ô∏è Top N inv√°lido, usando 5\")\n",
        "    \n",
        "    return {\n",
        "        \"fecha_inicio\": fecha_inicio,\n",
        "        \"fecha_fin\": fecha_max,\n",
        "        \"ciudades\": ciudades,\n",
        "        \"categorias\": categorias,\n",
        "        \"top_n\": top_n,\n",
        "        \"periodo\": periodo\n",
        "    }\n",
        "\n",
        "# Obtener par√°metros\n",
        "params = validar_y_obtener_parametros()\n",
        "\n",
        "print(\"\\nüìã PAR√ÅMETROS APLICADOS:\")\n",
        "print(f\"  üìÖ Per√≠odo: {params['periodo']}\")\n",
        "print(f\"  üìÖ Desde: {params['fecha_inicio']}\")\n",
        "print(f\"  üìÖ Hasta: {params['fecha_fin']}\")\n",
        "print(f\"  üåç Ciudades: {', '.join(params['ciudades'])}\")\n",
        "print(f\"  üì¶ Categor√≠as: {', '.join(params['categorias'])}\")\n",
        "print(f\"  üèÜ Top: {params['top_n']} productos\")\n",
        "```\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 4: AN√ÅLISIS CON PAR√ÅMETROS (15 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "7. **Celda 6 - Filtrar datos**:\n",
        "```python\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Aplicar filtros\n",
        "df_filtrado = df_ventas.filter(\n",
        "    (col(\"fecha\") >= params[\"fecha_inicio\"]) &\n",
        "    (col(\"fecha\") <= params[\"fecha_fin\"]) &\n",
        "    (col(\"ciudad\").isin(params[\"ciudades\"])) &\n",
        "    (col(\"categoria\").isin(params[\"categorias\"]))\n",
        ")\n",
        "\n",
        "total_registros = df_filtrado.count()\n",
        "print(f\"üìä Registros filtrados: {total_registros:,}\")\n",
        "\n",
        "if total_registros == 0:\n",
        "    print(\"\\n‚ö†Ô∏è No hay datos con los filtros seleccionados\")\n",
        "    dbutils.notebook.exit(\"Sin datos\")\n",
        "```\n",
        "\n",
        "8. **Celda 7 - KPIs principales**:\n",
        "```python\n",
        "from pyspark.sql.functions import sum, avg, count\n",
        "\n",
        "# Calcular KPIs\n",
        "kpis = df_filtrado.agg(\n",
        "    sum(\"ingresos\").alias(\"ingresos_totales\"),\n",
        "    count(\"*\").alias(\"num_ventas\"),\n",
        "    avg(\"ingresos\").alias(\"ticket_promedio\"),\n",
        "    count(col(\"producto\").distinct()).alias(\"productos_unicos\")\n",
        ").collect()[0]\n",
        "\n",
        "print(\"üí∞ M√âTRICAS PRINCIPALES\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üíµ Ingresos Totales:    ${kpis['ingresos_totales']:,.2f}\")\n",
        "print(f\"üõí N√∫mero de Ventas:    {kpis['num_ventas']:,}\")\n",
        "print(f\"üé´ Ticket Promedio:     ${kpis['ticket_promedio']:.2f}\")\n",
        "print(f\"üì¶ Productos √önicos:    {kpis['productos_unicos']}\")\n",
        "print(\"=\"*60)\n",
        "```\n",
        "\n",
        "9. **Celda 8 - Top productos**:\n",
        "```python\n",
        "from pyspark.sql.functions import sum, count, desc\n",
        "\n",
        "# Top N productos\n",
        "df_top = df_filtrado.groupBy(\"producto\").agg(\n",
        "    sum(\"ingresos\").alias(\"ingresos\"),\n",
        "    sum(\"cantidad\").alias(\"unidades\"),\n",
        "    count(\"*\").alias(\"transacciones\")\n",
        ").orderBy(desc(\"ingresos\")).limit(params['top_n'])\n",
        "\n",
        "print(f\"\\nüèÜ TOP {params['top_n']} PRODUCTOS POR INGRESOS\")\n",
        "print(\"=\"*60)\n",
        "df_top.show(truncate=False)\n",
        "\n",
        "# Visualizaci√≥n\n",
        "display(df_top)\n",
        "```\n",
        "\n",
        "10. **Celda 9 - An√°lisis por ciudad**:\n",
        "```python\n",
        "# Ventas por ciudad\n",
        "df_ciudad = df_filtrado.groupBy(\"ciudad\").agg(\n",
        "    sum(\"ingresos\").alias(\"ingresos\"),\n",
        "    count(\"*\").alias(\"ventas\")\n",
        ").orderBy(desc(\"ingresos\"))\n",
        "\n",
        "print(\"\\nüåç VENTAS POR CIUDAD\")\n",
        "print(\"=\"*60)\n",
        "df_ciudad.show()\n",
        "\n",
        "display(df_ciudad)\n",
        "```\n",
        "\n",
        "11. **Celda 10 - An√°lisis temporal**:\n",
        "```python\n",
        "# Ventas por d√≠a\n",
        "df_temporal = df_filtrado.groupBy(\"fecha\").agg(\n",
        "    sum(\"ingresos\").alias(\"ingresos\"),\n",
        "    count(\"*\").alias(\"ventas\")\n",
        ").orderBy(\"fecha\")\n",
        "\n",
        "print(f\"\\nüìà EVOLUCI√ìN TEMPORAL ({params['periodo']})\")\n",
        "print(\"=\"*60)\n",
        "df_temporal.show(10)\n",
        "\n",
        "display(df_temporal)\n",
        "```\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "PARTE 5: INTERACTIVIDAD (5 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "12. **Celda 11 - Instrucciones (Markdown)**:\n",
        "```markdown\n",
        "## üéÆ Interactividad\n",
        "\n",
        "**Prueba cambiar los widgets arriba:**\n",
        "\n",
        "1. Cambia per√≠odo a \"semana\" ‚Üí Re-ejecuta desde Celda 5\n",
        "2. Selecciona solo \"Barcelona\" ‚Üí Re-ejecuta\n",
        "3. Cambia Top N a \"3\" ‚Üí Re-ejecuta Celda 8\n",
        "4. Combina diferentes filtros\n",
        "\n",
        "üí° **Tip**: Usa \"Run All\" para re-ejecutar todo con nuevos par√°metros\n",
        "```\n",
        "\n",
        "13. **Celda 12 - Limpiar (opcional)**:\n",
        "```python\n",
        "# Descomentar para eliminar widgets\n",
        "# dbutils.widgets.removeAll()\n",
        "# print(\"üóëÔ∏è Widgets eliminados\")\n",
        "```\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "EXPERIMENTOS (5 minutos)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "14. **Experimentar con widgets**:\n",
        "    - [ ] Cambiar per√≠odo a \"semana\"\n",
        "    - [ ] Run All\n",
        "    - [ ] Observar cambios en m√©tricas\n",
        "    \n",
        "    - [ ] Seleccionar solo \"Madrid\" y \"Barcelona\"\n",
        "    - [ ] Run All\n",
        "    - [ ] Comparar resultados\n",
        "    \n",
        "    - [ ] Cambiar Top N a \"3\"\n",
        "    - [ ] Re-ejecutar solo Celda 8\n",
        "    - [ ] Ver top 3 en lugar de top 5\n",
        "\n",
        "**TIEMPO TOTAL**: ~45 minutos\n",
        "\n",
        "‚úÖ ¬°Has creado un dashboard completamente parametrizado!\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 4.4 - Desaf√≠o Final**\n",
        "\n",
        "```markdown\n",
        "### Desaf√≠o: Sistema de Reportes Automatizable\n",
        "\n",
        "**Objetivo**: Crear un notebook que pueda ejecutarse como Job con diferentes par√°metros\n",
        "\n",
        "**Escenario**:\n",
        "Tu empresa necesita reportes automatizados que puedan:\n",
        "1. Ejecutarse diariamente con fecha din√°mica\n",
        "2. Enviarse a diferentes destinatarios\n",
        "3. Filtrar por diferentes criterios\n",
        "4. Generar output en diferentes formatos\n",
        "\n",
        "**Requisitos**:\n",
        "\n",
        "1. **Widgets obligatorios**:\n",
        "   - Fecha inicio (text)\n",
        "   - Fecha fin (text)\n",
        "   - Tipo de reporte (dropdown: \"completo\", \"resumen\", \"ejecutivo\")\n",
        "   - Formato salida (dropdown: \"CSV\", \"Parquet\", \"Delta\")\n",
        "   - Email destinatario (text)\n",
        "\n",
        "2. **Validaci√≥n robusta**:\n",
        "   - Fechas en formato correcto\n",
        "   - Fecha fin > fecha inicio\n",
        "   - Tipo de reporte v√°lido\n",
        "   - Email con formato v√°lido\n",
        "\n",
        "3. **L√≥gica condicional**:\n",
        "   - Reporte \"completo\": Todos los datos\n",
        "   - Reporte \"resumen\": Agregaciones por d√≠a\n",
        "   - Reporte \"ejecutivo\": Solo top 10 y KPIs\n",
        "\n",
        "4. **Output**:\n",
        "   - Generar archivo en formato seleccionado\n",
        "   - Guardar en /FileStore/reportes/\n",
        "   - Nombre con timestamp\n",
        "   - Log de ejecuci√≥n\n",
        "\n",
        "5. **Documentaci√≥n**:\n",
        "   - Instrucciones para uso manual\n",
        "   - Instrucciones para configurar como Job\n",
        "   - Ejemplos de par√°metros\n",
        "\n",
        "**Bonus**:\n",
        "- A√±adir widget de \"modo debug\" (True/False)\n",
        "- Si debug=True, mostrar info adicional\n",
        "- Crear funci√≥n de env√≠o de email (simulado)\n",
        "- Generar m√∫ltiples outputs simult√°neamente\n",
        "\n",
        "**Entregable**:\n",
        "Notebook funcionando que puede:\n",
        "1. Ejecutarse manualmente cambiando widgets\n",
        "2. Ejecutarse como Job con par√°metros\n",
        "3. Manejar errores gracefully\n",
        "\n",
        "**Tiempo estimado**: 60-90 minutos\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **4 tipos de widgets**: text, dropdown, combobox, multiselect\n",
        "2. **dbutils.widgets.get()** retorna SIEMPRE string\n",
        "3. **Widgets NO tienen validaci√≥n** autom√°tica - debes hacerla t√∫\n",
        "4. **Multiselect retorna** string separado por comas\n",
        "5. **Widgets son clave** para Jobs y workflows\n",
        "6. **dbutils.widgets.removeAll()** limpia todos los widgets\n",
        "7. **Widgets aparecen** en la parte superior del notebook\n",
        "8. **Valores de Jobs** sobrescriben defaults de widgets\n",
        "9. **getAll()** retorna diccionario con todos los widgets\n",
        "10. **Validaci√≥n es cr√≠tica** - siempre valida inputs del usuario\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **Usa dropdown sobre text cuando sea posible**\n",
        "```markdown\n",
        "Previene errores de typo\n",
        "UX m√°s clara\n",
        "Menos validaci√≥n necesaria\n",
        "```\n",
        "\n",
        "‚úÖ **Siempre proporciona defaults sensatos**\n",
        "```markdown\n",
        "Valores que funcionen inmediatamente\n",
        "Ejemplos del formato esperado\n",
        "Facilita testing r√°pido\n",
        "```\n",
        "\n",
        "‚úÖ **Valida al inicio, falla r√°pido**\n",
        "```markdown\n",
        "No proceses si par√°metros son inv√°lidos\n",
        "Usa dbutils.notebook.exit() para detener\n",
        "Mensajes de error claros\n",
        "```\n",
        "\n",
        "‚úÖ **Documenta widgets en Markdown**\n",
        "```markdown\n",
        "Explica qu√© hace cada widget\n",
        "Da ejemplos de valores v√°lidos\n",
        "Indica formato esperado (fechas, etc.)\n",
        "```\n",
        "\n",
        "‚úÖ **Widgets al inicio del notebook**\n",
        "```markdown\n",
        "Crear widgets en las primeras celdas\n",
        "Usuario los ve inmediatamente\n",
        "Puede ajustar antes de ejecutar\n",
        "```\n",
        "\n",
        "‚úÖ **Usa labels descriptivos**\n",
        "```markdown\n",
        "‚ùå dbutils.widgets.text(\"d\", \"2025-01-01\")\n",
        "‚úÖ dbutils.widgets.text(\"fecha_inicio\", \"2025-01-01\", \"üìÖ Fecha Inicio (YYYY-MM-DD)\")\n",
        "```\n",
        "\n",
        "‚úÖ **Multiselect: siempre parsea**\n",
        "```markdown\n",
        "value = dbutils.widgets.get(\"opciones\")\n",
        "lista = [x.strip() for x in value.split(\",\") if x.strip()]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Has completado el Tema 4! üéâ**\n",
        "\n",
        "Ahora dominas:\n",
        "- Creaci√≥n y ejecuci√≥n de notebooks ‚úÖ\n",
        "- M√∫ltiples lenguajes (%python, %sql, etc.) ‚úÖ\n",
        "- Magic commands (%fs, %sh, %run, %pip) ‚úÖ\n",
        "- Widgets para parametrizaci√≥n ‚úÖ\n",
        "- Notebooks interactivos y profesionales ‚úÖ\n",
        "- Preparaci√≥n para Jobs automatizados ‚úÖ\n",
        "\n",
        "---\n",
        "\n",
        "**üéì RESUMEN DEL M√ìDULO 2 COMPLETO**\n",
        "\n",
        "Has aprendido:\n",
        "1. **Configurar entorno** - Cuenta, interfaz, workspace, clusters\n",
        "2. **Crear clusters** - Configuraci√≥n, autoscaling, autotermination\n",
        "3. **Trabajar con notebooks** - Celdas, ejecuci√≥n, estados\n",
        "4. **Usar m√∫ltiples lenguajes** - Python, SQL, Scala, R\n",
        "5. **Magic commands** - %fs, %sh, %run, %pip, %md\n",
        "6. **Parametrizar notebooks** - Widgets interactivos\n",
        "\n",
        "**¬°Est√°s listo para el M√≥dulo 3!** üöÄ\n",
        "\n",
        "¬øQuieres continuar con el **M√≥dulo 3: Trabajando con Datos** donde aprender√°s a importar, transformar y analizar datos en Databricks?"
      ],
      "metadata": {
        "id": "-XHs-G7GlG7G"
      }
    }
  ]
}