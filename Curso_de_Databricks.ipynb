{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMr2orAWzpnR1guHzIte/v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seldoncode/tutorial/blob/main/Curso_de_Databricks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Curso de Databricks**\n",
        "## **√çndice**\n",
        "\n",
        "### **M√≥dulo 1: Introducci√≥n y Conceptos Fundamentales**\n",
        "1. **Tema 1: Introducci√≥n a Databricks**\n",
        "   - 1.1 ¬øQu√© es Databricks y por qu√© es importante?\n",
        "   - 1.2 Arquitectura Lakehouse y sus ventajas\n",
        "   - 1.3 Casos de uso: Data Engineering, Data Science, Analytics, ML\n",
        "   - 1.4 Ecosistema: Relaci√≥n con Apache Spark y la nube\n",
        "\n",
        "2. **Tema 2: Fundamentos de Apache Spark**\n",
        "   - 2.1 ¬øQu√© es Apache Spark y procesamiento distribuido?\n",
        "   - 2.2 RDDs, DataFrames y Datasets\n",
        "   - 2.3 Transformaciones vs Acciones (lazy evaluation)\n",
        "   - 2.4 Spark UI b√°sico para monitoreo\n",
        "\n",
        "### **M√≥dulo 2: Primeros Pasos Pr√°cticos**\n",
        "3. **Tema 3: Configuraci√≥n del Entorno**\n",
        "   - 3.1 Creaci√≥n de cuenta en Databricks (Free Edition)\n",
        "   - 3.2 Tour por la interfaz: Workspace, Data, Compute, Workflows\n",
        "   - 3.3 Creaci√≥n del primer cluster\n",
        "   - 3.4 Configuraci√≥n b√°sica de clusters (autoscaling, autotermination)\n",
        "\n",
        "4. **Tema 4: Introducci√≥n a Notebooks**\n",
        "   - 4.1 Creaci√≥n y ejecuci√≥n de notebooks\n",
        "   - 4.2 Lenguajes disponibles y cambio entre ellos\n",
        "   - 4.3 Magic commands (%python, %sql, %fs, %sh)\n",
        "   - 4.4 Widgets para parametrizaci√≥n\n",
        "\n",
        "### **M√≥dulo 3: Trabajando con Datos**\n",
        "5. **Tema 5: Gesti√≥n de Datos en Databricks**\n",
        "   - 5.1 DBFS (Databricks File System) y rutas\n",
        "   - 5.2 Carga de datos desde m√∫ltiples fuentes (CSV, JSON, Parquet)\n",
        "   - 5.3 **Introducci√≥n a Delta Lake: ventajas y caracter√≠sticas**\n",
        "   - 5.4 Cat√°logo de datos y tablas (managed vs external)\n",
        "\n",
        "6. **Tema 6: Manipulaci√≥n de Datos con PySpark**\n",
        "   - 6.1 Creaci√≥n de DataFrames\n",
        "   - 6.2 Operaciones b√°sicas: select, filter, withColumn\n",
        "   - 6.3 Transformaciones comunes: groupBy, join, orderBy\n",
        "   - 6.4 Manejo de valores nulos y duplicados\n",
        "\n",
        "7. **Tema 7: SQL en Databricks**\n",
        "   - 7.1 SQL en notebooks\n",
        "   - 7.2 Creaci√≥n de tablas y vistas\n",
        "   - 7.3 Consultas SQL b√°sicas y avanzadas\n",
        "   - 7.4 **SQL Warehouses (introducci√≥n b√°sica)**\n",
        "\n",
        "### **M√≥dulo 4: Delta Lake (Fundamental para Databricks)**\n",
        "8. **Tema 8: Trabajando con Delta Lake**\n",
        "   - 8.1 Creaci√≥n de tablas Delta\n",
        "   - 8.2 ACID transactions en la pr√°ctica\n",
        "   - 8.3 Time Travel y versionado\n",
        "   - 8.4 Operaciones MERGE, UPDATE, DELETE\n",
        "\n",
        "### **M√≥dulo 5: An√°lisis y Visualizaci√≥n**\n",
        "9. **Tema 9: An√°lisis de Datos**\n",
        "   - 9.1 Agregaciones y estad√≠sticas descriptivas\n",
        "   - 9.2 Window functions\n",
        "   - 9.3 **User Defined Functions (UDF) - b√°sico**\n",
        "\n",
        "10. **Tema 10: Visualizaci√≥n**\n",
        "    - 10.1 Visualizaciones nativas en notebooks\n",
        "    - 10.2 Dashboards b√°sicos\n",
        "    - 10.3 Integraci√≥n con herramientas externas (opcional)\n",
        "\n",
        "### **M√≥dulo 6: Workflows y Automatizaci√≥n**\n",
        "11. **Tema 11: Jobs y Workflows**\n",
        "    - 11.1 ¬øQu√© son los Jobs en Databricks?\n",
        "    - 11.2 Creaci√≥n de un Job b√°sico\n",
        "    - 11.3 Programaci√≥n y triggers\n",
        "    - 11.4 Monitoreo de ejecuciones\n",
        "\n",
        "### **M√≥dulo 7: Buenas Pr√°cticas y Gesti√≥n**\n",
        "12. **Tema 12: Control de Versiones**\n",
        "    - 12.1 Integraci√≥n con Git (Repos)\n",
        "    - 12.2 Workflows de desarrollo (dev/prod)\n",
        "    - 12.3 Organizaci√≥n del workspace\n",
        "\n",
        "13. **Tema 13: Optimizaci√≥n y Mejores Pr√°cticas**\n",
        "    - 13.1 Gesti√≥n eficiente de clusters y costos\n",
        "    - 13.2 Particionamiento de datos\n",
        "    - 13.3 Optimizaci√≥n de Delta (OPTIMIZE, Z-ORDER)\n",
        "    - 13.4 Debugging y Spark UI avanzado\n",
        "\n",
        "### **M√≥dulo 8: Proyecto Final**\n",
        "14. **Tema 14: Proyecto Integrador**\n",
        "    - 14.1 ETL completo: ingesta ‚Üí transformaci√≥n ‚Üí almacenamiento\n",
        "    - 14.2 An√°lisis y visualizaci√≥n\n",
        "    - 14.3 Automatizaci√≥n con Jobs\n",
        "    - 14.4 Documentaci√≥n y presentaci√≥n\n",
        "\n",
        "### **Recursos y Pr√≥ximos Pasos**\n",
        "15. **Tema 15: Continuando el Aprendizaje**\n",
        "    - 15.1 Certificaciones Databricks\n",
        "    - 15.2 Temas avanzados sugeridos\n",
        "    - 15.3 Comunidad y recursos\n",
        "\n",
        "---\n",
        "\n",
        "### Duraci√≥n estimada\n",
        "- 8 semanas\n",
        "\n",
        "### ¬øQu√© aprender√°s en este curso?\n",
        "\n",
        "Este curso te guiar√° desde cero en el mundo de **Databricks**, la plataforma l√≠der para anal√≠tica de datos a gran escala. Al finalizar, ser√°s capaz de:\n",
        "\n",
        "- Comprender qu√© es Databricks y c√≥mo se integra en el ecosistema moderno de datos\n",
        "- Trabajar con Apache Spark de forma pr√°ctica\n",
        "- Procesar y analizar grandes vol√∫menes de datos\n",
        "- Crear pipelines de datos automatizados\n",
        "- Aplicar buenas pr√°cticas en ingenier√≠a de datos\n",
        "\n",
        "### Metodolog√≠a del Curso\n",
        "\n",
        "Este curso est√° dise√±ado con un enfoque **100% pr√°ctico**:\n",
        "\n",
        "- **Teor√≠a m√≠nima necesaria**: Solo los conceptos esenciales\n",
        "- **Ejercicios hands-on**: Cada concepto se practica inmediatamente\n",
        "- **Ejemplos del mundo real**: Casos de uso aplicables a tu trabajo\n",
        "- **Proyecto integrador**: Construcci√≥n de un pipeline completo de datos\n",
        "\n",
        "### Requisitos Previos\n",
        "\n",
        "Para aprovechar al m√°ximo este curso, necesitas:\n",
        "\n",
        "1. **Conocimientos b√°sicos de Python** (variables, funciones, estructuras de control)\n",
        "2. **Familiaridad con SQL** (SELECT, WHERE, JOIN b√°sicos)\n",
        "3. **Conceptos b√°sicos de bases de datos** (qu√© es una tabla, registro, columna)\n",
        "4. **Acceso a internet** para usar Databricks Community Edition\n",
        "\n",
        "> üí° **Nota**: Si no tienes experiencia con Python o SQL, no te preocupes. Iremos paso a paso y los conceptos se explican de forma clara.\n",
        "\n",
        "### Estructura de los Apuntes\n",
        "\n",
        "Estos apuntes siguen una estructura consistente:\n",
        "- üìö **Teor√≠a**: Conceptos fundamentales explicados de forma clara\n",
        "- üíª **Pr√°ctica**: C√≥digo ejecutable que puedes probar\n",
        "- ‚ö†Ô∏è **Importante**: Puntos cr√≠ticos a recordar\n",
        "- üí° **Tip**: Consejos y mejores pr√°cticas\n",
        "- üéØ **Ejercicio**: Retos para practicar lo aprendido\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YYbcVWVpOEd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **M√≥dulo 1: Introducci√≥n y Conceptos Fundamentales**"
      ],
      "metadata": {
        "id": "zsFQZKEaSjVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tema 1: Introducci√≥n a Databricks**"
      ],
      "metadata": {
        "id": "h8DZ0QAPSkHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 ¬øQu√© es Databricks y por qu√© es importante?**\n",
        "\n",
        "#### **¬øQu√© es Databricks?**\n",
        "\n",
        "**Databricks** es una plataforma unificada de anal√≠tica de datos construida sobre **Apache Spark**. Fue fundada en 2013 por los creadores originales de Apache Spark (en la Universidad de Berkeley).\n",
        "\n",
        "Piensa en Databricks como un **entorno de trabajo completo** que te permite:\n",
        "\n",
        "- **Procesar** grandes cantidades de datos (desde megabytes hasta petabytes)\n",
        "- **Colaborar** con tu equipo en tiempo real (como Google Docs, pero para datos)\n",
        "- **Automatizar** pipelines de datos complejos\n",
        "- **Analizar** datos con SQL, Python, Scala o R\n",
        "- **Entrenar** modelos de Machine Learning a escala\n",
        "- **Orquestar** workflows de principio a fin\n",
        "\n",
        "#### **Analog√≠a para entenderlo mejor**\n",
        "\n",
        "Imagina que eres un chef:\n",
        "\n",
        "- **Excel/Python local** = Tu cocina casera (buenos para platos peque√±os)\n",
        "- **Databricks** = Una cocina industrial profesional (dise√±ada para vol√∫menes grandes, equipos, y eficiencia)\n",
        "\n",
        "As√≠ como una cocina industrial tiene equipos especializados, procesos optimizados y permite que varios chefs trabajen simult√°neamente, Databricks hace lo mismo con tus datos.\n",
        "\n",
        "#### **¬øPor qu√© es importante Databricks?**\n",
        "\n",
        "##### **1. Resuelve problemas del mundo real**\n",
        "\n",
        "Cuando trabajas con datos en 2025, te enfrentas a:\n",
        "\n",
        "```python\n",
        "# Problema 1: VOLUMEN - Datos demasiado grandes\n",
        "# Tu laptop tiene 16GB RAM, pero tus datos pesan 500GB\n",
        "# ‚ùå Pandas no puede cargar todo en memoria\n",
        "# ‚úÖ Databricks distribuye el procesamiento en m√∫ltiples m√°quinas\n",
        "\n",
        "# Problema 2: VELOCIDAD - Procesamiento lento\n",
        "# Un an√°lisis en tu laptop tarda 8 horas\n",
        "# ‚ùå No puedes esperar tanto para tomar decisiones\n",
        "# ‚úÖ Databricks lo procesa en minutos usando procesamiento paralelo\n",
        "\n",
        "# Problema 3: COLABORACI√ìN - Trabajo aislado\n",
        "# Tu c√≥digo funciona en tu m√°quina, pero no en la de tu colega\n",
        "# ‚ùå \"En mi m√°quina funciona\"\n",
        "# ‚úÖ Databricks provee un entorno compartido y consistente\n",
        "```\n",
        "\n",
        "##### **2. Unifica el ciclo completo de datos**\n",
        "\n",
        "Antes de Databricks, necesitabas m√∫ltiples herramientas:\n",
        "\n",
        "```\n",
        "Ingesta de datos     ‚Üí  Herramienta A\n",
        "Transformaci√≥n       ‚Üí  Herramienta B  \n",
        "An√°lisis             ‚Üí  Herramienta C\n",
        "Machine Learning     ‚Üí  Herramienta D\n",
        "Orquestaci√≥n         ‚Üí  Herramienta E\n",
        "Visualizaci√≥n        ‚Üí  Herramienta F\n",
        "```\n",
        "\n",
        "Con Databricks, todo est√° en un solo lugar:\n",
        "\n",
        "```\n",
        "Databricks Lakehouse Platform\n",
        "‚îú‚îÄ‚îÄ Ingesta de datos\n",
        "‚îú‚îÄ‚îÄ Transformaci√≥n (ETL/ELT)\n",
        "‚îú‚îÄ‚îÄ Almacenamiento (Delta Lake)\n",
        "‚îú‚îÄ‚îÄ An√°lisis SQL\n",
        "‚îú‚îÄ‚îÄ Machine Learning\n",
        "‚îú‚îÄ‚îÄ Orquestaci√≥n (Jobs/Workflows)\n",
        "‚îî‚îÄ‚îÄ Dashboards\n",
        "```\n",
        "\n",
        "##### **3. Construido sobre Apache Spark**\n",
        "\n",
        "Apache Spark es el motor de procesamiento distribuido m√°s usado en el mundo para Big Data. Databricks te da acceso a Spark sin la complejidad de configurarlo y mantenerlo.\n",
        "\n",
        "```python\n",
        "# Sin Databricks: Configurar Spark localmente\n",
        "# - Instalar Java\n",
        "# - Instalar Scala\n",
        "# - Configurar SPARK_HOME\n",
        "# - Gestionar dependencias\n",
        "# - Configurar cluster\n",
        "# - Mantener versiones...\n",
        "# ‚è±Ô∏è Puede tomar d√≠as\n",
        "\n",
        "# Con Databricks:\n",
        "# 1. Crear cuenta\n",
        "# 2. Crear cluster (3 clics)\n",
        "# 3. Empezar a trabajar\n",
        "# ‚è±Ô∏è 5 minutos\n",
        "```\n",
        "\n",
        "#### **Casos de uso reales**\n",
        "\n",
        "##### **Ejemplo 1: E-commerce**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "Problema: Una tienda online tiene millones de transacciones diarias\n",
        "y necesita an√°lisis en tiempo real.\n",
        "\n",
        "Con Databricks:\n",
        "1. Ingesta de logs de servidor (streaming)\n",
        "2. Procesamiento de eventos de compra en tiempo real\n",
        "3. Detecci√≥n de fraude usando ML\n",
        "4. Dashboards para el equipo de negocio\n",
        "5. Recomendaciones personalizadas\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "##### **Ejemplo 2: Salud**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "Problema: Un hospital necesita analizar historiales m√©dicos de\n",
        "millones de pacientes para investigaci√≥n.\n",
        "\n",
        "Con Databricks:\n",
        "1. Unificaci√≥n de datos de m√∫ltiples sistemas\n",
        "2. Anonimizaci√≥n de datos sensibles\n",
        "3. An√°lisis estad√≠stico a gran escala\n",
        "4. Modelos predictivos para diagn√≥sticos\n",
        "5. Cumplimiento normativo (HIPAA, GDPR)\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "##### **Ejemplo 3: Finanzas**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "Problema: Un banco procesa millones de transacciones y necesita\n",
        "detectar actividades sospechosas.\n",
        "\n",
        "Con Databricks:\n",
        "1. Ingesta de transacciones en tiempo real\n",
        "2. An√°lisis de patrones hist√≥ricos\n",
        "3. Modelos de detecci√≥n de anomal√≠as\n",
        "4. Reportes regulatorios automatizados\n",
        "5. Data governance y auditor√≠a\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "#### **¬øQui√©n usa Databricks?**\n",
        "\n",
        "Empresas de todos los tama√±os y sectores:\n",
        "\n",
        "- **Tecnolog√≠a**: Comcast, Shell, H&M\n",
        "- **Finanzas**: ING, HSBC, Capital One\n",
        "- **Retail**: Walgreens, Cond√© Nast\n",
        "- **Salud**: Regeneron Pharmaceuticals\n",
        "- **Media**: Fox, NBC Universal\n",
        "\n",
        "#### **Ventajas clave de Databricks**\n",
        "\n",
        "```python\n",
        "# 1. ESCALABILIDAD\n",
        "# Procesa desde KB hasta PB sin cambiar tu c√≥digo\n",
        "df = spark.read.parquet(\"data.parquet\")  # Funciona igual con 1GB o 1TB\n",
        "\n",
        "# 2. VELOCIDAD\n",
        "# Procesamiento distribuido y en memoria\n",
        "# Lo que tarda horas en Pandas, puede tomar minutos\n",
        "\n",
        "# 3. COLABORACI√ìN\n",
        "# Notebooks compartidos en tiempo real\n",
        "# Control de versiones integrado\n",
        "\n",
        "# 4. MULTI-LENGUAJE\n",
        "# Python, SQL, Scala, R en el mismo notebook\n",
        "\n",
        "# 5. GESTI√ìN AUTOMATIZADA\n",
        "# Clusters que se crean/destruyen autom√°ticamente\n",
        "# Optimizaci√≥n de costos\n",
        "\n",
        "# 6. SEGURIDAD Y GOVERNANCE\n",
        "# Control de acceso granular\n",
        "# Auditor√≠a completa\n",
        "# Cumplimiento normativo\n",
        "```\n",
        "\n",
        "#### **Ecosistema Databricks**\n",
        "\n",
        "Databricks no trabaja solo, se integra con:\n",
        "\n",
        "```\n",
        "‚òÅÔ∏è Clouds:\n",
        "   - AWS (Amazon Web Services)\n",
        "   - Azure (Microsoft)\n",
        "   - GCP (Google Cloud Platform)\n",
        "\n",
        "üìä Almacenamiento:\n",
        "   - S3, Azure Blob, Google Cloud Storage\n",
        "   - Delta Lake (formato optimizado)\n",
        "\n",
        "üîß Herramientas:\n",
        "   - Power BI, Tableau (visualizaci√≥n)\n",
        "   - dbt (transformaci√≥n)\n",
        "   - Airflow (orquestaci√≥n)\n",
        "   - MLflow (ML lifecycle)\n",
        "\n",
        "üîå Conectores:\n",
        "   - Bases de datos (MySQL, PostgreSQL, SQL Server)\n",
        "   - Data warehouses (Snowflake, Redshift)\n",
        "   - APIs y servicios web\n",
        "```\n",
        "\n",
        "#### **Databricks vs Otras Herramientas**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DATABRICKS vs PANDAS\n",
        "--------------------\n",
        "Pandas: Excelente para datasets peque√±os (<5GB) en una sola m√°quina\n",
        "Databricks: Dise√±ado para datasets grandes, procesamiento distribuido\n",
        "\n",
        "DATABRICKS vs SNOWFLAKE\n",
        "-----------------------\n",
        "Snowflake: Data warehouse optimizado para SQL\n",
        "Databricks: Plataforma completa (SQL + Python + ML + Streaming)\n",
        "\n",
        "DATABRICKS vs AWS EMR\n",
        "---------------------\n",
        "EMR: Infraestructura Spark en AWS (m√°s control, m√°s complejidad)\n",
        "Databricks: Plataforma gestionada (menos administraci√≥n, m√°s productividad)\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica Inicial - Comprendiendo el Concepto**\n",
        "\n",
        "Antes de instalar nada, vamos a simular el problema que Databricks resuelve:\n",
        "\n",
        "```python\n",
        "# Celda de Python en tu Jupyter Notebook\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Simulemos procesar un dataset \"grande\" con Pandas\n",
        "print(\"üêº Procesamiento con Pandas (simulaci√≥n)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Crear un dataset de ejemplo\n",
        "data = {\n",
        "    'cliente_id': range(1000000),\n",
        "    'compra': [100 + i % 500 for i in range(1000000)],\n",
        "    'categoria': ['A', 'B', 'C'] * 333334\n",
        "}\n",
        "\n",
        "# Medir tiempo\n",
        "inicio = time.time()\n",
        "df_pandas = pd.DataFrame(data)\n",
        "resultado = df_pandas.groupby('categoria')['compra'].sum()\n",
        "fin = time.time()\n",
        "\n",
        "print(f\"‚úÖ Tiempo de procesamiento: {fin - inicio:.4f} segundos\")\n",
        "print(f\"üìä Resultado:\\n{resultado}\")\n",
        "print(\"\\n‚ö†Ô∏è Limitaci√≥n: Pandas requiere que TODO quepa en memoria\")\n",
        "print(\"‚ö†Ô∏è Con 10M, 100M o 1B de filas... tu laptop colapsar√≠a\")\n",
        "```\n",
        "\n",
        "**Salida esperada:**\n",
        "```\n",
        "üêº Procesamiento con Pandas (simulaci√≥n)\n",
        "--------------------------------------------------\n",
        "‚úÖ Tiempo de procesamiento: 0.1234 segundos\n",
        "üìä Resultado:\n",
        "categoria\n",
        "A    166583350000\n",
        "B    166583350000\n",
        "C    166666650000\n",
        "Name: compra, dtype: int64\n",
        "\n",
        "‚ö†Ô∏è Limitaci√≥n: Pandas requiere que TODO quepa en memoria\n",
        "‚ö†Ô∏è Con 10M, 100M o 1B de filas... tu laptop colapsar√≠a\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 1.1 - Reflexi√≥n**\n",
        "\n",
        "Antes de continuar, responde en una celda Markdown:\n",
        "\n",
        "```markdown\n",
        "### Reflexi√≥n Personal\n",
        "\n",
        "1. **¬øQu√© tipo de datos manejo actualmente en mi trabajo/estudios?**\n",
        "   - Volumen aproximado:\n",
        "   - Formato (CSV, Excel, JSON, etc.):\n",
        "   - Frecuencia de actualizaci√≥n:\n",
        "\n",
        "2. **¬øQu√© problemas enfrento con mis herramientas actuales?**\n",
        "   - [ ] Lentitud en el procesamiento\n",
        "   - [ ] Datos demasiado grandes para mi m√°quina\n",
        "   - [ ] Dificultad para compartir an√°lisis\n",
        "   - [ ] Falta de automatizaci√≥n\n",
        "   - [ ] Otro: _______________\n",
        "\n",
        "3. **¬øC√≥mo podr√≠a Databricks ayudarme?**\n",
        "   (Tu respuesta aqu√≠)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "```markdown\n",
        "1. **Databricks = Plataforma unificada** para todo el ciclo de vida de datos\n",
        "2. **Construida sobre Apache Spark** (procesamiento distribuido)\n",
        "3. **Resuelve problemas de escala** que herramientas tradicionales no pueden\n",
        "4. **Colaborativa y cloud-native** desde el dise√±o\n",
        "5. **Multi-prop√≥sito**: Data Engineering, Data Science, Analytics, ML\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "```markdown\n",
        "‚úÖ Databricks no reemplaza Excel o Pandas para an√°lisis peque√±os\n",
        "   ‚Üí √ösalo cuando el volumen o la complejidad lo justifique\n",
        "\n",
        "‚úÖ La curva de aprendizaje vale la pena\n",
        "   ‚Üí Es una habilidad muy demandada en el mercado laboral\n",
        "\n",
        "‚úÖ Empieza con Community Edition (gratis)\n",
        "   ‚Üí Practica sin costos antes de usar versiones empresariales\n",
        "\n",
        "‚úÖ Databricks es el futuro del an√°lisis de datos\n",
        "   ‚Üí Grandes empresas est√°n migrando a arquitecturas Lakehouse\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìö **Recursos Adicionales**\n",
        "\n",
        "```markdown\n",
        "üìñ Documentaci√≥n oficial: https://docs.databricks.com\n",
        "üéì Databricks Academy (cursos gratuitos): https://www.databricks.com/learn\n",
        "üì∫ Canal de YouTube: Databricks\n",
        "üê¶ Twitter: @databricks\n",
        "üí¨ Community Forums: https://community.databricks.com\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Pr√≥ximos Pasos**\n",
        "\n",
        "En la siguiente secci√≥n (1.2), profundizaremos en la **Arquitectura Lakehouse** y entenderemos c√≥mo Databricks organiza y procesa los datos de forma eficiente.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "7CFtVf2_O6C4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Arquitectura Lakehouse y sus ventajas**\n",
        "\n",
        "#### **El Problema Hist√≥rico: Data Warehouse vs Data Lake**\n",
        "\n",
        "Antes de Databricks, las empresas ten√≠an que elegir entre dos arquitecturas incompatibles:\n",
        "\n",
        "```markdown\n",
        "üìä DATA WAREHOUSE (Almac√©n de Datos)\n",
        "‚îú‚îÄ Ejemplos: Snowflake, Redshift, BigQuery\n",
        "‚îú‚îÄ Fortalezas:\n",
        "‚îÇ  ‚úÖ Excelente para SQL y BI\n",
        "‚îÇ  ‚úÖ Rendimiento r√°pido en consultas estructuradas\n",
        "‚îÇ  ‚úÖ ACID transactions (confiabilidad)\n",
        "‚îÇ  ‚úÖ Esquema bien definido\n",
        "‚îÇ\n",
        "‚îî‚îÄ Debilidades:\n",
        "   ‚ùå Solo datos estructurados (tablas)\n",
        "   ‚ùå Costoso para grandes vol√∫menes\n",
        "   ‚ùå No apto para Machine Learning\n",
        "   ‚ùå Inflexible para datos no estructurados\n",
        "\n",
        "üèûÔ∏è DATA LAKE (Lago de Datos)\n",
        "‚îú‚îÄ Ejemplos: HDFS, S3, Azure Data Lake\n",
        "‚îú‚îÄ Fortalezas:\n",
        "‚îÇ  ‚úÖ Almacena TODO tipo de datos (estructurados, semi-estructurados, no estructurados)\n",
        "‚îÇ  ‚úÖ Econ√≥mico (almacenamiento barato)\n",
        "‚îÇ  ‚úÖ Flexible y escalable\n",
        "‚îÇ  ‚úÖ Bueno para ML y an√°lisis exploratorio\n",
        "‚îÇ\n",
        "‚îî‚îÄ Debilidades:\n",
        "   ‚ùå Sin ACID transactions (inconsistencias)\n",
        "   ‚ùå Rendimiento SQL pobre\n",
        "   ‚ùå \"Data Swamp\" (pantano de datos) sin governance\n",
        "   ‚ùå Complejidad en la gesti√≥n\n",
        "```\n",
        "\n",
        "#### **Resultado: Arquitectura Dual (El Problema)**\n",
        "\n",
        "Las empresas terminaban con **dos sistemas separados**:\n",
        "\n",
        "```python\n",
        "# Arquitectura tradicional (problem√°tica)\n",
        "\n",
        "\"\"\"\n",
        "1. INGESTA DE DATOS\n",
        "   ‚Üì\n",
        "2. DATA LAKE (almacenamiento barato)\n",
        "   - Logs, JSONs, im√°genes, videos\n",
        "   - Datos crudos sin procesar\n",
        "   ‚Üì\n",
        "3. ETL (Extract, Transform, Load)\n",
        "   - Procesar y limpiar datos\n",
        "   - ‚ö†Ô∏è Proceso costoso y lento\n",
        "   ‚Üì\n",
        "4. DATA WAREHOUSE (almacenamiento caro)\n",
        "   - Solo datos estructurados y limpios\n",
        "   - Para reportes y BI\n",
        "   ‚Üì\n",
        "5. PROBLEMAS:\n",
        "   ‚ùå Duplicaci√≥n de datos (2x costo)\n",
        "   ‚ùå Dos sistemas que mantener\n",
        "   ‚ùå Latencia alta (ETL batch nocturno)\n",
        "   ‚ùå Complejidad operacional\n",
        "   ‚ùå Datos desactualizados para decisiones\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "#### **La Soluci√≥n: Lakehouse Architecture**\n",
        "\n",
        "Databricks introdujo **Lakehouse**: una arquitectura que combina lo mejor de ambos mundos.\n",
        "\n",
        "```markdown\n",
        "üèõÔ∏è LAKEHOUSE = Data Lake + Data Warehouse\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ         DATABRICKS LAKEHOUSE                ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  ‚úÖ Almacenamiento econ√≥mico (como Data Lake)‚îÇ\n",
        "‚îÇ  ‚úÖ ACID transactions (como Data Warehouse) ‚îÇ\n",
        "‚îÇ  ‚úÖ Performance SQL (como Data Warehouse)   ‚îÇ\n",
        "‚îÇ  ‚úÖ ML y an√°lisis (como Data Lake)          ‚îÇ\n",
        "‚îÇ  ‚úÖ Todos los tipos de datos                ‚îÇ\n",
        "‚îÇ  ‚úÖ Una sola copia de los datos             ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "#### **Componentes de la Arquitectura Lakehouse**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CAPAS DE LA ARQUITECTURA LAKEHOUSE\n",
        "\"\"\"\n",
        "\n",
        "# 1. STORAGE LAYER (Capa de Almacenamiento)\n",
        "storage = {\n",
        "    'tecnolog√≠a': 'Cloud Object Storage (S3, ADLS, GCS)',\n",
        "    'formato': 'Delta Lake (Parquet optimizado)',\n",
        "    'caracter√≠sticas': [\n",
        "        'Almacenamiento econ√≥mico',\n",
        "        'Escalabilidad infinita',\n",
        "        'Durabilidad y disponibilidad'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 2. DELTA LAKE (Motor de Almacenamiento)\n",
        "delta_lake = {\n",
        "    'funci√≥n': 'A√±ade ACID y confiabilidad al Data Lake',\n",
        "    'caracter√≠sticas': [\n",
        "        'ACID transactions',\n",
        "        'Time Travel (versionado)',\n",
        "        'Schema enforcement',\n",
        "        'Unified batch & streaming'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 3. COMPUTE LAYER (Capa de Procesamiento)\n",
        "compute = {\n",
        "    'motor': 'Apache Spark optimizado',\n",
        "    'caracter√≠sticas': [\n",
        "        'Procesamiento distribuido',\n",
        "        'Auto-scaling',\n",
        "        'Multi-lenguaje (SQL, Python, Scala, R)'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 4. GOVERNANCE & SECURITY (Gobernanza)\n",
        "governance = {\n",
        "    'herramienta': 'Unity Catalog',\n",
        "    'caracter√≠sticas': [\n",
        "        'Control de acceso centralizado',\n",
        "        'Auditor√≠a',\n",
        "        'Lineage (trazabilidad)',\n",
        "        'Data discovery'\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Visualizaci√≥n de la Arquitectura**\n",
        "\n",
        "```markdown\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    USUARIOS Y APLICACIONES               ‚îÇ\n",
        "‚îÇ  [Data Analysts] [Data Scientists] [ML Engineers] [Apps] ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                         ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ               DATABRICKS WORKSPACE                       ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
        "‚îÇ  ‚îÇNotebooks ‚îÇ ‚îÇ SQL Editor‚îÇ ‚îÇJobs/     ‚îÇ ‚îÇDashboards‚îÇ    ‚îÇ\n",
        "‚îÇ  ‚îÇ          ‚îÇ ‚îÇ           ‚îÇ ‚îÇWorkflows ‚îÇ ‚îÇ          ‚îÇ    ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                         ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                  COMPUTE LAYER                           ‚îÇ\n",
        "‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ\n",
        "‚îÇ         ‚îÇ    Apache Spark Clusters       ‚îÇ               ‚îÇ\n",
        "‚îÇ         ‚îÇ  (Auto-scaling, Optimizado)    ‚îÇ               ‚îÇ\n",
        "‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                         ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                   DELTA LAKE                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ\n",
        "‚îÇ  ‚îÇ  ACID Transactions | Time Travel | Schema      ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îÇ  Enforcement | Optimization | Indexing         ‚îÇ      ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                         ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                 CLOUD STORAGE                            ‚îÇ\n",
        "‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ\n",
        "‚îÇ    ‚îÇ AWS S3   ‚îÇ  ‚îÇ Azure    ‚îÇ  ‚îÇ Google   ‚îÇ              ‚îÇ\n",
        "‚îÇ    ‚îÇ          ‚îÇ  ‚îÇ ADLS     ‚îÇ  ‚îÇ Cloud    ‚îÇ              ‚îÇ\n",
        "‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ\n",
        "‚îÇ                                                          ‚îÇ\n",
        "‚îÇ  [Delta Tables] [Parquet] [JSON] [CSV] [Images] [Logs]   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "#### **Ventajas del Lakehouse**\n",
        "\n",
        "##### **1. Costo Reducido**\n",
        "\n",
        "```python\n",
        "# ANTES: Arquitectura Dual\n",
        "costos_antes = {\n",
        "    'data_lake': 100,      # Almacenamiento en S3\n",
        "    'data_warehouse': 500, # Snowflake/Redshift\n",
        "    'etl_tools': 100,      # Herramientas ETL\n",
        "    'total': 700\n",
        "}\n",
        "\n",
        "# AHORA: Lakehouse\n",
        "costos_ahora = {\n",
        "    'storage': 100,        # Solo almacenamiento en cloud\n",
        "    'databricks': 250,     # Plataforma unificada\n",
        "    'total': 350\n",
        "}\n",
        "\n",
        "ahorro = (costos_antes['total'] - costos_ahora['total']) / costos_antes['total']\n",
        "print(f\"üí∞ Ahorro: {ahorro:.0%}\")  # 50% de ahorro aproximado\n",
        "```\n",
        "\n",
        "**Salida:**\n",
        "```\n",
        "üí∞ Ahorro: 50%\n",
        "```\n",
        "\n",
        "##### **2. Rendimiento Superior**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "OPTIMIZACIONES EN LAKEHOUSE\n",
        "\"\"\"\n",
        "\n",
        "# Delta Lake optimiza autom√°ticamente\n",
        "optimizaciones = {\n",
        "    'z_ordering': 'Organiza datos para consultas r√°pidas',\n",
        "    'data_skipping': 'Salta archivos irrelevantes',\n",
        "    'caching': 'Datos en memoria para acceso r√°pido',\n",
        "    'photon': 'Motor nativo C++ (hasta 12x m√°s r√°pido)',\n",
        "    'liquid_clustering': 'Clustering autom√°tico y adaptativo'\n",
        "}\n",
        "\n",
        "# Ejemplo pr√°ctico\n",
        "\"\"\"\n",
        "Consulta t√≠pica en Data Lake tradicional: 45 segundos\n",
        "Misma consulta en Lakehouse con Delta: 3 segundos\n",
        "‚Üí 15x m√°s r√°pido\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "##### **3. Confiabilidad (ACID Transactions)**\n",
        "\n",
        "```python\n",
        "# Problema en Data Lake tradicional\n",
        "\"\"\"\n",
        "‚ùå ESCENARIO SIN ACID:\n",
        "\n",
        "1. Inicias escritura de 1M registros\n",
        "2. A mitad del proceso, falla la conexi√≥n\n",
        "3. Resultado: 500K registros escritos, 500K perdidos\n",
        "4. Datos corruptos e inconsistentes\n",
        "5. No puedes hacer rollback\n",
        "\"\"\"\n",
        "\n",
        "# Soluci√≥n en Lakehouse con Delta\n",
        "\"\"\"\n",
        "‚úÖ ESCENARIO CON ACID (Delta Lake):\n",
        "\n",
        "1. Inicias escritura de 1M registros\n",
        "2. Si falla, Delta hace rollback autom√°tico\n",
        "3. Resultado: 0 registros (transacci√≥n completa o nada)\n",
        "4. Datos siempre consistentes\n",
        "5. Puedes ver versiones anteriores\n",
        "\"\"\"\n",
        "\n",
        "# C√≥digo ejemplo\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "# Escritura ACID garantizada\n",
        "df.write.format(\"delta\").mode(\"append\").save(\"/data/ventas\")\n",
        "\n",
        "# Si algo falla, los datos anteriores permanecen intactos\n",
        "# No hay corrupci√≥n ni inconsistencias\n",
        "```\n",
        "\n",
        "##### **4. Time Travel (Viaje en el Tiempo)**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CAPACIDAD √öNICA DE DELTA LAKE\n",
        "\"\"\"\n",
        "\n",
        "# Ver versiones hist√≥ricas de tus datos\n",
        "# √ötil para:\n",
        "# - Auditor√≠a\n",
        "# - Recuperaci√≥n de errores\n",
        "# - An√°lisis temporal\n",
        "# - Compliance\n",
        "\n",
        "# Ejemplo pr√°ctico\n",
        "```\n",
        "\n",
        "```sql\n",
        "-- Celda SQL en tu notebook\n",
        "\n",
        "-- Ver la tabla actual\n",
        "SELECT * FROM ventas;\n",
        "\n",
        "-- Ver c√≥mo estaba hace 7 d√≠as\n",
        "SELECT * FROM ventas VERSION AS OF 7;\n",
        "\n",
        "-- Ver estado de ayer a las 3pm\n",
        "SELECT * FROM ventas TIMESTAMP AS OF '2026-01-31 15:00:00';\n",
        "\n",
        "-- Restaurar versi√≥n anterior si cometiste un error\n",
        "RESTORE TABLE ventas TO VERSION AS OF 5;\n",
        "```\n",
        "\n",
        "##### **5. Unificaci√≥n de Batch y Streaming**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ANTES: Dos sistemas diferentes\n",
        "\"\"\"\n",
        "# Sistema 1: Batch (procesos nocturnos)\n",
        "batch_pipeline = \"ETL cada 24 horas ‚Üí datos desactualizados\"\n",
        "\n",
        "# Sistema 2: Streaming (tiempo real)\n",
        "streaming_pipeline = \"Infraestructura compleja ‚Üí costoso mantener\"\n",
        "\n",
        "\"\"\"\n",
        "LAKEHOUSE: Un solo sistema\n",
        "\"\"\"\n",
        "# Mismo c√≥digo para batch y streaming\n",
        "df = spark.readStream.format(\"delta\") \\\n",
        "    .option(\"readChangeFeed\", \"true\") \\\n",
        "    .table(\"eventos\")\n",
        "\n",
        "# Procesa datos en tiempo real con la misma API\n",
        "df.writeStream.format(\"delta\") \\\n",
        "    .option(\"checkpointLocation\", \"/checkpoints/\") \\\n",
        "    .table(\"eventos_procesados\")\n",
        "\n",
        "# ‚úÖ Delta Lake maneja ambos casos transparentemente\n",
        "```\n",
        "\n",
        "##### **6. Schema Evolution (Evoluci√≥n de Esquema)**\n",
        "\n",
        "```python\n",
        "# Escenario com√∫n: Tu estructura de datos cambia con el tiempo\n",
        "\n",
        "# ANTES: Data Warehouse r√≠gido\n",
        "\"\"\"\n",
        "‚ùå Problema:\n",
        "- A√±adir columna nueva requiere cambios en toda la pipeline\n",
        "- Downtime para migraciones\n",
        "- Procesos complejos y arriesgados\n",
        "\"\"\"\n",
        "\n",
        "# LAKEHOUSE: Flexible y autom√°tico\n",
        "\"\"\"\n",
        "‚úÖ Soluci√≥n con Delta Lake:\n",
        "\"\"\"\n",
        "\n",
        "# Los datos originales tienen 3 columnas\n",
        "df_original = spark.createDataFrame([\n",
        "    (1, \"Juan\", 1000),\n",
        "    (2, \"Mar√≠a\", 1500)\n",
        "], [\"id\", \"nombre\", \"salario\"])\n",
        "\n",
        "df_original.write.format(\"delta\").save(\"/data/empleados\")\n",
        "\n",
        "# M√°s tarde, necesitas a√±adir una columna\n",
        "df_nuevo = spark.createDataFrame([\n",
        "    (3, \"Pedro\", 2000, \"Ventas\"),  # Nueva columna: departamento\n",
        "    (4, \"Ana\", 1800, \"IT\")\n",
        "], [\"id\", \"nombre\", \"salario\", \"departamento\"])\n",
        "\n",
        "# Delta Lake maneja el cambio autom√°ticamente\n",
        "df_nuevo.write.format(\"delta\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .option(\"mergeSchema\", \"true\") \\\n",
        "    .save(\"/data/empleados\")\n",
        "\n",
        "# ‚úÖ Registros antiguos tendr√°n NULL en 'departamento'\n",
        "# ‚úÖ No hay downtime\n",
        "# ‚úÖ No hay migraci√≥n compleja\n",
        "```\n",
        "\n",
        "##### **7. Soporte Multi-Formato**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "LAKEHOUSE puede trabajar con CUALQUIER formato\n",
        "\"\"\"\n",
        "\n",
        "formatos_soportados = {\n",
        "    'estructurados': ['Delta', 'Parquet', 'ORC', 'Avro'],\n",
        "    'semi_estructurados': ['JSON', 'XML', 'CSV'],\n",
        "    'no_estructurados': ['Im√°genes', 'Videos', 'Audio', 'PDFs', 'Logs'],\n",
        "    'streaming': ['Kafka', 'Kinesis', 'Event Hubs']\n",
        "}\n",
        "\n",
        "# Ejemplo: Leer m√∫ltiples formatos en una misma sesi√≥n\n",
        "df_json = spark.read.json(\"/data/logs.json\")\n",
        "df_csv = spark.read.csv(\"/data/ventas.csv\", header=True)\n",
        "df_parquet = spark.read.parquet(\"/data/clientes.parquet\")\n",
        "df_delta = spark.read.format(\"delta\").load(\"/data/productos\")\n",
        "\n",
        "# Todos pueden trabajarse juntos y convertirse a Delta\n",
        "df_json.write.format(\"delta\").save(\"/delta/logs\")\n",
        "```\n",
        "\n",
        "#### **Comparaci√≥n Visual: Antes vs Lakehouse**\n",
        "\n",
        "```python\n",
        "# Celda Python para visualizar la diferencia\n",
        "\n",
        "comparacion = {\n",
        "    'Caracter√≠stica': [\n",
        "        'Tipos de datos',\n",
        "        'ACID transactions',\n",
        "        'Performance SQL',\n",
        "        'Machine Learning',\n",
        "        'Costo',\n",
        "        'Complejidad',\n",
        "        'Time Travel',\n",
        "        'Streaming',\n",
        "        'Escalabilidad'\n",
        "    ],\n",
        "    'Data Warehouse': [\n",
        "        'Solo estructurados',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ Excelente',\n",
        "        '‚ùå Limitado',\n",
        "        'üí∞üí∞üí∞ Alto',\n",
        "        'üîß Media',\n",
        "        '‚ùå No',\n",
        "        '‚ùå No',\n",
        "        '‚ö†Ô∏è Limitada'\n",
        "    ],\n",
        "    'Data Lake': [\n",
        "        'Todos',\n",
        "        '‚ùå No',\n",
        "        '‚ùå Pobre',\n",
        "        '‚úÖ Excelente',\n",
        "        'üí∞ Bajo',\n",
        "        'üîßüîßüîß Alta',\n",
        "        '‚ùå No',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ Infinita'\n",
        "    ],\n",
        "    'Lakehouse': [\n",
        "        'Todos',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ Excelente',\n",
        "        '‚úÖ Excelente',\n",
        "        'üí∞üí∞ Medio',\n",
        "        'üîß Baja',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ Infinita'\n",
        "    ]\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "df_comp = pd.DataFrame(comparacion)\n",
        "print(df_comp.to_string(index=False))\n",
        "```\n",
        "\n",
        "**Salida:**\n",
        "```\n",
        "       Caracter√≠stica    Data Warehouse          Data Lake           Lakehouse\n",
        "     Tipos de datos Solo estructurados              Todos               Todos\n",
        "  ACID transactions             ‚úÖ S√≠              ‚ùå No               ‚úÖ S√≠\n",
        "     Performance SQL       ‚úÖ Excelente           ‚ùå Pobre        ‚úÖ Excelente\n",
        "   Machine Learning         ‚ùå Limitado       ‚úÖ Excelente        ‚úÖ Excelente\n",
        "                Costo       üí∞üí∞üí∞ Alto            üí∞ Bajo          üí∞üí∞ Medio\n",
        "          Complejidad           üîß Media        üîßüîßüîß Alta            üîß Baja\n",
        "         Time Travel               ‚ùå No              ‚ùå No               ‚úÖ S√≠\n",
        "           Streaming               ‚ùå No              ‚úÖ S√≠               ‚úÖ S√≠\n",
        "       Escalabilidad      ‚ö†Ô∏è Limitada        ‚úÖ Infinita         ‚úÖ Infinita\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Entendiendo Delta Lake**\n",
        "\n",
        "Aunque a√∫n no tenemos Databricks configurado, podemos simular el concepto:\n",
        "\n",
        "```python\n",
        "# Celda Python - Simulaci√≥n conceptual\n",
        "\n",
        "\"\"\"\n",
        "SIMULACI√ìN: Por qu√© Delta Lake es superior\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Simulemos un escenario de Data Lake tradicional\n",
        "print(\"üìÅ Escenario 1: Data Lake Tradicional (Parquet)\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Escribimos datos\n",
        "datos_v1 = pd.DataFrame({\n",
        "    'id': [1, 2, 3],\n",
        "    'producto': ['Laptop', 'Mouse', 'Teclado'],\n",
        "    'precio': [1000, 20, 50]\n",
        "})\n",
        "\n",
        "print(\"Versi√≥n 1 de los datos:\")\n",
        "print(datos_v1)\n",
        "print(\"\\n‚ö†Ô∏è Problema: Si actualizamos, perdemos la versi√≥n anterior\")\n",
        "print(\"‚ö†Ô∏è No hay forma de recuperar datos si cometemos un error\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üî∑ Escenario 2: Lakehouse con Delta Lake\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Con Delta Lake (conceptualmente)\n",
        "versiones = {\n",
        "    'v1': datos_v1,\n",
        "    'v2': pd.DataFrame({\n",
        "        'id': [1, 2, 3, 4],\n",
        "        'producto': ['Laptop', 'Mouse', 'Teclado', 'Monitor'],\n",
        "        'precio': [900, 20, 50, 300]  # Laptop con descuento\n",
        "    }),\n",
        "    'v3': pd.DataFrame({\n",
        "        'id': [1, 2, 3, 4, 5],\n",
        "        'producto': ['Laptop', 'Mouse', 'Teclado', 'Monitor', 'Webcam'],\n",
        "        'precio': [900, 20, 50, 300, 80]\n",
        "    })\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Delta Lake mantiene TODAS las versiones:\")\n",
        "for version, df in versiones.items():\n",
        "    print(f\"\\n{version}: {len(df)} registros\")\n",
        "\n",
        "print(\"\\n‚úÖ Puedes volver a cualquier versi√≥n anterior\")\n",
        "print(\"‚úÖ Puedes ver qui√©n hizo cada cambio y cu√°ndo\")\n",
        "print(\"‚úÖ Auditor√≠a completa y compliance garantizado\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 1.2 - Casos de Uso**\n",
        "\n",
        "Identifica qu√© arquitectura es mejor para cada escenario:\n",
        "\n",
        "```markdown\n",
        "### Ejercicio: ¬øWarehouse, Lake, o Lakehouse?\n",
        "\n",
        "Para cada caso, indica la mejor opci√≥n y justifica:\n",
        "\n",
        "**Caso 1**: Startup con presupuesto limitado que necesita an√°lisis b√°sicos de ventas (10K registros/d√≠a)\n",
        "- Soluci√≥n: _______________\n",
        "- Justificaci√≥n: _______________\n",
        "\n",
        "**Caso 2**: Banco que procesa millones de transacciones diarias, necesita detecci√≥n de fraude en tiempo real y cumplimiento regulatorio estricto\n",
        "- Soluci√≥n: _______________\n",
        "- Justificaci√≥n: _______________\n",
        "\n",
        "**Caso 3**: Empresa de e-commerce que necesita recomendaciones personalizadas con ML, an√°lisis de clickstream, y dashboards ejecutivos\n",
        "- Soluci√≥n: _______________\n",
        "- Justificaci√≥n: _______________\n",
        "\n",
        "**Caso 4**: Peque√±a empresa con Excel files que quiere empezar a hacer an√°lisis m√°s avanzados\n",
        "- Soluci√≥n: _______________\n",
        "- Justificaci√≥n: _______________\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **Lakehouse = Data Lake + Data Warehouse** en una sola arquitectura\n",
        "2. **Delta Lake** es la tecnolog√≠a clave que hace posible Lakehouse\n",
        "3. **ACID transactions** garantizan confiabilidad en datos\n",
        "4. **Time Travel** permite auditor√≠a y recuperaci√≥n\n",
        "5. **Una sola copia de datos** reduce costos y complejidad\n",
        "6. **Mismo c√≥digo** para batch y streaming\n",
        "7. **Almacenamiento econ√≥mico** con rendimiento de warehouse\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "- El Lakehouse no es solo una moda, es el futuro de la arquitectura de datos\n",
        "- Empresas est√°n migrando activamente de arquitecturas duales a Lakehouse\n",
        "- Delta Lake es open source, no es exclusivo de Databricks\n",
        "- La inversi√≥n en aprender Lakehouse tiene alto ROI para tu carrera\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el siguiente punto **1.3 Casos de uso: Data Engineering, Data Science, Analytics, ML**?"
      ],
      "metadata": {
        "id": "CvmKHbNMSWST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3 Casos de uso: Data Engineering, Data Science, Analytics, ML**\n",
        "\n",
        "#### **Introducci√≥n: Un Workspace, M√∫ltiples Roles**\n",
        "\n",
        "Databricks es una plataforma **unificada** que sirve a diferentes roles profesionales. Aunque todos trabajan en el mismo entorno, cada rol tiene objetivos y flujos de trabajo distintos.\n",
        "\n",
        "```markdown\n",
        "üè¢ ANTES: Equipos aislados con herramientas diferentes\n",
        "\n",
        "Data Engineers  ‚Üí  Herramienta A (Airflow, Spark on EMR)\n",
        "Data Scientists ‚Üí  Herramienta B (Jupyter, R Studio)  \n",
        "Data Analysts   ‚Üí  Herramienta C (Tableau, Power BI)\n",
        "ML Engineers    ‚Üí  Herramienta D (Kubeflow, SageMaker)\n",
        "\n",
        "‚ùå Problemas:\n",
        "   - No hay colaboraci√≥n efectiva\n",
        "   - Duplicaci√≥n de trabajo\n",
        "   - Versiones inconsistentes de datos\n",
        "   - Handoffs lentos entre equipos\n",
        "\n",
        "üéØ DATABRICKS: Todos en la misma plataforma\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ      DATABRICKS WORKSPACE           ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  Data Engineers  ‚îê                  ‚îÇ\n",
        "‚îÇ  Data Scientists ‚îÇ‚Üí Mismo workspace ‚îÇ\n",
        "‚îÇ  Data Analysts   ‚îÇ‚Üí Mismos datos    ‚îÇ\n",
        "‚îÇ  ML Engineers    ‚îò‚Üí Colaboraci√≥n    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚úÖ Ventajas:\n",
        "   - Colaboraci√≥n en tiempo real\n",
        "   - Datos compartidos y consistentes\n",
        "   - Workflows integrados\n",
        "   - Menor time-to-market\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Caso de Uso 1: Data Engineering (Ingenier√≠a de Datos)**\n",
        "\n",
        "#### **¬øQu√© hace un Data Engineer?**\n",
        "\n",
        "Los Data Engineers son los **arquitectos de datos**. Construyen y mantienen la infraestructura que permite que los datos fluyan desde las fuentes hasta los consumidores.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "RESPONSABILIDADES DE DATA ENGINEERING\n",
        "\"\"\"\n",
        "\n",
        "responsabilidades = {\n",
        "    'ingesta': 'Traer datos desde m√∫ltiples fuentes',\n",
        "    'transformaci√≥n': 'Limpiar, normalizar y enriquecer datos',\n",
        "    'orquestaci√≥n': 'Automatizar pipelines de datos',\n",
        "    'calidad': 'Garantizar precisi√≥n y consistencia',\n",
        "    'optimizaci√≥n': 'Hacer que todo sea r√°pido y eficiente',\n",
        "    'governance': 'Seguridad, auditor√≠a y compliance'\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Ejemplo Pr√°ctico: Pipeline ETL para E-commerce**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CASO REAL: Tienda Online\n",
        "\n",
        "Problema:\n",
        "- Datos de ventas en base de datos transaccional (MySQL)\n",
        "- Logs de clickstream en archivos JSON\n",
        "- Inventario en archivos CSV actualizados cada hora\n",
        "- Necesitan dashboard unificado actualizado cada 15 minutos\n",
        "\n",
        "Soluci√≥n con Databricks:\n",
        "\"\"\"\n",
        "\n",
        "# ===== PASO 1: INGESTA DE DATOS =====\n",
        "\n",
        "# Leer desde MySQL\n",
        "df_ventas = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:mysql://db.empresa.com:3306/ventas\") \\\n",
        "    .option(\"dbtable\", \"transacciones\") \\\n",
        "    .option(\"user\", \"readonly_user\") \\\n",
        "    .option(\"password\", \"secret\") \\\n",
        "    .load()\n",
        "\n",
        "# Leer logs de clickstream (JSON)\n",
        "df_clickstream = spark.read \\\n",
        "    .format(\"json\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"/data/logs/clickstream/*.json\")\n",
        "\n",
        "# Leer inventario (CSV que se actualiza cada hora)\n",
        "df_inventario = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"/data/inventario/stock_*.csv\")\n",
        "\n",
        "print(\"‚úÖ Datos ingestados desde 3 fuentes diferentes\")\n",
        "\n",
        "# ===== PASO 2: TRANSFORMACI√ìN Y LIMPIEZA =====\n",
        "\n",
        "from pyspark.sql.functions import col, when, to_timestamp, current_timestamp\n",
        "\n",
        "# Limpiar ventas\n",
        "df_ventas_clean = df_ventas \\\n",
        "    .filter(col(\"monto\") > 0) \\\n",
        "    .filter(col(\"estado\") == \"completado\") \\\n",
        "    .withColumn(\"fecha\", to_timestamp(col(\"fecha\"))) \\\n",
        "    .dropDuplicates([\"id_transaccion\"])\n",
        "\n",
        "# Enriquecer clickstream con informaci√≥n de sesi√≥n\n",
        "df_clickstream_clean = df_clickstream \\\n",
        "    .withColumn(\"duracion_sesion\",\n",
        "                col(\"timestamp_fin\") - col(\"timestamp_inicio\")) \\\n",
        "    .filter(col(\"duracion_sesion\") > 0)\n",
        "\n",
        "# Normalizar inventario\n",
        "df_inventario_clean = df_inventario \\\n",
        "    .withColumn(\"stock\", col(\"stock\").cast(\"integer\")) \\\n",
        "    .withColumn(\"actualizado_en\", current_timestamp())\n",
        "\n",
        "print(\"‚úÖ Datos transformados y limpiados\")\n",
        "\n",
        "# ===== PASO 3: UNIFICAR DATOS =====\n",
        "\n",
        "# Combinar ventas con inventario\n",
        "df_ventas_inventario = df_ventas_clean.join(\n",
        "    df_inventario_clean,\n",
        "    df_ventas_clean.producto_id == df_inventario_clean.producto_id,\n",
        "    \"left\"\n",
        ")\n",
        "\n",
        "# A√±adir m√©tricas de comportamiento web\n",
        "df_completo = df_ventas_inventario.join(\n",
        "    df_clickstream_clean,\n",
        "    df_ventas_inventario.usuario_id == df_clickstream_clean.usuario_id,\n",
        "    \"left\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Datos unificados en una sola vista\")\n",
        "\n",
        "# ===== PASO 4: ESCRIBIR A DELTA LAKE =====\n",
        "\n",
        "# Guardar en formato Delta para consumo downstream\n",
        "df_completo.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"mergeSchema\", \"true\") \\\n",
        "    .partitionBy(\"fecha\") \\\n",
        "    .save(\"/delta/ventas_unificadas\")\n",
        "\n",
        "print(\"‚úÖ Datos persistidos en Delta Lake\")\n",
        "print(\"‚úÖ Listos para Analytics, ML y Dashboards\")\n",
        "```\n",
        "\n",
        "#### **Ventajas para Data Engineering en Databricks**\n",
        "\n",
        "```python\n",
        "ventajas_data_engineering = {\n",
        "    '1. Auto-scaling':\n",
        "        'Los clusters crecen/reducen autom√°ticamente seg√∫n carga',\n",
        "    \n",
        "    '2. Conectores nativos':\n",
        "        'Integraci√≥n con 100+ fuentes de datos sin configuraci√≥n compleja',\n",
        "    \n",
        "    '3. Delta Lake':\n",
        "        'ACID transactions garantizan calidad de datos',\n",
        "    \n",
        "    '4. Workflows':\n",
        "        'Orquestaci√≥n nativa sin necesidad de Airflow',\n",
        "    \n",
        "    '5. Monitoring':\n",
        "        'Observabilidad completa de pipelines en tiempo real',\n",
        "    \n",
        "    '6. Data Quality':\n",
        "        'Expectations para validar datos autom√°ticamente'\n",
        "}\n",
        "\n",
        "# Ejemplo: Data Quality Check\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def validar_calidad_ventas(df):\n",
        "    \"\"\"Valida que los datos cumplan reglas de negocio\"\"\"\n",
        "    \n",
        "    checks = {\n",
        "        'no_nulos': df.filter(col(\"monto\").isNull()).count() == 0,\n",
        "        'montos_positivos': df.filter(col(\"monto\") <= 0).count() == 0,\n",
        "        'fechas_validas': df.filter(col(\"fecha\") > current_timestamp()).count() == 0\n",
        "    }\n",
        "    \n",
        "    if all(checks.values()):\n",
        "        print(\"‚úÖ Todos los checks de calidad pasaron\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"‚ùå Fallos en calidad de datos:\")\n",
        "        for check, resultado in checks.items():\n",
        "            if not resultado:\n",
        "                print(f\"   - {check}\")\n",
        "        return False\n",
        "\n",
        "# Ejecutar validaci√≥n\n",
        "if validar_calidad_ventas(df_ventas_clean):\n",
        "    # Proceder con el pipeline\n",
        "    pass\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Caso de Uso 2: Data Science (Ciencia de Datos)**\n",
        "\n",
        "#### **¬øQu√© hace un Data Scientist?**\n",
        "\n",
        "Los Data Scientists son los **investigadores**. Exploran datos, descubren patrones, construyen modelos predictivos y extraen insights accionables.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "RESPONSABILIDADES DE DATA SCIENCE\n",
        "\"\"\"\n",
        "\n",
        "responsabilidades = {\n",
        "    'exploraci√≥n': 'An√°lisis exploratorio de datos (EDA)',\n",
        "    'hip√≥tesis': 'Formular y probar hip√≥tesis',\n",
        "    'feature_engineering': 'Crear variables predictivas',\n",
        "    'modelado': 'Construir modelos de ML/estad√≠sticos',\n",
        "    'experimentaci√≥n': 'A/B testing y validaci√≥n',\n",
        "    'comunicaci√≥n': 'Visualizar y presentar hallazgos'\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Ejemplo Pr√°ctico: Predicci√≥n de Churn (Abandono de Clientes)**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CASO REAL: Empresa de Telecomunicaciones\n",
        "\n",
        "Problema:\n",
        "- 20% de clientes cancelan su servicio cada a√±o\n",
        "- Cuesta 5x m√°s adquirir cliente nuevo que retener uno existente\n",
        "- Necesitan identificar clientes en riesgo para retenerlos\n",
        "\n",
        "Soluci√≥n con Databricks:\n",
        "\"\"\"\n",
        "\n",
        "# ===== PASO 1: CARGAR Y EXPLORAR DATOS =====\n",
        "\n",
        "# Leer datos hist√≥ricos de clientes\n",
        "df_clientes = spark.read.format(\"delta\").table(\"clientes_historico\")\n",
        "\n",
        "# Vista r√°pida de los datos\n",
        "display(df_clientes.limit(10))\n",
        "\n",
        "# Estad√≠sticas descriptivas\n",
        "df_clientes.describe().show()\n",
        "\n",
        "# Distribuci√≥n de churn\n",
        "df_clientes.groupBy(\"churn\").count().show()\n",
        "\n",
        "\"\"\"\n",
        "Resultado:\n",
        "+-----+------+\n",
        "|churn| count|\n",
        "+-----+------+\n",
        "|   No|160000|\n",
        "|  Yes| 40000|\n",
        "+-----+------+\n",
        "\n",
        "‚ö†Ô∏è Dataset desbalanceado: 80% No churn, 20% Churn\n",
        "\"\"\"\n",
        "\n",
        "# ===== PASO 2: FEATURE ENGINEERING =====\n",
        "\n",
        "from pyspark.sql.functions import col, datediff, current_date, when, avg\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "\n",
        "# Crear nuevas features\n",
        "df_features = df_clientes \\\n",
        "    .withColumn(\"antiguedad_dias\",\n",
        "                datediff(current_date(), col(\"fecha_alta\"))) \\\n",
        "    .withColumn(\"promedio_consumo_mes\",\n",
        "                col(\"total_consumo\") / col(\"meses_activo\")) \\\n",
        "    .withColumn(\"llamadas_servicio_cliente\",\n",
        "                col(\"tickets_soporte\").cast(\"integer\")) \\\n",
        "    .withColumn(\"tiene_plan_premium\",\n",
        "                when(col(\"plan_tipo\") == \"premium\", 1).otherwise(0))\n",
        "\n",
        "# Codificar variables categ√≥ricas\n",
        "indexer = StringIndexer(inputCol=\"region\", outputCol=\"region_idx\")\n",
        "df_encoded = indexer.fit(df_features).transform(df_features)\n",
        "\n",
        "# Seleccionar features para el modelo\n",
        "feature_cols = [\n",
        "    \"antiguedad_dias\",\n",
        "    \"promedio_consumo_mes\",\n",
        "    \"llamadas_servicio_cliente\",\n",
        "    \"tiene_plan_premium\",\n",
        "    \"region_idx\",\n",
        "    \"edad\",\n",
        "    \"num_servicios_contratados\"\n",
        "]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "df_model = assembler.transform(df_encoded)\n",
        "\n",
        "print(\"‚úÖ Features creadas y preparadas\")\n",
        "\n",
        "# ===== PASO 3: ENTRENAR MODELO =====\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Preparar label\n",
        "indexer_label = StringIndexer(inputCol=\"churn\", outputCol=\"label\")\n",
        "df_model = indexer_label.fit(df_model).transform(df_model)\n",
        "\n",
        "# Split train/test\n",
        "train_df, test_df = df_model.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Entrenar modelo\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    numTrees=100,\n",
        "    maxDepth=10\n",
        ")\n",
        "\n",
        "modelo = rf.fit(train_df)\n",
        "\n",
        "print(\"‚úÖ Modelo entrenado con Random Forest\")\n",
        "\n",
        "# ===== PASO 4: EVALUAR MODELO =====\n",
        "\n",
        "# Predicciones\n",
        "predictions = modelo.transform(test_df)\n",
        "\n",
        "# M√©tricas\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "print(f\"üìä AUC-ROC: {auc:.3f}\")\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "predictions.groupBy(\"label\", \"prediction\").count().show()\n",
        "\n",
        "\"\"\"\n",
        "Resultado:\n",
        "+-----+----------+-----+\n",
        "|label|prediction|count|\n",
        "+-----+----------+-----+\n",
        "|  0.0|       0.0|31500|\n",
        "|  0.0|       1.0|  500|\n",
        "|  1.0|       0.0| 1200|\n",
        "|  1.0|       1.0| 6800|\n",
        "+-----+----------+-----+\n",
        "\n",
        "‚úÖ 85% de clientes en riesgo identificados correctamente\n",
        "‚úÖ Falsos positivos: 500 (aceptable para negocio)\n",
        "\"\"\"\n",
        "\n",
        "# ===== PASO 5: FEATURE IMPORTANCE =====\n",
        "\n",
        "# ¬øQu√© variables son m√°s importantes?\n",
        "import pandas as pd\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': modelo.featureImportances.toArray()\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nüìä Variables m√°s importantes para predecir churn:\")\n",
        "print(feature_importance)\n",
        "\n",
        "\"\"\"\n",
        "Resultado:\n",
        "                    feature  importance\n",
        "1     promedio_consumo_mes      0.3200\n",
        "2  llamadas_servicio_cliente    0.2800\n",
        "0            antiguedad_dias      0.1500\n",
        "6 num_servicios_contratados      0.1200\n",
        "3         tiene_plan_premium      0.0900\n",
        "4                region_idx      0.0300\n",
        "5                      edad      0.0100\n",
        "\n",
        "üí° Insight: El consumo promedio y las llamadas a soporte\n",
        "   son los mejores predictores de churn\n",
        "\"\"\"\n",
        "\n",
        "# ===== PASO 6: GUARDAR MODELO =====\n",
        "\n",
        "# MLflow tracking autom√°tico en Databricks\n",
        "import mlflow\n",
        "\n",
        "with mlflow.start_run(run_name=\"churn_model_v1\"):\n",
        "    mlflow.log_param(\"num_trees\", 100)\n",
        "    mlflow.log_param(\"max_depth\", 10)\n",
        "    mlflow.log_metric(\"auc\", auc)\n",
        "    mlflow.spark.log_model(modelo, \"model\")\n",
        "    \n",
        "print(\"‚úÖ Modelo versionado y registrado en MLflow\")\n",
        "```\n",
        "\n",
        "#### **Ventajas para Data Science en Databricks**\n",
        "\n",
        "```python\n",
        "ventajas_data_science = {\n",
        "    '1. Escalabilidad':\n",
        "        'Entrena modelos con millones/billones de registros sin problemas',\n",
        "    \n",
        "    '2. MLflow integrado':\n",
        "        'Tracking, versionado y deployment de modelos autom√°tico',\n",
        "    \n",
        "    '3. Notebooks colaborativos':\n",
        "        'Varios data scientists trabajando en el mismo an√°lisis',\n",
        "    \n",
        "    '4. Feature Store':\n",
        "        'Reutilizar features entre proyectos y equipos',\n",
        "    \n",
        "    '5. AutoML':\n",
        "        'Genera modelos baseline autom√°ticamente',\n",
        "    \n",
        "    '6. GPU clusters':\n",
        "        'Entrenar deep learning models con GPUs'\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Caso de Uso 3: Analytics (Anal√≠tica de Negocio)**\n",
        "\n",
        "#### **¬øQu√© hace un Data Analyst?**\n",
        "\n",
        "Los Data Analysts son los **traductores de datos**. Convierten datos en insights accionables para decisiones de negocio.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "RESPONSABILIDADES DE DATA ANALYTICS\n",
        "\"\"\"\n",
        "\n",
        "responsabilidades = {\n",
        "    'reporting': 'Crear reportes regulares para stakeholders',\n",
        "    'dashboards': 'Visualizaciones interactivas',\n",
        "    'ad_hoc': 'Responder preguntas de negocio espec√≠ficas',\n",
        "    'kpis': 'Monitorear m√©tricas clave del negocio',\n",
        "    'sql': 'Consultas SQL para extraer insights',\n",
        "    'presentaciones': 'Comunicar hallazgos a audiencias no t√©cnicas'\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Ejemplo Pr√°ctico: Dashboard Ejecutivo de Ventas**\n",
        "\n",
        "```sql\n",
        "-- Celda SQL en Databricks\n",
        "\n",
        "-- ===== AN√ÅLISIS 1: TENDENCIA DE VENTAS =====\n",
        "\n",
        "SELECT\n",
        "    DATE_TRUNC('month', fecha) AS mes,\n",
        "    COUNT(DISTINCT id_transaccion) AS num_transacciones,\n",
        "    SUM(monto) AS ingresos_totales,\n",
        "    AVG(monto) AS ticket_promedio,\n",
        "    COUNT(DISTINCT cliente_id) AS clientes_unicos\n",
        "FROM ventas_unificadas\n",
        "WHERE fecha >= DATE_SUB(CURRENT_DATE(), 365)\n",
        "GROUP BY mes\n",
        "ORDER BY mes;\n",
        "\n",
        "/*\n",
        "Resultado:\n",
        "+----------+------------------+----------------+----------------+-----------------+\n",
        "|      mes |num_transacciones |ingresos_totales|ticket_promedio |clientes_unicos  |\n",
        "+----------+------------------+----------------+----------------+-----------------+\n",
        "|2025-02-01|           125,000|      $5,250,000|          $42.00|           45,000|\n",
        "|2025-03-01|           138,000|      $5,980,000|          $43.33|           48,500|\n",
        "|2025-04-01|           145,000|      $6,380,000|          $44.00|           51,200|\n",
        "+----------+------------------+----------------+----------------+-----------------+\n",
        "\n",
        "üìà Tendencia positiva: Crecimiento 16% en transacciones\n",
        "üìà Ticket promedio aument√≥ 5%\n",
        "*/\n",
        "```\n",
        "\n",
        "```sql\n",
        "-- ===== AN√ÅLISIS 2: TOP PRODUCTOS =====\n",
        "\n",
        "SELECT\n",
        "    producto_nombre,\n",
        "    categoria,\n",
        "    COUNT(*) AS unidades_vendidas,\n",
        "    SUM(monto) AS ingresos,\n",
        "    ROUND(SUM(monto) / SUM(SUM(monto)) OVER () * 100, 2) AS porcentaje_ingresos\n",
        "FROM ventas_unificadas\n",
        "WHERE fecha >= DATE_SUB(CURRENT_DATE(), 30)\n",
        "GROUP BY producto_nombre, categoria\n",
        "ORDER BY ingresos DESC\n",
        "LIMIT 10;\n",
        "\n",
        "/*\n",
        "Resultado:\n",
        "+------------------+-----------+------------------+-----------+--------------------+\n",
        "|producto_nombre   |categoria  |unidades_vendidas |ingresos   |porcentaje_ingresos |\n",
        "+------------------+-----------+------------------+-----------+--------------------+\n",
        "|iPhone 15 Pro     |Electr√≥nica|             2,500|$2,500,000 |              15.50%|\n",
        "|MacBook Air M3    |Electr√≥nica|             1,200|$1,440,000 |               8.93%|\n",
        "|AirPods Pro 2     |Accesorios |             8,500|  $850,000 |               5.27%|\n",
        "+------------------+-----------+------------------+-----------+--------------------+\n",
        "\n",
        "üí° Insight: Top 3 productos generan 30% de ingresos totales\n",
        "üí° Categor√≠a Electr√≥nica domina con 45% de ventas\n",
        "*/\n",
        "```\n",
        "\n",
        "```sql\n",
        "-- ===== AN√ÅLISIS 3: AN√ÅLISIS DE COHORTES =====\n",
        "\n",
        "WITH cohortes AS (\n",
        "    SELECT\n",
        "        cliente_id,\n",
        "        DATE_TRUNC('month', MIN(fecha)) AS mes_primera_compra\n",
        "    FROM ventas_unificadas\n",
        "    GROUP BY cliente_id\n",
        "),\n",
        "actividad AS (\n",
        "    SELECT\n",
        "        v.cliente_id,\n",
        "        c.mes_primera_compra,\n",
        "        DATE_TRUNC('month', v.fecha) AS mes_actividad,\n",
        "        DATEDIFF(MONTH, c.mes_primera_compra, v.fecha) AS meses_desde_primera_compra\n",
        "    FROM ventas_unificadas v\n",
        "    JOIN cohortes c ON v.cliente_id = c.cliente_id\n",
        ")\n",
        "SELECT\n",
        "    mes_primera_compra AS cohorte,\n",
        "    meses_desde_primera_compra,\n",
        "    COUNT(DISTINCT cliente_id) AS clientes_activos,\n",
        "    ROUND(COUNT(DISTINCT cliente_id) * 100.0 /\n",
        "          FIRST_VALUE(COUNT(DISTINCT cliente_id)) OVER (\n",
        "              PARTITION BY mes_primera_compra\n",
        "              ORDER BY meses_desde_primera_compra\n",
        "          ), 2) AS retencion_porcentaje\n",
        "FROM actividad\n",
        "WHERE meses_desde_primera_compra <= 12\n",
        "GROUP BY mes_primera_compra, meses_desde_primera_compra\n",
        "ORDER BY mes_primera_compra, meses_desde_primera_compra;\n",
        "\n",
        "/*\n",
        "Resultado (simplificado):\n",
        "+-----------------+-------------------------------+------------------+---------------------+\n",
        "|cohorte          |meses_desde_primera_compra     |clientes_activos  |retencion_porcentaje |\n",
        "+-----------------+-------------------------------+------------------+---------------------+\n",
        "|2025-01-01       |                             0 |           10,000 |              100.00%|\n",
        "|2025-01-01       |                             1 |            7,500 |               75.00%|\n",
        "|2025-01-01       |                             2 |            6,200 |               62.00%|\n",
        "|2025-01-01       |                             3 |            5,500 |               55.00%|\n",
        "+-----------------+-------------------------------+------------------+---------------------+\n",
        "\n",
        "‚ö†Ô∏è Problema identificado: 45% de clientes se pierden en 3 meses\n",
        "üí° Acci√≥n: Implementar programa de retenci√≥n temprana\n",
        "*/\n",
        "```\n",
        "\n",
        "#### **Crear Dashboard Interactivo**\n",
        "\n",
        "```python\n",
        "# Celda Python para visualizaci√≥n\n",
        "\n",
        "from pyspark.sql.functions import col, sum, avg, count, month, year\n",
        "\n",
        "# Cargar datos\n",
        "df_ventas = spark.read.format(\"delta\").table(\"ventas_unificadas\")\n",
        "\n",
        "# KPIs principales\n",
        "kpis = df_ventas.agg(\n",
        "    count(\"id_transaccion\").alias(\"total_transacciones\"),\n",
        "    sum(\"monto\").alias(\"ingresos_totales\"),\n",
        "    avg(\"monto\").alias(\"ticket_promedio\"),\n",
        "    count(col(\"cliente_id\").distinct()).alias(\"clientes_unicos\")\n",
        ").collect()[0]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üìä DASHBOARD EJECUTIVO - RESUMEN DEL MES\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üí∞ Ingresos Totales:      ${kpis['ingresos_totales']:,.2f}\")\n",
        "print(f\"üõí Transacciones:         {kpis['total_transacciones']:,}\")\n",
        "print(f\"üé´ Ticket Promedio:       ${kpis['ticket_promedio']:.2f}\")\n",
        "print(f\"üë• Clientes √önicos:       {kpis['clientes_unicos']:,}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Databricks tiene visualizaciones nativas\n",
        "# display() crea gr√°ficos autom√°ticamente\n",
        "display(df_ventas.groupBy(month(\"fecha\").alias(\"mes\"))\n",
        "        .agg(sum(\"monto\").alias(\"ingresos\"))\n",
        "        .orderBy(\"mes\"))\n",
        "```\n",
        "\n",
        "#### **Ventajas para Analytics en Databricks**\n",
        "\n",
        "```python\n",
        "ventajas_analytics = {\n",
        "    '1. SQL Warehouses':\n",
        "        'Consultas SQL optimizadas con cach√© inteligente',\n",
        "    \n",
        "    '2. Dashboards nativos':\n",
        "        'Crear visualizaciones sin herramientas externas',\n",
        "    \n",
        "    '3. Consultas sobre Delta':\n",
        "        'Acceso a datos frescos y consistentes (no ETL adicional)',\n",
        "    \n",
        "    '4. Par√°metros y widgets':\n",
        "        'Dashboards interactivos con filtros din√°micos',\n",
        "    \n",
        "    '5. Programaci√≥n autom√°tica':\n",
        "        'Reportes que se actualizan solos',\n",
        "    \n",
        "    '6. Compartir f√°cilmente':\n",
        "        'URLs p√∫blicas o embeds en otras apps'\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Caso de Uso 4: Machine Learning Engineering**\n",
        "\n",
        "#### **¬øQu√© hace un ML Engineer?**\n",
        "\n",
        "Los ML Engineers son los **operadores de modelos**. Llevan modelos desde notebooks experimentales a producci√≥n a escala.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "RESPONSABILIDADES DE ML ENGINEERING\n",
        "\"\"\"\n",
        "\n",
        "responsabilidades = {\n",
        "    'productionizaci√≥n': 'Desplegar modelos en producci√≥n',\n",
        "    'mlops': 'CI/CD para modelos de ML',\n",
        "    'monitoring': 'Detectar data drift y model decay',\n",
        "    'retraining': 'Automatizar reentrenamiento de modelos',\n",
        "    'serving': 'APIs de inferencia escalables',\n",
        "    'governance': 'Versionado y auditor√≠a de modelos'\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Ejemplo Pr√°ctico: Despliegue de Modelo de Recomendaciones**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CASO REAL: Sistema de Recomendaciones en Producci√≥n\n",
        "\n",
        "Problema:\n",
        "- Data Scientists crearon modelo de recomendaci√≥n de productos\n",
        "- Necesita servir predicciones a 10,000 requests/segundo\n",
        "- Debe actualizarse diariamente con nuevos datos\n",
        "- Requiere monitoreo de performance en tiempo real\n",
        "\n",
        "Soluci√≥n con Databricks ML:\n",
        "\"\"\"\n",
        "\n",
        "# ===== PASO 1: REGISTRAR MODELO EN MLFLOW =====\n",
        "\n",
        "import mlflow\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "# El modelo ya fue entrenado por Data Science\n",
        "# Ahora lo registramos en Model Registry\n",
        "\n",
        "model_name = \"recomendador_productos\"\n",
        "model_uri = \"runs:/<run_id>/model\"  # Del experimento anterior\n",
        "\n",
        "# Registrar modelo\n",
        "mlflow.register_model(model_uri, model_name)\n",
        "\n",
        "# Promover a producci√≥n\n",
        "client = MlflowClient()\n",
        "client.transition_model_version_stage(\n",
        "    name=model_name,\n",
        "    version=1,\n",
        "    stage=\"Production\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Modelo '{model_name}' v1 en Producci√≥n\")\n",
        "\n",
        "# ===== PASO 2: CREAR ENDPOINT DE INFERENCIA =====\n",
        "\n",
        "\"\"\"\n",
        "En Databricks UI:\n",
        "1. Ir a \"Machine Learning\" ‚Üí \"Serving\"\n",
        "2. Crear endpoint con el modelo registrado\n",
        "3. Configurar auto-scaling\n",
        "\n",
        "Resultado: API REST lista para uso\n",
        "\"\"\"\n",
        "\n",
        "# Ejemplo de uso del endpoint\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# URL del endpoint (generada autom√°ticamente)\n",
        "endpoint_url = \"https://workspace.cloud.databricks.com/serving-endpoints/recomendador/invocations\"\n",
        "\n",
        "# Token de autenticaci√≥n\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {databricks_token}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Request de inferencia\n",
        "payload = {\n",
        "    \"dataframe_records\": [\n",
        "        {\n",
        "            \"usuario_id\": 12345,\n",
        "            \"categoria_preferida\": \"electronica\",\n",
        "            \"historial_compras\": [101, 205, 389]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Llamar al endpoint\n",
        "response = requests.post(endpoint_url, headers=headers, json=payload)\n",
        "recomendaciones = response.json()\n",
        "\n",
        "print(\"üéØ Recomendaciones para usuario 12345:\")\n",
        "print(recomendaciones)\n",
        "\n",
        "\"\"\"\n",
        "Resultado:\n",
        "{\n",
        "  \"predictions\": [\n",
        "    {\"producto_id\": 450, \"score\": 0.92, \"nombre\": \"iPhone 15\"},\n",
        "    {\"producto_id\": 328, \"score\": 0.87, \"nombre\": \"AirPods Pro\"},\n",
        "    {\"producto_id\": 562, \"score\": 0.81, \"nombre\": \"Apple Watch\"}\n",
        "  ]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# ===== PASO 3: BATCH INFERENCE (ALTERNATIVA) =====\n",
        "\n",
        "# Para casos donde no necesitas real-time\n",
        "\n",
        "# Cargar modelo de producci√≥n\n",
        "modelo_prod = mlflow.pyfunc.load_model(f\"models:/{model_name}/Production\")\n",
        "\n",
        "# Leer usuarios que necesitan recomendaciones\n",
        "df_usuarios = spark.read.format(\"delta\").table(\"usuarios_activos\")\n",
        "\n",
        "# Generar recomendaciones en batch\n",
        "df_recomendaciones = modelo_prod.predict(df_usuarios)\n",
        "\n",
        "# Guardar resultados\n",
        "df_recomendaciones.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"/delta/recomendaciones_diarias\")\n",
        "\n",
        "print(\"‚úÖ Recomendaciones generadas para 500K usuarios en 5 minutos\")\n",
        "\n",
        "# ===== PASO 4: MONITOREO DEL MODELO =====\n",
        "\n",
        "from databricks.feature_engineering import FeatureEngineeringClient\n",
        "\n",
        "# Feature Store tracking\n",
        "fe = FeatureEngineeringClient()\n",
        "\n",
        "# Log inference data para monitoring\n",
        "inference_log = spark.read.format(\"delta\").table(\"inference_logs\")\n",
        "\n",
        "# Detectar data drift\n",
        "from scipy import stats\n",
        "\n",
        "# Comparar distribuci√≥n de features en training vs producci√≥n\n",
        "feature_col = \"edad_usuario\"\n",
        "\n",
        "train_dist = spark.read.format(\"delta\").table(\"training_data\") \\\n",
        "    .select(feature_col).toPandas()\n",
        "\n",
        "prod_dist = inference_log.select(feature_col) \\\n",
        "    .limit(10000).toPandas()\n",
        "\n",
        "# Test de Kolmogorov-Smirnov\n",
        "ks_statistic, p_value = stats.ks_2samp(\n",
        "    train_dist[feature_col],\n",
        "    prod_dist[feature_col]\n",
        ")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(f\"‚ö†Ô∏è DATA DRIFT detectado en {feature_col}\")\n",
        "    print(f\"   KS statistic: {ks_statistic:.3f}, p-value: {p_value:.4f}\")\n",
        "    print(\"   ‚Üí Considerar reentrenar modelo\")\n",
        "else:\n",
        "    print(f\"‚úÖ Distribuci√≥n de {feature_col} estable\")\n",
        "\n",
        "# ===== PASO 5: REENTRENAMIENTO AUTOMATIZADO =====\n",
        "\n",
        "# Job programado que corre diariamente\n",
        "def retrain_model():\n",
        "    \"\"\"\n",
        "    Este c√≥digo corre autom√°ticamente cada noche\n",
        "    \"\"\"\n",
        "    from datetime import datetime\n",
        "    \n",
        "    # Cargar datos frescos\n",
        "    df_train_new = spark.read.format(\"delta\") \\\n",
        "        .table(\"ventas_unificadas\") \\\n",
        "        .filter(col(\"fecha\") >= datetime.now() - timedelta(days=90))\n",
        "    \n",
        "    # Reentrenar modelo\n",
        "    nuevo_modelo = entrenar_modelo(df_train_new)\n",
        "    \n",
        "    # Evaluar contra modelo actual\n",
        "    metricas_actual = evaluar_modelo(modelo_prod, test_set)\n",
        "    metricas_nuevo = evaluar_modelo(nuevo_modelo, test_set)\n",
        "    \n",
        "    # Solo promover si mejora\n",
        "    if metricas_nuevo['auc'] > metricas_actual['auc']:\n",
        "        # Registrar nuevo modelo\n",
        "        mlflow.spark.log_model(nuevo_modelo, \"model\")\n",
        "        \n",
        "        # Promover a staging primero\n",
        "        client.transition_model_version_stage(\n",
        "            name=model_name,\n",
        "            version=2,\n",
        "            stage=\"Staging\"\n",
        "        )\n",
        "        \n",
        "        print(\"‚úÖ Nuevo modelo en Staging para testing\")\n",
        "        # Despu√©s de validaci√≥n manual ‚Üí Producci√≥n\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è Modelo actual sigue siendo mejor, no se actualiza\")\n",
        "\n",
        "# Programar con Databricks Jobs (en UI):\n",
        "# Frequency: Daily at 2 AM\n",
        "# Cluster: Job cluster (costo-eficiente)\n",
        "# Alerts: Email si falla\n",
        "```\n",
        "\n",
        "#### **Ventajas para ML Engineering en Databricks**\n",
        "\n",
        "```python\n",
        "ventajas_ml_engineering = {\n",
        "    '1. MLflow integrado':\n",
        "        'Versionado, registry, serving todo en uno',\n",
        "    \n",
        "    '2. Model Serving':\n",
        "        'Endpoints auto-escalables sin configurar infraestructura',\n",
        "    \n",
        "    '3. Feature Store':\n",
        "        'Features reutilizables y consistentes train/serve',\n",
        "    \n",
        "    '4. Monitoring':\n",
        "        'Detecci√≥n autom√°tica de drift y degradaci√≥n',\n",
        "    \n",
        "    '5. CI/CD':\n",
        "        'Integraci√≥n con Git, testing automatizado',\n",
        "    \n",
        "    '6. Governance':\n",
        "        'Auditor√≠a completa de modelos en producci√≥n'\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparaci√≥n de Roles: Flujo de Trabajo Completo**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "PROYECTO REAL: Sistema de Detecci√≥n de Fraude\n",
        "\n",
        "Veamos c√≥mo los 4 roles colaboran en Databricks:\n",
        "\"\"\"\n",
        "\n",
        "# ====== ROL 1: DATA ENGINEER ======\n",
        "print(\"üë∑ DATA ENGINEER:\")\n",
        "print(\"- Construye pipeline de ingesta desde sistemas transaccionales\")\n",
        "print(\"- Limpia y normaliza datos de transacciones\")\n",
        "print(\"- Crea tabla Delta 'transacciones_limpias'\")\n",
        "print(\"- Programa Job para actualizar cada 15 minutos\")\n",
        "print()\n",
        "\n",
        "# C√≥digo del Data Engineer\n",
        "df_transacciones = spark.read.jdbc(...)  # Leer desde DB\n",
        "df_clean = limpiar_datos(df_transacciones)\n",
        "df_clean.write.format(\"delta\").mode(\"append\").save(\"/delta/transacciones\")\n",
        "\n",
        "# ====== ROL 2: DATA SCIENTIST ======\n",
        "print(\"üî¨ DATA SCIENTIST:\")\n",
        "print(\"- Lee tabla 'transacciones_limpias'\")\n",
        "print(\"- An√°lisis exploratorio: patrones de fraude\")\n",
        "print(\"- Crea features: monto_promedio, frecuencia, ubicaci√≥n inusual\")\n",
        "print(\"- Entrena modelo XGBoost\")\n",
        "print(\"- Registra modelo en MLflow\")\n",
        "print()\n",
        "\n",
        "# C√≥digo del Data Scientist\n",
        "df_data = spark.read.format(\"delta\").table(\"transacciones_limpias\")\n",
        "df_features = feature_engineering(df_data)\n",
        "modelo = entrenar_xgboost(df_features)\n",
        "mlflow.sklearn.log_model(modelo, \"fraud_detector\")\n",
        "\n",
        "# ====== ROL 3: ML ENGINEER ======\n",
        "print(\"‚öôÔ∏è ML ENGINEER:\")\n",
        "print(\"- Toma modelo del Data Scientist\")\n",
        "print(\"- Crea endpoint de serving\")\n",
        "print(\"- Implementa monitoreo de performance\")\n",
        "print(\"- Configura reentrenamiento semanal\")\n",
        "print(\"- Establece alertas de degradaci√≥n\")\n",
        "print()\n",
        "\n",
        "# C√≥digo del ML Engineer\n",
        "mlflow.register_model(\"runs:/.../model\", \"fraud_detector\")\n",
        "# Crear endpoint via UI\n",
        "# Configurar monitoring y retraining job\n",
        "\n",
        "# ====== ROL 4: DATA ANALYST ======\n",
        "print(\"üìä DATA ANALYST:\")\n",
        "print(\"- Crea dashboard de fraude detectado\")\n",
        "print(\"- Reportes diarios para equipo de riesgos\")\n",
        "print(\"- KPIs: tasa de fraude, falsos positivos, dinero ahorrado\")\n",
        "print(\"- An√°lisis de tendencias de fraude por regi√≥n/tiempo\")\n",
        "print()\n",
        "\n",
        "# C√≥digo del Data Analyst (SQL)\n",
        "\"\"\"\n",
        "SELECT\n",
        "    fecha,\n",
        "    COUNT(*) AS transacciones_totales,\n",
        "    SUM(CASE WHEN fraude_detectado = 1 THEN 1 ELSE 0 END) AS fraudes,\n",
        "    SUM(CASE WHEN fraude_detectado = 1 THEN monto ELSE 0 END) AS dinero_protegido\n",
        "FROM predicciones_fraude\n",
        "GROUP BY fecha\n",
        "ORDER BY fecha DESC;\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÖ RESULTADO FINAL:\")\n",
        "print(\"   Sistema de detecci√≥n de fraude en producci√≥n\")\n",
        "print(\"   Protegiendo $500K diarios\")\n",
        "print(\"   Todos los roles colaborando en UN SOLO WORKSPACE\")\n",
        "print(\"=\"*60)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Simulando Roles**\n",
        "\n",
        "```python\n",
        "# Ejercicio conceptual: Identifica el rol apropiado\n",
        "\n",
        "casos = [\n",
        "    {\n",
        "        'tarea': 'Construir pipeline que ingesta datos de 5 APIs diferentes cada hora',\n",
        "        'rol': '___________',\n",
        "        'herramientas': '___________'\n",
        "    },\n",
        "    {\n",
        "        'tarea': 'Analizar por qu√© las ventas cayeron 15% en el √∫ltimo trimestre',\n",
        "        'rol': '___________',\n",
        "        'herramientas': '___________'\n",
        "    },\n",
        "    {\n",
        "        'tarea': 'Crear modelo que prediga demanda de productos para los pr√≥ximos 30 d√≠as',\n",
        "        'rol': '___________',\n",
        "        'herramientas': '___________'\n",
        "    },\n",
        "    {\n",
        "        'tarea': 'Desplegar modelo de recomendaciones que sirva 50K requests/min',\n",
        "        'rol': '___________',\n",
        "        'herramientas': '___________'\n",
        "    },\n",
        "    {\n",
        "        'tarea': 'Detectar que el modelo en producci√≥n est√° degrad√°ndose y necesita reentrenamiento',\n",
        "        'rol': '___________',\n",
        "        'herramientas': '___________'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Respuestas:\n",
        "# 1. Data Engineer | Workflows, Delta Lake, Autoloader\n",
        "# 2. Data Analyst | SQL, Dashboards, Visualizaciones\n",
        "# 3. Data Scientist | PySpark ML, MLflow, Feature Store\n",
        "# 4. ML Engineer | Model Serving, Endpoints, Auto-scaling\n",
        "# 5. ML Engineer | Monitoring, MLflow, Retraining Jobs\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 1.3 - Caso Pr√°ctico**\n",
        "\n",
        "```markdown\n",
        "### Proyecto: Sistema de An√°lisis de Sentimiento para Reviews\n",
        "\n",
        "Una empresa de e-commerce recibe 100K reviews diarias de productos.\n",
        "Necesitan automatizar el an√°lisis de sentimiento.\n",
        "\n",
        "**Tu tarea**: Describe c√≥mo cada rol contribuir√≠a al proyecto\n",
        "\n",
        "**Data Engineer**:\n",
        "- Tareas: ___________\n",
        "- Entregables: ___________\n",
        "\n",
        "**Data Scientist**:\n",
        "- Tareas: ___________\n",
        "- Entregables: ___________\n",
        "\n",
        "**ML Engineer**:\n",
        "- Tareas: ___________\n",
        "- Entregables: ___________\n",
        "\n",
        "**Data Analyst**:\n",
        "- Tareas: ___________\n",
        "- Entregables: ___________\n",
        "\n",
        "**Bonus**: ¬øEn qu√© orden trabajar√≠an? ¬øC√≥mo colaborar√≠an?\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **Databricks unifica** 4 roles en una plataforma\n",
        "2. **Data Engineers** construyen la infraestructura de datos\n",
        "3. **Data Scientists** crean modelos y extraen insights\n",
        "4. **ML Engineers** operan modelos en producci√≥n\n",
        "5. **Data Analysts** traducen datos en decisiones de negocio\n",
        "6. **Colaboraci√≥n fluida** sin handoffs entre herramientas\n",
        "7. **Todos usan Delta Lake** como fuente √∫nica de verdad\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "- En equipos peque√±os, una persona puede hacer varios roles\n",
        "- En Databricks, cambiar entre roles es natural (mismo workspace)\n",
        "- La l√≠nea entre roles se difumina - colaboraci√≥n > silos\n",
        "- Aprende los fundamentos de todos los roles para mejor colaboraci√≥n\n",
        "- Las empresas buscan perfiles \"T-shaped\": profundo en uno, amplio en varios\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el siguiente punto **1.4 Ecosistema: Relaci√≥n con Apache Spark y la nube**?"
      ],
      "metadata": {
        "id": "Cn3XPS3dTluD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.4 Ecosistema: Relaci√≥n con Apache Spark y la nube**\n",
        "\n",
        "#### **Introducci√≥n: Databricks no existe en el vac√≠o**\n",
        "\n",
        "Databricks es parte de un **ecosistema m√°s amplio** de tecnolog√≠as de Big Data y Cloud Computing. Entender c√≥mo se relaciona con Apache Spark y los proveedores cloud es fundamental para aprovechar todo su potencial.\n",
        "\n",
        "```markdown\n",
        "üåç EL ECOSISTEMA DE DATABRICKS\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ            CLOUD PROVIDERS                  ‚îÇ\n",
        "‚îÇ   [AWS]    [Azure]    [Google Cloud]        ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                   ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ         DATABRICKS PLATFORM                 ‚îÇ\n",
        "‚îÇ  (Capa de gesti√≥n y optimizaci√≥n)           ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                   ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ          APACHE SPARK                       ‚îÇ\n",
        "‚îÇ  (Motor de procesamiento distribuido)       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                   ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ      ALMACENAMIENTO (S3, ADLS, GCS)         ‚îÇ\n",
        "‚îÇ         + DELTA LAKE                        ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 1: Apache Spark - El Motor Detr√°s de Databricks**\n",
        "\n",
        "#### **¬øQu√© es Apache Spark?**\n",
        "\n",
        "**Apache Spark** es un motor de procesamiento de datos distribuido open-source creado en 2009 en UC Berkeley (por los mismos fundadores de Databricks).\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "APACHE SPARK EN POCAS PALABRAS\n",
        "\"\"\"\n",
        "\n",
        "caracteristicas_spark = {\n",
        "    'velocidad': 'Hasta 100x m√°s r√°pido que Hadoop MapReduce',\n",
        "    'procesamiento_en_memoria': 'Datos en RAM para operaciones r√°pidas',\n",
        "    'distribuido': 'Procesamiento paralelo en m√∫ltiples m√°quinas',\n",
        "    'unificado': 'Batch, Streaming, SQL, ML, Graphs en un solo framework',\n",
        "    'multi_lenguaje': 'APIs en Python, Scala, Java, R, SQL',\n",
        "    'open_source': 'Gratis y con comunidad activa'\n",
        "}\n",
        "\n",
        "# Spark es el \"motor\" que hace posible procesar\n",
        "# billones de registros en segundos\n",
        "```\n",
        "\n",
        "#### **¬øPor qu√© Spark es revolucionario?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ANTES DE SPARK: Hadoop MapReduce (2006-2014)\n",
        "\"\"\"\n",
        "\n",
        "# Problema: Lent√≠simo\n",
        "hadoop_ejemplo = \"\"\"\n",
        "1. Leer datos del disco ‚Üí Procesar ‚Üí Escribir al disco\n",
        "2. Leer del disco ‚Üí Procesar ‚Üí Escribir al disco\n",
        "3. Leer del disco ‚Üí Procesar ‚Üí Escribir al disco\n",
        "...\n",
        "‚è±Ô∏è Cada paso tarda minutos/horas\n",
        "‚ùå Total: Horas o d√≠as para an√°lisis complejo\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "CON SPARK: Procesamiento en Memoria (2014-presente)\n",
        "\"\"\"\n",
        "\n",
        "spark_ejemplo = \"\"\"\n",
        "1. Cargar datos en memoria (RAM)\n",
        "2. Procesar todo en memoria\n",
        "3. Solo escribir resultado final al disco\n",
        "\n",
        "‚è±Ô∏è Total: Minutos o segundos\n",
        "‚úÖ 10-100x m√°s r√°pido que Hadoop\n",
        "\"\"\"\n",
        "\n",
        "# Ejemplo visual de la diferencia\n",
        "print(\"üêå HADOOP MapReduce:\")\n",
        "print(\"Disco ‚Üí CPU ‚Üí Disco ‚Üí CPU ‚Üí Disco ‚Üí CPU ‚Üí Disco\")\n",
        "print(\"‚è±Ô∏è  Tiempo: 4 horas\")\n",
        "print()\n",
        "print(\"‚ö° APACHE SPARK:\")\n",
        "print(\"Disco ‚Üí RAM ‚Üí CPU ‚Üí CPU ‚Üí CPU ‚Üí Disco\")\n",
        "print(\"‚è±Ô∏è  Tiempo: 15 minutos\")\n",
        "```\n",
        "\n",
        "#### **Componentes de Apache Spark**\n",
        "\n",
        "```markdown\n",
        "üì¶ STACK DE APACHE SPARK\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ         APLICACIONES Y APIS                 ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  [Python]  [Scala]  [Java]  [R]  [SQL]      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                   ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ        BIBLIOTECAS ESPECIALIZADAS           ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Spark SQL ‚îÇ MLlib ‚îÇ Spark   ‚îÇ GraphX        ‚îÇ\n",
        "‚îÇ (SQL y    ‚îÇ (ML)  ‚îÇ Streaming‚îÇ (Grafos)     ‚îÇ\n",
        "‚îÇ  DataFrames)‚îÇ      ‚îÇ (Tiempo  ‚îÇ              ‚îÇ\n",
        "‚îÇ            ‚îÇ       ‚îÇ  real)   ‚îÇ              ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                   ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ           SPARK CORE ENGINE                 ‚îÇ\n",
        "‚îÇ  (Gesti√≥n de memoria, scheduling, fault     ‚îÇ\n",
        "‚îÇ   tolerance, distribuci√≥n de tareas)        ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                   ‚îÇ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ      CLUSTER MANAGER                        ‚îÇ\n",
        "‚îÇ  [Standalone] [YARN] [Mesos] [Kubernetes]   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "#### **Spark Core Concepts**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CONCEPTOS FUNDAMENTALES DE SPARK\n",
        "\"\"\"\n",
        "\n",
        "# 1. RDD (Resilient Distributed Dataset)\n",
        "rdd_concepto = \"\"\"\n",
        "RDD = Colecci√≥n inmutable y distribuida de objetos\n",
        "\n",
        "Caracter√≠sticas:\n",
        "- Distribuido: Particionado en m√∫ltiples nodos\n",
        "- Inmutable: No se puede modificar (solo crear nuevos RDDs)\n",
        "- Resiliente: Si un nodo falla, se recupera autom√°ticamente\n",
        "- Lazy: Las transformaciones no se ejecutan hasta una acci√≥n\n",
        "\"\"\"\n",
        "\n",
        "# Ejemplo conceptual (no ejecutar a√∫n)\n",
        "# rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
        "# rdd_doubled = rdd.map(lambda x: x * 2)  # Transformaci√≥n (lazy)\n",
        "# result = rdd_doubled.collect()  # Acci√≥n (ejecuta todo)\n",
        "\n",
        "# 2. DataFrame\n",
        "dataframe_concepto = \"\"\"\n",
        "DataFrame = Tabla distribuida con esquema (filas + columnas)\n",
        "\n",
        "Ventajas sobre RDD:\n",
        "- Optimizaci√≥n autom√°tica (Catalyst Optimizer)\n",
        "- API m√°s intuitiva (similar a Pandas/SQL)\n",
        "- Mejor rendimiento\n",
        "- Soporte para m√∫ltiples lenguajes\n",
        "\n",
        "‚Üí En Databricks, SIEMPRE usamos DataFrames, no RDDs\n",
        "\"\"\"\n",
        "\n",
        "# 3. Transformations vs Actions\n",
        "transformations_vs_actions = {\n",
        "    'Transformations': {\n",
        "        'definici√≥n': 'Operaciones que crean nuevo DataFrame (lazy)',\n",
        "        'ejemplos': ['select', 'filter', 'groupBy', 'join', 'withColumn'],\n",
        "        'ejecuci√≥n': 'NO se ejecutan inmediatamente',\n",
        "        'retorno': 'Nuevo DataFrame'\n",
        "    },\n",
        "    'Actions': {\n",
        "        'definici√≥n': 'Operaciones que retornan resultado (trigger)',\n",
        "        'ejemplos': ['show', 'count', 'collect', 'write', 'take'],\n",
        "        'ejecuci√≥n': 'Ejecutan todas las transformaciones pendientes',\n",
        "        'retorno': 'Datos al driver o almacenamiento'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Ejemplo pr√°ctico del concepto\n",
        "print(\"üîÑ TRANSFORMATIONS (Lazy):\")\n",
        "print(\"df.filter(...) ‚Üí No ejecuta nada a√∫n\")\n",
        "print(\"df.select(...) ‚Üí No ejecuta nada a√∫n\")\n",
        "print(\"df.groupBy(...) ‚Üí No ejecuta nada a√∫n\")\n",
        "print()\n",
        "print(\"‚ö° ACTION (Trigger):\")\n",
        "print(\"df.show() ‚Üí ¬°AHORA ejecuta todo!\")\n",
        "```\n",
        "\n",
        "#### **Ejemplo Pr√°ctico: Spark en Acci√≥n**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CASO: Procesar 1 Bill√≥n de registros de logs web\n",
        "\"\"\"\n",
        "\n",
        "# Imaginemos que tenemos archivos de logs distribuidos\n",
        "# Tama√±o total: 500 GB\n",
        "# Objetivo: Encontrar las 10 URLs m√°s visitadas\n",
        "\n",
        "# ===== SIN SPARK (Pandas tradicional) =====\n",
        "pandas_approach = \"\"\"\n",
        "‚ùå IMPOSIBLE CON PANDAS:\n",
        "- 500 GB no caben en memoria de una sola m√°quina\n",
        "- Incluso con 128 GB RAM, pandas colapsar√≠a\n",
        "- Proceso tomar√≠a d√≠as (si no falla)\n",
        "\"\"\"\n",
        "\n",
        "# ===== CON SPARK (Databricks) =====\n",
        "\n",
        "# PASO 1: Leer datos (lazy)\n",
        "df_logs = spark.read.json(\"/data/logs/*.json\")  # 500 GB, 1B registros\n",
        "\n",
        "print(\"‚úÖ Spark ley√≥ metadata, no carg√≥ los 500 GB a√∫n\")\n",
        "\n",
        "# PASO 2: Transformaciones (lazy)\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_urls = df_logs.select(\"url\", \"timestamp\")  # Solo columnas necesarias\n",
        "df_filtered = df_urls.filter(col(\"timestamp\") > \"2025-01-01\")  # Filtrar\n",
        "df_grouped = df_filtered.groupBy(\"url\").count()  # Agrupar\n",
        "df_sorted = df_grouped.orderBy(col(\"count\").desc())  # Ordenar\n",
        "df_top10 = df_sorted.limit(10)  # Top 10\n",
        "\n",
        "print(\"‚úÖ Transformaciones definidas, a√∫n NO ejecutadas\")\n",
        "\n",
        "# PASO 3: Acci√≥n (trigger - ejecuta todo)\n",
        "top_urls = df_top10.collect()  # AQU√ç se ejecuta todo\n",
        "\n",
        "print(\"\\nüéØ Top 10 URLs m√°s visitadas:\")\n",
        "for row in top_urls:\n",
        "    print(f\"   {row.url}: {row.count:,} visitas\")\n",
        "\n",
        "\"\"\"\n",
        "RESULTADO:\n",
        "‚úÖ Procesamiento completado en 5 minutos\n",
        "‚úÖ Spark distribuy√≥ el trabajo en 50 workers\n",
        "‚úÖ Cada worker proces√≥ ~10 GB en paralelo\n",
        "‚úÖ Total: 500 GB procesados eficientemente\n",
        "\n",
        "SIN SPARK:\n",
        "‚ùå Imposible o tomar√≠a d√≠as\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "#### **Lazy Evaluation - El Secreto de Spark**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "¬øPOR QU√â LAZY EVALUATION?\n",
        "\"\"\"\n",
        "\n",
        "# Sin lazy evaluation (ejecuci√≥n inmediata)\n",
        "sin_lazy = \"\"\"\n",
        "df = read_data()          ‚Üí Ejecuta: Lee 500 GB\n",
        "df = filter(df)           ‚Üí Ejecuta: Escanea 500 GB\n",
        "df = select(df)           ‚Üí Ejecuta: Escanea 500 GB otra vez\n",
        "df = group_by(df)         ‚Üí Ejecuta: Escanea 500 GB otra vez\n",
        "...\n",
        "‚ùå M√∫ltiples pasadas sobre los datos\n",
        "‚ùå Extremadamente ineficiente\n",
        "\"\"\"\n",
        "\n",
        "# Con lazy evaluation (Spark)\n",
        "con_lazy = \"\"\"\n",
        "df = read_data()          ‚Üí Solo registra: 'Voy a leer'\n",
        "df = filter(df)           ‚Üí Solo registra: 'Luego filtrar'\n",
        "df = select(df)           ‚Üí Solo registra: 'Luego seleccionar'\n",
        "df = group_by(df)         ‚Üí Solo registra: 'Luego agrupar'\n",
        "df.show()                 ‚Üí AQU√ç optimiza y ejecuta TODO en 1 pasada\n",
        "‚úÖ Una sola pasada sobre los datos\n",
        "‚úÖ Optimizaci√≥n autom√°tica (Catalyst)\n",
        "‚úÖ Extremadamente eficiente\n",
        "\"\"\"\n",
        "\n",
        "# Visualizaci√≥n del plan de ejecuci√≥n\n",
        "ejemplo_plan = \"\"\"\n",
        "Spark analiza TODAS las transformaciones antes de ejecutar\n",
        "y crea un \"plan de ejecuci√≥n optimizado\":\n",
        "\n",
        "Plan Original (naive):\n",
        "1. Leer 500 GB\n",
        "2. Filtrar ‚Üí 400 GB\n",
        "3. Seleccionar 2 columnas ‚Üí 100 GB\n",
        "4. Agrupar\n",
        "\n",
        "Plan Optimizado por Spark:\n",
        "1. Leer SOLO las 2 columnas necesarias ‚Üí 50 GB\n",
        "2. Mientras lee, aplicar filtro ‚Üí 40 GB\n",
        "3. Agrupar directamente\n",
        "\n",
        "Ahorro: 500 GB ‚Üí 40 GB procesados\n",
        "Velocidad: 10x m√°s r√°pido\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_plan)\n",
        "```\n",
        "\n",
        "#### **Databricks Optimiza Spark**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "SPARK VANILLA vs DATABRICKS RUNTIME\n",
        "\"\"\"\n",
        "\n",
        "comparacion = {\n",
        "    'Apache Spark Open Source': {\n",
        "        'instalaci√≥n': 'Manual, compleja',\n",
        "        'configuraci√≥n': 'Requiere expertise',\n",
        "        'optimizaciones': 'Manuales',\n",
        "        'monitoreo': 'Limitado',\n",
        "        'colaboraci√≥n': 'No incluida',\n",
        "        'costo': 'Gratis + infra + tiempo'\n",
        "    },\n",
        "    'Databricks Runtime': {\n",
        "        'instalaci√≥n': 'Clic en bot√≥n',\n",
        "        'configuraci√≥n': 'Autom√°tica y optimizada',\n",
        "        'optimizaciones': 'Photon Engine (3-8x m√°s r√°pido)',\n",
        "        'monitoreo': 'UI avanzada integrada',\n",
        "        'colaboraci√≥n': 'Notebooks compartidos',\n",
        "        'costo': 'Licencia + infra (pero m√°s productivo)'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Mejoras espec√≠ficas de Databricks sobre Spark\n",
        "mejoras_databricks = \"\"\"\n",
        "1. PHOTON ENGINE\n",
        "   - Motor nativo en C++ (vs JVM)\n",
        "   - 3-8x m√°s r√°pido en consultas SQL\n",
        "   - Gratuito en Databricks (no en Spark vanilla)\n",
        "\n",
        "2. AUTO-TUNING\n",
        "   - Databricks ajusta configuraci√≥n autom√°ticamente\n",
        "   - Spark vanilla: Debes configurar manualmente 50+ par√°metros\n",
        "\n",
        "3. DELTA LAKE\n",
        "   - ACID transactions (no en Parquet vanilla)\n",
        "   - Time Travel\n",
        "   - Optimizaciones autom√°ticas\n",
        "\n",
        "4. ADAPTIVE QUERY EXECUTION\n",
        "   - Re-optimiza queries durante ejecuci√≥n\n",
        "   - Ajusta joins din√°micamente\n",
        "   - Versi√≥n mejorada vs Spark OSS\n",
        "\n",
        "5. LIQUID CLUSTERING\n",
        "   - Clustering autom√°tico de datos\n",
        "   - No disponible en Spark vanilla\n",
        "\"\"\"\n",
        "\n",
        "print(mejoras_databricks)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 2: Relaci√≥n con Cloud Providers**\n",
        "\n",
        "#### **Databricks es Multi-Cloud**\n",
        "\n",
        "```markdown\n",
        "‚òÅÔ∏è DATABRICKS EN LOS 3 GRANDES CLOUDS\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ    DATABRICKS ON AWS                    ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Almacenamiento: Amazon S3               ‚îÇ\n",
        "‚îÇ Compute: EC2 instances                  ‚îÇ\n",
        "‚îÇ Networking: VPC, Security Groups        ‚îÇ\n",
        "‚îÇ Identity: IAM Roles                     ‚îÇ\n",
        "‚îÇ Integraci√≥n: Redshift, RDS, Kinesis     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ    DATABRICKS ON AZURE                  ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Almacenamiento: Azure Data Lake Storage ‚îÇ\n",
        "‚îÇ Compute: Azure VMs                      ‚îÇ\n",
        "‚îÇ Networking: VNet, NSG                   ‚îÇ\n",
        "‚îÇ Identity: Azure AD, Managed Identities  ‚îÇ\n",
        "‚îÇ Integraci√≥n: Synapse, SQL DB, Event Hub ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ    DATABRICKS ON GCP                    ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Almacenamiento: Google Cloud Storage    ‚îÇ\n",
        "‚îÇ Compute: Compute Engine VMs             ‚îÇ\n",
        "‚îÇ Networking: VPC, Firewall Rules         ‚îÇ\n",
        "‚îÇ Identity: Service Accounts              ‚îÇ\n",
        "‚îÇ Integraci√≥n: BigQuery, Cloud SQL, Pub/Sub‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "#### **Arquitectura en la Nube**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "C√ìMO FUNCIONA DATABRICKS EN LA NUBE\n",
        "\"\"\"\n",
        "\n",
        "# Databricks tiene 2 planos:\n",
        "\n",
        "plano_control = {\n",
        "    'qu√©_es': 'Gesti√≥n de Databricks (UI, Jobs, Notebooks)',\n",
        "    'd√≥nde': 'Managed by Databricks (AWS/Azure/GCP)',\n",
        "    'responsable': 'Databricks',\n",
        "    'contiene': [\n",
        "        'Web Application (UI)',\n",
        "        'Cluster Manager',\n",
        "        'Job Scheduler',\n",
        "        'Notebook Server',\n",
        "        'MLflow Tracking Server'\n",
        "    ]\n",
        "}\n",
        "\n",
        "plano_datos = {\n",
        "    'qu√©_es': 'Procesamiento y almacenamiento de datos',\n",
        "    'd√≥nde': 'Tu cuenta de cloud (AWS/Azure/GCP)',\n",
        "    'responsable': 'T√∫ (cliente)',\n",
        "    'contiene': [\n",
        "        'Spark Clusters (VMs)',\n",
        "        'Almacenamiento (S3/ADLS/GCS)',\n",
        "        'Networking (VPC)',\n",
        "        'Datos (NUNCA salen de tu cloud)'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"üîí SEGURIDAD CLAVE:\")\n",
        "print(\"   Tus datos NUNCA salen de tu cuenta cloud\")\n",
        "print(\"   Databricks solo gestiona la orquestaci√≥n\")\n",
        "print(\"   Cumplimiento: GDPR, HIPAA, SOC 2, etc.\")\n",
        "```\n",
        "\n",
        "#### **Arquitectura T√©cnica Detallada (AWS como ejemplo)**\n",
        "\n",
        "```markdown\n",
        "üèóÔ∏è ARQUITECTURA DATABRICKS ON AWS\n",
        "\n",
        "TU CUENTA AWS\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                                                ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
        "‚îÇ  ‚îÇ  VPC (Red Privada)                        ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  DATABRICKS WORKSPACE               ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ                                     ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ Driver   ‚îÇ  ‚îÇ Worker 1 ‚îÇ       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  Node    ‚îÇ  ‚îÇ          ‚îÇ       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ (EC2)    ‚îÇ  ‚îÇ  (EC2)   ‚îÇ       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ                                     ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ Worker 2 ‚îÇ  ‚îÇ Worker N ‚îÇ       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  (EC2)   ‚îÇ  ‚îÇ  (EC2)   ‚îÇ       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
        "‚îÇ                                                ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
        "‚îÇ  ‚îÇ  AMAZON S3 (ALMACENAMIENTO)              ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  /dbfs/                             ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  /delta/                            ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îÇ  /mnt/                              ‚îÇ  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
        "‚îÇ                                                ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚ñ≤\n",
        "         ‚îÇ Secure Communication\n",
        "         ‚îÇ\n",
        "DATABRICKS CONTROL PLANE (Managed by Databricks)\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Web UI ‚îÇ Notebooks ‚îÇ Jobs ‚îÇ Cluster Manager  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "#### **Ventajas de la Arquitectura Cloud**\n",
        "\n",
        "```python\n",
        "ventajas_cloud = {\n",
        "    '1. Separaci√≥n de compute y storage': {\n",
        "        'beneficio': 'Escala independientemente',\n",
        "        'ejemplo': \"\"\"\n",
        "        - Puedes tener 1 PB de datos en S3 (barato)\n",
        "        - Solo pagas por compute cuando procesas\n",
        "        - Apagas clusters cuando no los usas ‚Üí $0\n",
        "        \"\"\"\n",
        "    },\n",
        "    \n",
        "    '2. Elasticidad': {\n",
        "        'beneficio': 'Recursos bajo demanda',\n",
        "        'ejemplo': \"\"\"\n",
        "        Lunes 9am: Cluster de 2 workers\n",
        "        Lunes 2pm: Pico de trabajo, auto-scale a 50 workers\n",
        "        Lunes 6pm: De vuelta a 2 workers\n",
        "        Noche: Cluster apagado ‚Üí $0\n",
        "        \"\"\"\n",
        "    },\n",
        "    \n",
        "    '3. Durabilidad': {\n",
        "        'beneficio': 'Datos ultra-seguros',\n",
        "        'ejemplo': \"\"\"\n",
        "        S3: 99.999999999% durabilidad (11 nueves)\n",
        "        ‚Üí Pr√°cticamente imposible perder datos\n",
        "        ‚Üí Replicaci√≥n autom√°tica\n",
        "        \"\"\"\n",
        "    },\n",
        "    \n",
        "    '4. Multi-regi√≥n': {\n",
        "        'beneficio': 'Baja latencia global',\n",
        "        'ejemplo': \"\"\"\n",
        "        Workspace en EU-West (Europa)\n",
        "        Workspace en US-East (Norte Am√©rica)\n",
        "        Workspace en AP-Southeast (Asia)\n",
        "        ‚Üí Usuarios acceden al m√°s cercano\n",
        "        \"\"\"\n",
        "    },\n",
        "    \n",
        "    '5. Pay-as-you-go': {\n",
        "        'beneficio': 'Solo pagas lo que usas',\n",
        "        'ejemplo': \"\"\"\n",
        "        NO necesitas:\n",
        "        - Comprar servidores ($100K+)\n",
        "        - Mantener data center\n",
        "        - Personal de ops 24/7\n",
        "        \n",
        "        Pagas:\n",
        "        - Compute por minuto\n",
        "        - Storage por GB/mes\n",
        "        - Networking (m√≠nimo)\n",
        "        \"\"\"\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Integraci√≥n con Servicios Cloud**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DATABRICKS SE INTEGRA NATIVAMENTE CON TODO EL ECOSISTEMA CLOUD\n",
        "\"\"\"\n",
        "\n",
        "# ===== AWS =====\n",
        "integraciones_aws = {\n",
        "    'data_sources': [\n",
        "        'S3 (almacenamiento)',\n",
        "        'RDS (bases de datos SQL)',\n",
        "        'DynamoDB (NoSQL)',\n",
        "        'Redshift (data warehouse)',\n",
        "        'Kinesis (streaming)',\n",
        "        'MSK (Kafka managed)'\n",
        "    ],\n",
        "    'security': [\n",
        "        'IAM Roles',\n",
        "        'KMS (encriptaci√≥n)',\n",
        "        'Secrets Manager',\n",
        "        'VPC Endpoints'\n",
        "    ],\n",
        "    'analytics': [\n",
        "        'Athena',\n",
        "        'QuickSight',\n",
        "        'SageMaker'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Ejemplo: Leer desde S3\n",
        "df_s3 = spark.read \\\n",
        "    .format(\"delta\") \\\n",
        "    .load(\"s3://mi-bucket/datos/\")\n",
        "\n",
        "# Ejemplo: Escribir a Redshift\n",
        "df.write \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:redshift://cluster.amazonaws.com:5439/db\") \\\n",
        "    .option(\"dbtable\", \"ventas\") \\\n",
        "    .option(\"tempdir\", \"s3://temp-bucket/\") \\\n",
        "    .save()\n",
        "\n",
        "# ===== AZURE =====\n",
        "integraciones_azure = {\n",
        "    'data_sources': [\n",
        "        'ADLS Gen2 (almacenamiento)',\n",
        "        'Azure SQL Database',\n",
        "        'Cosmos DB (NoSQL)',\n",
        "        'Synapse Analytics',\n",
        "        'Event Hubs (streaming)',\n",
        "        'Azure Data Factory'\n",
        "    ],\n",
        "    'security': [\n",
        "        'Azure AD',\n",
        "        'Managed Identities',\n",
        "        'Key Vault',\n",
        "        'Private Link'\n",
        "    ],\n",
        "    'ai': [\n",
        "        'Azure ML',\n",
        "        'Cognitive Services',\n",
        "        'Power BI'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Ejemplo: Leer desde ADLS\n",
        "df_adls = spark.read \\\n",
        "    .format(\"delta\") \\\n",
        "    .load(\"abfss://container@storage.dfs.core.windows.net/datos/\")\n",
        "\n",
        "# ===== GCP =====\n",
        "integraciones_gcp = {\n",
        "    'data_sources': [\n",
        "        'GCS (almacenamiento)',\n",
        "        'BigQuery (data warehouse)',\n",
        "        'Cloud SQL',\n",
        "        'Firestore (NoSQL)',\n",
        "        'Pub/Sub (streaming)',\n",
        "        'Dataflow'\n",
        "    ],\n",
        "    'security': [\n",
        "        'Service Accounts',\n",
        "        'Cloud KMS',\n",
        "        'Secret Manager',\n",
        "        'VPC Service Controls'\n",
        "    ],\n",
        "    'ai': [\n",
        "        'Vertex AI',\n",
        "        'AutoML',\n",
        "        'Looker'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Ejemplo: Leer desde BigQuery\n",
        "df_bq = spark.read \\\n",
        "    .format(\"bigquery\") \\\n",
        "    .option(\"table\", \"proyecto.dataset.tabla\") \\\n",
        "    .load()\n",
        "```\n",
        "\n",
        "#### **Comparaci√≥n: Cloud vs On-Premise**\n",
        "\n",
        "```python\n",
        "# Escenario: Empresa necesita procesar 10 TB diarios\n",
        "\n",
        "# ===== ON-PREMISE (Data Center Propio) =====\n",
        "on_premise = {\n",
        "    'capex_inicial': '$500,000',  # Servidores, storage, networking\n",
        "    'setup_tiempo': '6-12 meses',\n",
        "    'personal': '5-10 ingenieros DevOps/SRE',\n",
        "    'opex_anual': '$200,000',  # Electricidad, cooling, mantenimiento\n",
        "    'escalabilidad': 'Comprar m√°s hardware (meses)',\n",
        "    'disponibilidad': '99.9% (con trabajo)',\n",
        "    'disaster_recovery': 'Complejo y caro',\n",
        "    'actualizaciones': 'Manuales, arriesgadas'\n",
        "}\n",
        "\n",
        "# ===== CLOUD (Databricks on AWS/Azure/GCP) =====\n",
        "cloud = {\n",
        "    'capex_inicial': '$0',  # No hardware\n",
        "    'setup_tiempo': '1 d√≠a',\n",
        "    'personal': '1-2 Data Engineers',\n",
        "    'opex_mensual': '$15,000',  # Solo lo que usas\n",
        "    'escalabilidad': 'Instant√°nea (auto-scaling)',\n",
        "    'disponibilidad': '99.99% (SLA garantizado)',\n",
        "    'disaster_recovery': 'Autom√°tico, multi-regi√≥n',\n",
        "    'actualizaciones': 'Autom√°ticas, sin downtime'\n",
        "}\n",
        "\n",
        "# C√°lculo ROI (3 a√±os)\n",
        "roi_on_premise = 500_000 + (200_000 * 3)  # $1,100,000\n",
        "roi_cloud = 15_000 * 36  # $540,000\n",
        "\n",
        "ahorro = roi_on_premise - roi_cloud\n",
        "print(f\"üí∞ Ahorro en 3 a√±os: ${ahorro:,}\")\n",
        "print(f\"üí∞ Ahorro porcentual: {ahorro/roi_on_premise*100:.1f}%\")\n",
        "\n",
        "\"\"\"\n",
        "Resultado:\n",
        "üí∞ Ahorro en 3 a√±os: $560,000\n",
        "üí∞ Ahorro porcentual: 50.9%\n",
        "\n",
        "Y esto SIN contar:\n",
        "+ Flexibilidad\n",
        "+ Velocidad de innovaci√≥n\n",
        "+ Menor riesgo t√©cnico\n",
        "+ Mejor disaster recovery\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Entendiendo la Arquitectura**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "EJERCICIO CONCEPTUAL: Flujo de datos completo\n",
        "\"\"\"\n",
        "\n",
        "# Simulaci√≥n de arquitectura\n",
        "print(\"üèóÔ∏è ARQUITECTURA DATABRICKS EN ACCI√ìN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# PASO 1: Datos llegan a la nube\n",
        "print(\"\\n1Ô∏è‚É£ INGESTA DE DATOS\")\n",
        "print(\"   üì§ Aplicaci√≥n m√≥vil ‚Üí API Gateway\")\n",
        "print(\"   üì• API Gateway ‚Üí Kinesis Stream (AWS)\")\n",
        "print(\"   üíæ Kinesis ‚Üí S3 (raw data)\")\n",
        "print(\"   ‚úÖ Datos persistidos en tu cuenta AWS\")\n",
        "\n",
        "# PASO 2: Databricks procesa\n",
        "print(\"\\n2Ô∏è‚É£ PROCESAMIENTO CON DATABRICKS\")\n",
        "print(\"   üñ•Ô∏è  Control Plane (Databricks):\")\n",
        "print(\"      - Usuario abre notebook\")\n",
        "print(\"      - Databricks crea cluster en TU VPC\")\n",
        "print(\"   ‚öôÔ∏è  Data Plane (Tu AWS):\")\n",
        "print(\"      - Cluster lee de S3\")\n",
        "print(\"      - Spark procesa en memoria (EC2)\")\n",
        "print(\"      - Escribe a Delta Lake (S3)\")\n",
        "\n",
        "# PASO 3: Consumo\n",
        "print(\"\\n3Ô∏è‚É£ CONSUMO DE DATOS\")\n",
        "print(\"   üìä Analyst: SQL en Databricks ‚Üí Dashboard\")\n",
        "print(\"   ü§ñ ML Model: Lee Delta ‚Üí Predicciones ‚Üí S3\")\n",
        "print(\"   üìà BI Tool: Power BI/Tableau ‚Üí Databricks SQL Warehouse\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üîí SEGURIDAD:\")\n",
        "print(\"   ‚úÖ Datos NUNCA salen de tu cuenta AWS\")\n",
        "print(\"   ‚úÖ Encriptaci√≥n en tr√°nsito y en reposo\")\n",
        "print(\"   ‚úÖ Acceso controlado por IAM roles\")\n",
        "print(\"=\"*60)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 1.4 - Decisiones de Arquitectura**\n",
        "\n",
        "```markdown\n",
        "### Ejercicio: Elige la mejor arquitectura\n",
        "\n",
        "**Escenario 1**: Startup con presupuesto limitado, necesita analizar 100 GB de datos hist√≥ricos\n",
        "\n",
        "Opciones:\n",
        "A) Databricks on AWS con cluster auto-scaling\n",
        "B) Comprar servidor f√≠sico ($50K)\n",
        "C) Pandas en laptop\n",
        "\n",
        "Tu elecci√≥n: ___________\n",
        "Justificaci√≥n: ___________\n",
        "\n",
        "\n",
        "**Escenario 2**: Banco multinacional, regulaciones estrictas de que datos NO pueden salir del pa√≠s\n",
        "\n",
        "Opciones:\n",
        "A) Databricks SaaS (Control Plane en US)\n",
        "B) Databricks con Customer-Managed VPC (Data Plane en tu regi√≥n)\n",
        "C) On-premise Spark cluster\n",
        "\n",
        "Tu elecci√≥n: ___________\n",
        "Justificaci√≥n: ___________\n",
        "\n",
        "\n",
        "**Escenario 3**: Empresa retail con picos de Black Friday (100x tr√°fico normal)\n",
        "\n",
        "Opciones:\n",
        "A) Cluster fijo on-premise (dimensionado para pico)\n",
        "B) Databricks con auto-scaling (2-100 workers)\n",
        "C) Cluster fijo cloud (dimensionado para promedio)\n",
        "\n",
        "Tu elecci√≥n: ___________\n",
        "Justificaci√≥n: ___________\n",
        "\n",
        "\n",
        "**Escenario 4**: Proyecto de investigaci√≥n, presupuesto $0, dataset 50 GB\n",
        "\n",
        "Opciones:\n",
        "A) Databricks Community Edition (gratis)\n",
        "B) Comprar licencias enterprise\n",
        "C) Google Colab con Pandas\n",
        "\n",
        "Tu elecci√≥n: ___________\n",
        "Justificaci√≥n: ___________\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **Apache Spark** es el motor que hace posible el procesamiento distribuido\n",
        "2. **Databricks optimiza Spark** con Photon, auto-tuning, y Delta Lake\n",
        "3. **Lazy evaluation** permite optimizaciones autom√°ticas\n",
        "4. **Databricks es multi-cloud**: funciona igual en AWS, Azure y GCP\n",
        "5. **Separaci√≥n compute/storage** permite escalabilidad independiente\n",
        "6. **Tus datos NUNCA salen** de tu cuenta cloud (seguridad)\n",
        "7. **Pay-as-you-go** elimina CAPEX y reduce OPEX vs on-premise\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **No necesitas ser experto en Spark** para usar Databricks - la plataforma abstrae la complejidad\n",
        "\n",
        "‚úÖ **Elige tu cloud** bas√°ndote en:\n",
        "   - D√≥nde est√°n tus datos actuales\n",
        "   - Qu√© servicios usas (AWS ‚Üí Databricks on AWS)\n",
        "   - Regulaciones (GDPR puede requerir EU region)\n",
        "\n",
        "‚úÖ **Empieza peque√±o, escala cuando necesites**\n",
        "   - Community Edition para aprender\n",
        "   - Trial para POCs\n",
        "   - Production cuando est√©s listo\n",
        "\n",
        "‚úÖ **Databricks no es solo para Big Data**\n",
        "   - √ötil incluso con datasets medianos (GB)\n",
        "   - La colaboraci√≥n y governance valen la pena\n",
        "\n",
        "‚úÖ **Cloud es el presente y futuro**\n",
        "   - 90%+ de nuevos proyectos de datos est√°n en cloud\n",
        "   - Habilidad muy demandada en el mercado laboral\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el siguiente tema **Tema 2: Fundamentos de Apache Spark**?"
      ],
      "metadata": {
        "id": "WEJEcOz8T8h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tema 2: Fundamentos de Apache Spark**\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1 ¬øQu√© es Apache Spark y procesamiento distribuido?**\n",
        "\n",
        "#### **Introducci√≥n: El Problema que Spark Resuelve**\n",
        "\n",
        "Imagina que tienes que contar todas las palabras en un libro de 100 p√°ginas:\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ESCENARIO 1: Una persona (Procesamiento Secuencial)\n",
        "\"\"\"\n",
        "# T√∫ solo, leyendo p√°gina por p√°gina\n",
        "tiempo_una_persona = \"100 p√°ginas √ó 2 minutos/p√°gina = 200 minutos (3.3 horas)\"\n",
        "\n",
        "\"\"\"\n",
        "ESCENARIO 2: 10 personas (Procesamiento Distribuido)\n",
        "\"\"\"\n",
        "# Cada persona lee 10 p√°ginas en paralelo\n",
        "tiempo_diez_personas = \"10 p√°ginas √ó 2 minutos/p√°gina = 20 minutos\"\n",
        "\n",
        "# Ventaja: 10x m√°s r√°pido\n",
        "speedup = 200 / 20\n",
        "print(f\"‚ö° Speedup: {speedup}x m√°s r√°pido\")\n",
        "\n",
        "\"\"\"\n",
        "Este es el concepto fundamental del procesamiento distribuido:\n",
        "DIVIDIR el trabajo entre m√∫ltiples trabajadores que operan en PARALELO\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "**Apache Spark** es el framework que hace esto posible con datos a gran escala.\n",
        "\n",
        "---\n",
        "\n",
        "### **¬øQu√© es Apache Spark?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DEFINICI√ìN FORMAL\n",
        "\"\"\"\n",
        "\n",
        "apache_spark = {\n",
        "    'qu√©_es': 'Motor de procesamiento de datos distribuido y de prop√≥sito general',\n",
        "    'creado': '2009 en UC Berkeley (AMPLab)',\n",
        "    'open_source': 'S√≠, Apache License 2.0',\n",
        "    'lenguajes': ['Scala (nativo)', 'Python (PySpark)', 'Java', 'R', 'SQL'],\n",
        "    'velocidad': 'Hasta 100x m√°s r√°pido que Hadoop MapReduce',\n",
        "    'caracter√≠stica_clave': 'Procesamiento en memoria (in-memory computing)',\n",
        "    'casos_de_uso': [\n",
        "        'Procesamiento batch',\n",
        "        'Streaming en tiempo real',\n",
        "        'Machine Learning',\n",
        "        'An√°lisis SQL',\n",
        "        'Procesamiento de grafos'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"üî• APACHE SPARK\")\n",
        "print(\"=\"*60)\n",
        "for key, value in apache_spark.items():\n",
        "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "```\n",
        "\n",
        "#### **¬øPor qu√© \"Spark\" (Chispa)?**\n",
        "\n",
        "El nombre refleja la **velocidad** del procesamiento:\n",
        "\n",
        "```python\n",
        "# Comparaci√≥n hist√≥rica\n",
        "\n",
        "hadoop_mapreduce = {\n",
        "    'a√±o': 2006,\n",
        "    'enfoque': 'Disco ‚Üí Procesar ‚Üí Disco (repetir)',\n",
        "    'velocidad': 'üêå Lento (horas/d√≠as)',\n",
        "    'memoria': 'Escribe a disco en cada paso'\n",
        "}\n",
        "\n",
        "apache_spark = {\n",
        "    'a√±o': 2009,\n",
        "    'enfoque': 'Cargar en memoria ‚Üí Procesar todo ‚Üí Escribir una vez',\n",
        "    'velocidad': '‚ö° R√°pido (minutos/segundos)',\n",
        "    'memoria': 'Todo en RAM cuando es posible'\n",
        "}\n",
        "\n",
        "print(\"EVOLUCI√ìN DEL PROCESAMIENTO DISTRIBUIDO:\")\n",
        "print(\"\\nüìÖ 2006 - HADOOP MAPREDUCE:\")\n",
        "print(\"   Paso 1: Leer 100GB del disco ‚Üí Procesar ‚Üí Escribir 100GB\")\n",
        "print(\"   Paso 2: Leer 100GB del disco ‚Üí Procesar ‚Üí Escribir 100GB\")\n",
        "print(\"   Paso 3: Leer 100GB del disco ‚Üí Procesar ‚Üí Escribir 100GB\")\n",
        "print(\"   ‚è±Ô∏è  Total: 12 horas\")\n",
        "\n",
        "print(\"\\nüìÖ 2014 - APACHE SPARK:\")\n",
        "print(\"   Paso 1: Leer 100GB del disco ‚Üí Cargar en RAM\")\n",
        "print(\"   Pasos 2-N: Procesar en RAM (sin disco)\")\n",
        "print(\"   √öltimo paso: Escribir resultado final al disco\")\n",
        "print(\"   ‚è±Ô∏è  Total: 30 minutos\")\n",
        "\n",
        "print(\"\\n‚ö° Mejora: 24x m√°s r√°pido\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Procesamiento Distribuido: Conceptos Fundamentales**\n",
        "\n",
        "#### **1. ¬øQu√© es el Procesamiento Distribuido?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "PROCESAMIENTO DISTRIBUIDO =\n",
        "Dividir una tarea grande en sub-tareas peque√±as que se ejecutan\n",
        "en paralelo en m√∫ltiples computadoras (nodos)\n",
        "\"\"\"\n",
        "\n",
        "# Analog√≠a: Restaurante\n",
        "restaurante_analogia = \"\"\"\n",
        "üçΩÔ∏è RESTAURANTE SIN DISTRIBUCI√ìN (1 chef):\n",
        "- 1 chef prepara todos los platos\n",
        "- Atiende 1 mesa a la vez\n",
        "- Capacidad: 10 clientes/hora\n",
        "- Si llegan 100 clientes ‚Üí 10 horas de espera\n",
        "\n",
        "üçΩÔ∏è RESTAURANTE CON DISTRIBUCI√ìN (10 chefs):\n",
        "- 10 chefs trabajan en paralelo\n",
        "- Cada chef prepara platos simult√°neamente\n",
        "- Coordinador (ma√Ætre) asigna tareas\n",
        "- Capacidad: 100 clientes/hora\n",
        "- Si llegan 100 clientes ‚Üí 1 hora de espera\n",
        "\n",
        "CONCEPTOS SPARK:\n",
        "- Chefs = Workers (nodos de trabajo)\n",
        "- Ma√Ætre = Driver (coordinador)\n",
        "- Platos = Particiones de datos\n",
        "- Cocinar = Transformaciones\n",
        "- Servir = Acciones\n",
        "\"\"\"\n",
        "\n",
        "print(restaurante_analogia)\n",
        "```\n",
        "\n",
        "#### **2. Componentes de un Sistema Distribuido Spark**\n",
        "\n",
        "```markdown\n",
        "üèóÔ∏è ARQUITECTURA SPARK\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                   DRIVER                        ‚îÇ\n",
        "‚îÇ  (El Cerebro - Coordina todo)                   ‚îÇ\n",
        "‚îÇ                                                 ‚îÇ\n",
        "‚îÇ  ‚Ä¢ SparkContext / SparkSession                  ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Analiza el c√≥digo                            ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Crea plan de ejecuci√≥n                       ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Distribuye tareas a Workers                  ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Recolecta resultados                         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "               ‚îÇ\n",
        "               ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "               ‚îÇ             ‚îÇ              ‚îÇ              ‚îÇ\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ  WORKER 1   ‚îÇ ‚îÇ  WORKER 2  ‚îÇ ‚îÇ  WORKER 3  ‚îÇ ‚îÇ  WORKER N  ‚îÇ\n",
        "        ‚îÇ             ‚îÇ ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ\n",
        "        ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
        "        ‚îÇ ‚îÇExecutor ‚îÇ ‚îÇ ‚îÇ ‚îÇExecutor‚îÇ ‚îÇ ‚îÇ ‚îÇExecutor‚îÇ ‚îÇ ‚îÇ ‚îÇExecutor‚îÇ ‚îÇ\n",
        "        ‚îÇ ‚îÇ         ‚îÇ ‚îÇ ‚îÇ ‚îÇ        ‚îÇ ‚îÇ ‚îÇ ‚îÇ        ‚îÇ ‚îÇ ‚îÇ ‚îÇ        ‚îÇ ‚îÇ\n",
        "        ‚îÇ ‚îÇ Task 1  ‚îÇ ‚îÇ ‚îÇ ‚îÇ Task 2 ‚îÇ ‚îÇ ‚îÇ ‚îÇ Task 3 ‚îÇ ‚îÇ ‚îÇ ‚îÇ Task N ‚îÇ ‚îÇ\n",
        "        ‚îÇ ‚îÇ Task 2  ‚îÇ ‚îÇ ‚îÇ ‚îÇ Task 3 ‚îÇ ‚îÇ ‚îÇ ‚îÇ Task 4 ‚îÇ ‚îÇ ‚îÇ ‚îÇ Task M ‚îÇ ‚îÇ\n",
        "        ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
        "        ‚îÇ             ‚îÇ ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ\n",
        "        ‚îÇ  Cache/RAM  ‚îÇ ‚îÇ Cache/RAM  ‚îÇ ‚îÇ Cache/RAM  ‚îÇ ‚îÇ Cache/RAM  ‚îÇ\n",
        "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "               ‚îÇ             ‚îÇ              ‚îÇ              ‚îÇ\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ           ALMACENAMIENTO DISTRIBUIDO                     ‚îÇ\n",
        "        ‚îÇ     (HDFS, S3, ADLS, Delta Lake)                        ‚îÇ\n",
        "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "#### **Glosario de Componentes**\n",
        "\n",
        "```python\n",
        "componentes_spark = {\n",
        "    'Driver': {\n",
        "        'funci√≥n': 'Nodo maestro que coordina la ejecuci√≥n',\n",
        "        'responsabilidades': [\n",
        "            'Ejecutar el c√≥digo del usuario (main)',\n",
        "            'Crear SparkContext',\n",
        "            'Construir el DAG (Directed Acyclic Graph)',\n",
        "            'Programar tareas en workers',\n",
        "            'Recolectar resultados'\n",
        "        ],\n",
        "        'analog√≠a': 'Director de orquesta'\n",
        "    },\n",
        "    \n",
        "    'Worker Node': {\n",
        "        'funci√≥n': 'Nodo que ejecuta el trabajo real',\n",
        "        'responsabilidades': [\n",
        "            'Ejecutar tareas asignadas por el Driver',\n",
        "            'Almacenar datos en memoria/disco',\n",
        "            'Reportar resultados al Driver'\n",
        "        ],\n",
        "        'analog√≠a': 'M√∫sico en la orquesta'\n",
        "    },\n",
        "    \n",
        "    'Executor': {\n",
        "        'funci√≥n': 'Proceso JVM en el Worker que ejecuta tareas',\n",
        "        'responsabilidades': [\n",
        "            'Ejecutar c√≥digo (transformaciones/acciones)',\n",
        "            'Almacenar datos en cache/memoria',\n",
        "            'Comunicarse con el Driver'\n",
        "        ],\n",
        "        'analog√≠a': 'Trabajador individual dentro del m√∫sico',\n",
        "        'nota': 'Cada Worker puede tener 1 o m√°s Executors'\n",
        "    },\n",
        "    \n",
        "    'Task': {\n",
        "        'funci√≥n': 'Unidad m√°s peque√±a de trabajo',\n",
        "        'responsabilidades': [\n",
        "            'Operar sobre una partici√≥n de datos',\n",
        "            'Ejecutar transformaciones'\n",
        "        ],\n",
        "        'analog√≠a': 'Una nota musical espec√≠fica',\n",
        "        'nota': 'Una Task = Una operaci√≥n sobre una partici√≥n'\n",
        "    },\n",
        "    \n",
        "    'Partition': {\n",
        "        'funci√≥n': 'Fragmento de datos distribuido',\n",
        "        'responsabilidades': [\n",
        "            'Almacenar porci√≥n de los datos totales',\n",
        "            'Ser procesada por una Task'\n",
        "        ],\n",
        "        'analog√≠a': 'Cap√≠tulo de un libro',\n",
        "        'nota': '# Particiones determina paralelismo'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Visualizaci√≥n\n",
        "print(\"üìö COMPONENTES DE SPARK\")\n",
        "print(\"=\"*60)\n",
        "for componente, info in componentes_spark.items():\n",
        "    print(f\"\\nüîπ {componente.upper()}\")\n",
        "    print(f\"   Funci√≥n: {info['funci√≥n']}\")\n",
        "    print(f\"   Analog√≠a: {info['analog√≠a']}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Ejemplo Pr√°ctico: Procesamiento Distribuido en Acci√≥n**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CASO PR√ÅCTICO: Contar palabras en 1 Bill√≥n de tweets\n",
        "\"\"\"\n",
        "\n",
        "# ===== DATOS =====\n",
        "datos = {\n",
        "    'total_tweets': 1_000_000_000,  # 1 bill√≥n\n",
        "    'tama√±o_promedio': 140,  # bytes por tweet\n",
        "    'tama√±o_total_gb': (1_000_000_000 * 140) / (1024**3),  # ~130 GB\n",
        "    'palabras_promedio': 20\n",
        "}\n",
        "\n",
        "print(\"üìä DATASET:\")\n",
        "print(f\"   Total tweets: {datos['total_tweets']:,}\")\n",
        "print(f\"   Tama√±o total: ~{datos['tama√±o_total_gb']:.0f} GB\")\n",
        "\n",
        "# ===== SIN SPARK (Una sola m√°quina) =====\n",
        "print(\"\\n‚ùå SIN SPARK (Procesamiento Secuencial):\")\n",
        "print(\"   M√°quina: 1 core, 16 GB RAM\")\n",
        "print(\"   Problema: Dataset no cabe en memoria\")\n",
        "print(\"   Tiempo estimado: IMPOSIBLE o d√≠as\")\n",
        "\n",
        "# ===== CON SPARK (Distribuido) =====\n",
        "print(\"\\n‚úÖ CON SPARK (Procesamiento Distribuido):\")\n",
        "print(\"   Cluster: 1 Driver + 10 Workers\")\n",
        "print(\"   Cada Worker: 4 cores, 32 GB RAM\")\n",
        "\n",
        "# Configuraci√≥n del cluster\n",
        "cluster = {\n",
        "    'workers': 10,\n",
        "    'cores_por_worker': 4,\n",
        "    'total_cores': 10 * 4,  # 40 cores\n",
        "    'ram_por_worker_gb': 32,\n",
        "    'total_ram_gb': 10 * 32  # 320 GB\n",
        "}\n",
        "\n",
        "# Particiones autom√°ticas\n",
        "particiones = cluster['total_cores'] * 2  # Regla general: 2-3x cores\n",
        "datos_por_particion_gb = datos['tama√±o_total_gb'] / particiones\n",
        "\n",
        "print(f\"   Total cores: {cluster['total_cores']}\")\n",
        "print(f\"   Total RAM: {cluster['total_ram_gb']} GB\")\n",
        "print(f\"   Particiones: {particiones}\")\n",
        "print(f\"   Datos por partici√≥n: ~{datos_por_particion_gb:.2f} GB\")\n",
        "\n",
        "# ===== EJECUCI√ìN PASO A PASO =====\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚öôÔ∏è  EJECUCI√ìN DISTRIBUIDA:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Paso 1: Cargar datos\n",
        "print(\"\\n1Ô∏è‚É£ CARGAR DATOS (Lectura paralela):\")\n",
        "print(\"   ‚Ä¢ Driver lee metadata de archivos en S3\")\n",
        "print(\"   ‚Ä¢ Driver divide datos en 80 particiones\")\n",
        "print(\"   ‚Ä¢ Asigna particiones a Workers disponibles\")\n",
        "print(\"   ‚Ä¢ Cada Worker lee sus particiones (~1.6 GB cada una)\")\n",
        "print(\"   ‚è±Ô∏è  Tiempo: 2 minutos (paralelo)\")\n",
        "\n",
        "# C√≥digo conceptual\n",
        "\"\"\"\n",
        "# Este c√≥digo se ejecuta en el Driver\n",
        "df_tweets = spark.read.text(\"s3://tweets/*.txt\")\n",
        "# Spark autom√°ticamente:\n",
        "# - Detecta 130 GB de datos\n",
        "# - Crea 80 particiones (~1.6 GB cada una)\n",
        "# - Distribuye a Workers\n",
        "\"\"\"\n",
        "\n",
        "# Paso 2: Transformaciones (Lazy)\n",
        "print(\"\\n2Ô∏è‚É£ TRANSFORMACIONES (En memoria, paralelo):\")\n",
        "print(\"   ‚Ä¢ Cada Worker procesa sus particiones en RAM\")\n",
        "print(\"   ‚Ä¢ Split de texto en palabras\")\n",
        "print(\"   ‚Ä¢ Filtrar palabras vac√≠as\")\n",
        "print(\"   ‚Ä¢ Map: palabra ‚Üí (palabra, 1)\")\n",
        "\n",
        "\"\"\"\n",
        "# Transformaciones (lazy - no ejecutan a√∫n)\n",
        "words = df_tweets.flatMap(lambda line: line.split(\" \"))\n",
        "word_counts = words.map(lambda word: (word, 1))\n",
        "\"\"\"\n",
        "\n",
        "print(\"   ‚úÖ Sin comunicaci√≥n entre Workers (eficiente)\")\n",
        "print(\"   ‚è±Ô∏è  Tiempo: 0 segundos (lazy, no ejecuta a√∫n)\")\n",
        "\n",
        "# Paso 3: Shuffle (Redistribuci√≥n)\n",
        "print(\"\\n3Ô∏è‚É£ SHUFFLE (Redistribuci√≥n de datos):\")\n",
        "print(\"   ‚Ä¢ Agrupar por palabra requiere reagrupar datos\")\n",
        "print(\"   ‚Ä¢ Palabras iguales deben ir al mismo Worker\")\n",
        "print(\"   ‚Ä¢ Workers intercambian datos por la red\")\n",
        "\n",
        "\"\"\"\n",
        "# Esta operaci√≥n causa shuffle\n",
        "result = word_counts.reduceByKey(lambda a, b: a + b)\n",
        "\"\"\"\n",
        "\n",
        "print(\"   ‚ö†Ô∏è  Operaci√≥n costosa (red)\")\n",
        "print(\"   ‚è±Ô∏è  Tiempo: 5 minutos\")\n",
        "\n",
        "# Paso 4: Agregaci√≥n\n",
        "print(\"\\n4Ô∏è‚É£ AGREGACI√ìN (Reducci√≥n paralela):\")\n",
        "print(\"   ‚Ä¢ Cada Worker suma conteos de sus palabras\")\n",
        "print(\"   ‚Ä¢ Resultado parcial por Worker\")\n",
        "\n",
        "print(\"   ‚è±Ô∏è  Tiempo: 1 minuto\")\n",
        "\n",
        "# Paso 5: Colectar resultados\n",
        "print(\"\\n5Ô∏è‚É£ ACCI√ìN (Trigger y recolecci√≥n):\")\n",
        "print(\"   ‚Ä¢ Driver solicita top 10 palabras\")\n",
        "print(\"   ‚Ä¢ Workers env√≠an sus top 10 al Driver\")\n",
        "print(\"   ‚Ä¢ Driver combina y ordena resultados finales\")\n",
        "\n",
        "\"\"\"\n",
        "# Acci√≥n - ejecuta toda la pipeline\n",
        "top_words = result.takeOrdered(10, key=lambda x: -x[1])\n",
        "\"\"\"\n",
        "\n",
        "print(\"   ‚è±Ô∏è  Tiempo: 30 segundos\")\n",
        "\n",
        "# Resumen\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä RESUMEN:\")\n",
        "print(\"=\"*60)\n",
        "print(\"‚è±Ô∏è  Tiempo total: ~9 minutos\")\n",
        "print(\"‚ö° Speedup vs 1 m√°quina: ~40x m√°s r√°pido\")\n",
        "print(\"üí∞ Costo: Solo pagas por 9 minutos de compute\")\n",
        "print(\"‚úÖ Escalable: Con 100 workers ‚Üí 1 minuto\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Conceptos Clave del Procesamiento Distribuido**\n",
        "\n",
        "#### **1. Particionamiento (Partitioning)**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "PARTICIONAMIENTO = Dividir datos en fragmentos\n",
        "\"\"\"\n",
        "\n",
        "# Ejemplo conceptual\n",
        "print(\"üì¶ PARTICIONAMIENTO DE DATOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Dataset original\n",
        "dataset_completo = {\n",
        "    'registros_totales': 1_000_000,\n",
        "    'tama√±o_gb': 10\n",
        "}\n",
        "\n",
        "# Sin particiones (imposible procesar distribuido)\n",
        "sin_particiones = \"\"\"\n",
        "‚ùå SIN PARTICIONES:\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  1,000,000 registros (10 GB)       ‚îÇ\n",
        "‚îÇ  [Todo en un solo bloque]          ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "‚Üí Solo 1 Worker puede procesarlo\n",
        "‚Üí No hay paralelismo\n",
        "‚Üí Lento\n",
        "\"\"\"\n",
        "\n",
        "# Con particiones\n",
        "con_particiones = \"\"\"\n",
        "‚úÖ CON 10 PARTICIONES:\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Part 1   ‚îÇ ‚îÇ Part 2   ‚îÇ ‚îÇ Part 3   ‚îÇ\n",
        "‚îÇ 100K     ‚îÇ ‚îÇ 100K     ‚îÇ ‚îÇ 100K     ‚îÇ\n",
        "‚îÇ (1 GB)   ‚îÇ ‚îÇ (1 GB)   ‚îÇ ‚îÇ (1 GB)   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ...\n",
        "‚îÇ Part 4   ‚îÇ ‚îÇ Part 5   ‚îÇ\n",
        "‚îÇ 100K     ‚îÇ ‚îÇ 100K     ‚îÇ\n",
        "‚îÇ (1 GB)   ‚îÇ ‚îÇ (1 GB)   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚Üí 10 Workers procesan en paralelo\n",
        "‚Üí M√°ximo paralelismo\n",
        "‚Üí 10x m√°s r√°pido\n",
        "\"\"\"\n",
        "\n",
        "print(sin_particiones)\n",
        "print(con_particiones)\n",
        "\n",
        "# Reglas de particionamiento\n",
        "reglas = \"\"\"\n",
        "üìè REGLAS DE PARTICIONAMIENTO:\n",
        "\n",
        "1. N√∫mero √≥ptimo de particiones:\n",
        "   ‚Ä¢ M√≠nimo: 2-3 particiones por core\n",
        "   ‚Ä¢ M√°ximo: Evitar particiones <128 MB\n",
        "   \n",
        "   Ejemplo: 40 cores ‚Üí 80-120 particiones\n",
        "\n",
        "2. Tama√±o de partici√≥n:\n",
        "   ‚Ä¢ Ideal: 128 MB - 1 GB por partici√≥n\n",
        "   ‚Ä¢ Demasiado peque√±as ‚Üí Overhead\n",
        "   ‚Ä¢ Demasiado grandes ‚Üí Problemas de memoria\n",
        "\n",
        "3. Balance:\n",
        "   ‚Ä¢ Particiones del mismo tama√±o (evitar skew)\n",
        "   ‚Ä¢ Distribuci√≥n uniforme entre Workers\n",
        "\"\"\"\n",
        "\n",
        "print(reglas)\n",
        "```\n",
        "\n",
        "#### **2. Paralelismo**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "PARALELISMO = M√∫ltiples operaciones simult√°neas\n",
        "\"\"\"\n",
        "\n",
        "# Niveles de paralelismo en Spark\n",
        "\n",
        "print(\"‚ö° NIVELES DE PARALELISMO EN SPARK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Nivel 1: Paralelismo de tareas\n",
        "paralelismo_tareas = \"\"\"\n",
        "1Ô∏è‚É£ PARALELISMO DE TAREAS:\n",
        "   \n",
        "   Cluster con 10 Workers, 4 cores cada uno = 40 cores\n",
        "   80 particiones de datos\n",
        "   \n",
        "   Ejecuci√≥n:\n",
        "   ‚Ä¢ Ronda 1: 40 tareas ejecutan simult√°neamente (1 por core)\n",
        "   ‚Ä¢ Ronda 2: Otras 40 tareas ejecutan simult√°neamente\n",
        "   ‚Ä¢ Total: 2 rondas para procesar 80 particiones\n",
        "   \n",
        "   Tiempo = Tiempo_por_tarea √ó (Particiones / Cores)\n",
        "\"\"\"\n",
        "\n",
        "# Nivel 2: Paralelismo de stages\n",
        "paralelismo_stages = \"\"\"\n",
        "2Ô∏è‚É£ PARALELISMO DE STAGES:\n",
        "\n",
        "   Pipeline: Read ‚Üí Filter ‚Üí GroupBy ‚Üí Aggregate\n",
        "   \n",
        "   ‚ö†Ô∏è  SECUENCIAL (dependencias):\n",
        "   Stage 1: Read + Filter    ‚Üí Completa primero\n",
        "   Stage 2: GroupBy (shuffle) ‚Üí Luego ejecuta\n",
        "   Stage 3: Aggregate        ‚Üí Finalmente ejecuta\n",
        "   \n",
        "   Dentro de cada stage ‚Üí Paralelismo de tareas\n",
        "\"\"\"\n",
        "\n",
        "# Nivel 3: Paralelismo de jobs\n",
        "paralelismo_jobs = \"\"\"\n",
        "3Ô∏è‚É£ PARALELISMO DE JOBS (Avanzado):\n",
        "\n",
        "   Si tienes 2 acciones independientes:\n",
        "   \n",
        "   Job 1: df1.filter(...).count()\n",
        "   Job 2: df2.select(...).write(...)\n",
        "   \n",
        "   ‚Üí Pueden ejecutarse en paralelo si hay recursos\n",
        "\"\"\"\n",
        "\n",
        "print(paralelismo_tareas)\n",
        "print(paralelismo_stages)\n",
        "print(paralelismo_jobs)\n",
        "```\n",
        "\n",
        "#### **3. Shuffle - El Villano del Procesamiento Distribuido**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "SHUFFLE = Redistribuci√≥n de datos entre Workers\n",
        "\"\"\"\n",
        "\n",
        "print(\"üîÄ SHUFFLE - OPERACI√ìN COSTOSA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ¬øQu√© es shuffle?\n",
        "shuffle_explicacion = \"\"\"\n",
        "ANTES DEL SHUFFLE:\n",
        "Worker 1: [(\"casa\", 1), (\"perro\", 1), (\"casa\", 1)]\n",
        "Worker 2: [(\"gato\", 1), (\"casa\", 1), (\"perro\", 1)]\n",
        "Worker 3: [(\"perro\", 1), (\"casa\", 1), (\"gato\", 1)]\n",
        "\n",
        "DESPU√âS DEL SHUFFLE (agrupado por palabra):\n",
        "Worker 1: [(\"casa\", 1), (\"casa\", 1), (\"casa\", 1), (\"casa\", 1)]\n",
        "Worker 2: [(\"perro\", 1), (\"perro\", 1), (\"perro\", 1)]\n",
        "Worker 3: [(\"gato\", 1), (\"gato\", 1)]\n",
        "\n",
        "üì° Requiere comunicaci√≥n por red entre Workers\n",
        "üí∞ Costoso en tiempo y recursos\n",
        "\"\"\"\n",
        "\n",
        "print(shuffle_explicacion)\n",
        "\n",
        "# Operaciones que causan shuffle\n",
        "operaciones_shuffle = \"\"\"\n",
        "‚ö†Ô∏è  OPERACIONES QUE CAUSAN SHUFFLE:\n",
        "\n",
        "‚úÖ Necesarias (inevitables):\n",
        "   ‚Ä¢ groupBy()\n",
        "   ‚Ä¢ reduceByKey()\n",
        "   ‚Ä¢ join()\n",
        "   ‚Ä¢ repartition()\n",
        "   ‚Ä¢ distinct()\n",
        "   ‚Ä¢ sortBy()\n",
        "\n",
        "‚ùå Evitables (optimiza):\n",
        "   ‚Ä¢ repartition() innecesarios\n",
        "   ‚Ä¢ joins sin broadcast\n",
        "   ‚Ä¢ groupBy cuando podr√≠as usar aggregates\n",
        "\n",
        "üí° ESTRATEGIAS PARA MINIMIZAR SHUFFLE:\n",
        "   1. Usar operaciones que no requieren shuffle cuando sea posible\n",
        "   2. Broadcast de datasets peque√±os en joins\n",
        "   3. Particionar datos estrat√©gicamente\n",
        "   4. Reusar particionamiento\n",
        "\"\"\"\n",
        "\n",
        "print(operaciones_shuffle)\n",
        "\n",
        "# Ejemplo de costo de shuffle\n",
        "ejemplo_shuffle = \"\"\"\n",
        "EJEMPLO REAL:\n",
        "\n",
        "Dataset: 100 GB en 10 Workers\n",
        "\n",
        "SIN SHUFFLE (filter, map, select):\n",
        "‚Üí Cada Worker procesa sus datos localmente\n",
        "‚Üí No hay comunicaci√≥n de red\n",
        "‚Üí Tiempo: 2 minutos\n",
        "\n",
        "CON SHUFFLE (groupBy):\n",
        "‚Üí Workers intercambian ~80 GB de datos\n",
        "‚Üí Red: 10 Gbps ‚Üí 64 segundos solo de transferencia\n",
        "‚Üí Overhead serializaci√≥n/deserializaci√≥n\n",
        "‚Üí Tiempo total: 5 minutos\n",
        "\n",
        "Diferencia: 2.5x m√°s lento\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_shuffle)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Simulaci√≥n Conceptual**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "EJERCICIO: Simular procesamiento distribuido\n",
        "\"\"\"\n",
        "\n",
        "# Simulaci√≥n simple de word count distribuido\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"üß™ SIMULACI√ìN: WORD COUNT DISTRIBUIDO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Dataset simulado\n",
        "documentos = [\n",
        "    \"apache spark es r√°pido\",\n",
        "    \"spark procesa datos grandes\",\n",
        "    \"python y spark trabajan juntos\",\n",
        "    \"spark es distribuido\"\n",
        "] * 25  # 100 documentos\n",
        "\n",
        "print(f\"üìÑ Total documentos: {len(documentos)}\")\n",
        "\n",
        "# Configuraci√≥n del cluster\n",
        "num_workers = 4\n",
        "particiones = 4\n",
        "\n",
        "# PASO 1: Particionar datos\n",
        "print(f\"\\n1Ô∏è‚É£ PARTICIONAR en {particiones} particiones:\")\n",
        "particiones_datos = [[] for _ in range(particiones)]\n",
        "\n",
        "for i, doc in enumerate(documentos):\n",
        "    partition_id = i % particiones\n",
        "    particiones_datos[partition_id].append(doc)\n",
        "\n",
        "for i, partition in enumerate(particiones_datos):\n",
        "    print(f\"   Partici√≥n {i}: {len(partition)} documentos\")\n",
        "\n",
        "# PASO 2: Map fase (cada worker procesa su partici√≥n)\n",
        "print(f\"\\n2Ô∏è‚É£ MAP (Paralelamente en {num_workers} workers):\")\n",
        "\n",
        "def map_function(docs):\n",
        "    \"\"\"Simula el trabajo de un worker en la fase map\"\"\"\n",
        "    word_counts = defaultdict(int)\n",
        "    for doc in docs:\n",
        "        for word in doc.split():\n",
        "            word_counts[word] += 1\n",
        "    return word_counts\n",
        "\n",
        "# Simular procesamiento paralelo\n",
        "resultados_parciales = []\n",
        "for i, partition in enumerate(particiones_datos):\n",
        "    resultado = map_function(partition)\n",
        "    resultados_parciales.append(resultado)\n",
        "    print(f\"   Worker {i}: {len(resultado)} palabras √∫nicas\")\n",
        "\n",
        "# PASO 3: Shuffle (redistribuir por palabra)\n",
        "print(\"\\n3Ô∏è‚É£ SHUFFLE (Redistribuir por palabra):\")\n",
        "print(\"   ‚ö†Ô∏è  Datos se mueven entre workers...\")\n",
        "\n",
        "# Combinar resultados (simula shuffle)\n",
        "all_words = defaultdict(list)\n",
        "for worker_result in resultados_parciales:\n",
        "    for word, count in worker_result.items():\n",
        "        all_words[word].append(count)\n",
        "\n",
        "print(f\"   ‚úÖ {len(all_words)} palabras √∫nicas despu√©s de shuffle\")\n",
        "\n",
        "# PASO 4: Reduce (agregar conteos finales)\n",
        "print(\"\\n4Ô∏è‚É£ REDUCE (Agregar conteos):\")\n",
        "\n",
        "resultado_final = {}\n",
        "for word, counts in all_words.items():\n",
        "    resultado_final[word] = sum(counts)\n",
        "\n",
        "# Ordenar por frecuencia\n",
        "top_words = sorted(resultado_final.items(),\n",
        "                   key=lambda x: x[1],\n",
        "                   reverse=True)[:5]\n",
        "\n",
        "print(\"\\nüìä TOP 5 PALABRAS:\")\n",
        "for word, count in top_words:\n",
        "    print(f\"   '{word}': {count} veces\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Procesamiento distribuido completado!\")\n",
        "print(\"=\"*60)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 2.1 - Comprensi√≥n**\n",
        "\n",
        "```markdown\n",
        "### Ejercicio: Dise√±a un sistema distribuido\n",
        "\n",
        "**Escenario**: Tienes 1 TB de datos (1,000 GB) de logs de servidor que necesitas procesar.\n",
        "\n",
        "**Tu cluster**:\n",
        "- 1 Driver (16 GB RAM)\n",
        "- 20 Workers (32 GB RAM cada uno, 4 cores)\n",
        "\n",
        "**Preguntas**:\n",
        "\n",
        "1. **¬øCu√°ntas particiones crear√≠as?**\n",
        "   F√≥rmula: ___________\n",
        "   Respuesta: ___________\n",
        "   \n",
        "2. **¬øCu√°l ser√≠a el tama√±o ideal de cada partici√≥n?**\n",
        "   C√°lculo: ___________\n",
        "   Respuesta: ___________\n",
        "\n",
        "3. **¬øCu√°ntas tareas podr√≠an ejecutarse simult√°neamente?**\n",
        "   C√°lculo: ___________\n",
        "   Respuesta: ___________\n",
        "\n",
        "4. **Si cada tarea tarda 2 minutos, ¬øcu√°nto tardar√≠a el proceso completo?**\n",
        "   C√°lculo: ___________\n",
        "   Respuesta: ___________\n",
        "\n",
        "5. **¬øQu√© pasar√≠a si usaras solo 10 particiones en lugar de las √≥ptimas?**\n",
        "   Problema: ___________\n",
        "   Impacto: ___________\n",
        "\n",
        "6. **Identifica qu√© operaciones causan shuffle**:\n",
        "   - [ ] df.filter(col(\"edad\") > 18)\n",
        "   - [ ] df.select(\"nombre\", \"edad\")\n",
        "   - [ ] df.groupBy(\"ciudad\").count()\n",
        "   - [ ] df.withColumn(\"edad_doble\", col(\"edad\") * 2)\n",
        "   - [ ] df.join(df2, \"id\")\n",
        "   - [ ] df.orderBy(\"edad\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **Spark = Procesamiento distribuido en memoria** (hasta 100x m√°s r√°pido que Hadoop)\n",
        "2. **Driver coordina**, **Workers ejecutan** el trabajo real\n",
        "3. **Particiones** determinan el nivel de paralelismo\n",
        "4. **Shuffle** es costoso - minim√≠zalo cuando sea posible\n",
        "5. **Paralelismo** permite procesar datos masivos en minutos vs d√≠as\n",
        "6. **Tama√±o √≥ptimo de partici√≥n**: 128 MB - 1 GB\n",
        "7. **Regla particiones**: 2-3x n√∫mero de cores en el cluster\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **M√°s Workers ‚â† Siempre mejor**\n",
        "   - Hay overhead de coordinaci√≥n\n",
        "   - Encuentra el balance para tu caso de uso\n",
        "\n",
        "‚úÖ **Monitorea particiones**\n",
        "   - Particiones desbalanceadas (skew) ‚Üí Workers ociosos\n",
        "   - Usa Spark UI para detectar problemas\n",
        "\n",
        "‚úÖ **Shuffle es inevitable a veces**\n",
        "   - No evites groupBy/join si los necesitas\n",
        "   - Optimiza, pero no sacrifiques l√≥gica de negocio\n",
        "\n",
        "‚úÖ **In-memory ‚â† Todo cabe en RAM**\n",
        "   - Spark puede procesar datasets > RAM total\n",
        "   - Spill to disk cuando es necesario (m√°s lento pero funciona)\n",
        "\n",
        "‚úÖ **Prueba en peque√±o primero**\n",
        "   - Antes de procesar 1 TB, prueba con 1 GB\n",
        "   - Valida l√≥gica y estima tiempos\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el siguiente punto **2.2 RDDs, DataFrames y Datasets**?"
      ],
      "metadata": {
        "id": "IYCTuNc1UNtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 RDDs, DataFrames y Datasets**\n",
        "\n",
        "#### **Introducci√≥n: Las Tres Abstracciones de Spark**\n",
        "\n",
        "Apache Spark ofrece **tres abstracciones** para trabajar con datos distribuidos, cada una con diferentes niveles de optimizaci√≥n y facilidad de uso:\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "EVOLUCI√ìN DE LAS ABSTRACCIONES EN SPARK\n",
        "\"\"\"\n",
        "\n",
        "evolucion = \"\"\"\n",
        "üìÖ 2011 - RDD (Resilient Distributed Dataset)\n",
        "   ‚îú‚îÄ Primera abstracci√≥n de Spark\n",
        "   ‚îú‚îÄ Bajo nivel, m√°ximo control\n",
        "   ‚îî‚îÄ Sin optimizaciones autom√°ticas\n",
        "\n",
        "üìÖ 2013 - DataFrame\n",
        "   ‚îú‚îÄ Inspirado en Pandas y R\n",
        "   ‚îú‚îÄ API de alto nivel (SQL-like)\n",
        "   ‚îú‚îÄ Optimizaci√≥n autom√°tica (Catalyst)\n",
        "   ‚îî‚îÄ ‚≠ê M√ÅS USADO EN DATABRICKS\n",
        "\n",
        "üìÖ 2015 - Dataset\n",
        "   ‚îú‚îÄ Combina RDD + DataFrame\n",
        "   ‚îú‚îÄ Type-safe (solo Scala/Java)\n",
        "   ‚îî‚îÄ Usado principalmente en Scala\n",
        "\n",
        "HOY (2025):\n",
        "‚Üí 95% usamos DataFrames (PySpark/SQL)\n",
        "‚Üí 4% usan Datasets (Scala)\n",
        "‚Üí 1% usan RDDs (casos muy espec√≠ficos)\n",
        "\"\"\"\n",
        "\n",
        "print(evolucion)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 1: RDD (Resilient Distributed Dataset)**\n",
        "\n",
        "#### **¬øQu√© es un RDD?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "RDD = Resilient Distributed Dataset\n",
        "\"\"\"\n",
        "\n",
        "rdd_definicion = {\n",
        "    'R - Resilient': 'Tolerante a fallos (se recupera autom√°ticamente)',\n",
        "    'D - Distributed': 'Datos distribuidos en m√∫ltiples nodos',\n",
        "    'D - Dataset': 'Colecci√≥n de objetos',\n",
        "    'caracteristicas': [\n",
        "        'Inmutable (no se puede modificar)',\n",
        "        'Particionado (dividido en chunks)',\n",
        "        'Lazy evaluation',\n",
        "        'Bajo nivel (m√°s control, menos optimizaci√≥n)'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"üî∑ RDD - LA ABSTRACCI√ìN ORIGINAL\")\n",
        "print(\"=\"*60)\n",
        "for key, value in rdd_definicion.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "```\n",
        "\n",
        "#### **Anatom√≠a de un RDD**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ESTRUCTURA INTERNA DE UN RDD\n",
        "\"\"\"\n",
        "\n",
        "ejemplo_rdd = \"\"\"\n",
        "RDD con 12 elementos distribuidos en 3 particiones:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ           SPARK CLUSTER                 ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  Worker 1         Worker 2    Worker 3  ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
        "‚îÇ  ‚îÇPartition0‚îÇ   ‚îÇPartition1‚îÇ ‚îÇPart. 2 ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ\n",
        "‚îÇ  ‚îÇ   1      ‚îÇ   ‚îÇ    5     ‚îÇ ‚îÇ   9    ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ   2      ‚îÇ   ‚îÇ    6     ‚îÇ ‚îÇ  10    ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ   3      ‚îÇ   ‚îÇ    7     ‚îÇ ‚îÇ  11    ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îÇ   4      ‚îÇ   ‚îÇ    8     ‚îÇ ‚îÇ  12    ‚îÇ ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "Cada Worker procesa su partici√≥n independientemente\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_rdd)\n",
        "```\n",
        "\n",
        "#### **Operaciones con RDDs**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "OPERACIONES B√ÅSICAS CON RDDS\n",
        "\"\"\"\n",
        "\n",
        "# NOTA: Este c√≥digo es conceptual para entender RDDs\n",
        "# En Databricks, normalmente NO usar√≠as RDDs directamente\n",
        "\n",
        "# ===== CREAR UN RDD =====\n",
        "print(\"1Ô∏è‚É£ CREAR RDD:\")\n",
        "print()\n",
        "\n",
        "# Desde una lista (parallelize)\n",
        "# rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "# print(f\"RDD creado con {rdd.count()} elementos\")\n",
        "\n",
        "# Desde un archivo\n",
        "# rdd_texto = spark.sparkContext.textFile(\"/data/archivo.txt\")\n",
        "\n",
        "# ===== TRANSFORMACIONES (Lazy) =====\n",
        "print(\"\\n2Ô∏è‚É£ TRANSFORMACIONES RDD:\")\n",
        "print()\n",
        "\n",
        "transformaciones_rdd = \"\"\"\n",
        "# map() - Aplicar funci√≥n a cada elemento\n",
        "rdd_doubled = rdd.map(lambda x: x * 2)\n",
        "# [1,2,3] ‚Üí [2,4,6]\n",
        "\n",
        "# filter() - Filtrar elementos\n",
        "rdd_pares = rdd.filter(lambda x: x % 2 == 0)\n",
        "# [1,2,3,4,5] ‚Üí [2,4]\n",
        "\n",
        "# flatMap() - Map que retorna m√∫ltiples valores\n",
        "rdd_texto = sc.parallelize([\"hola mundo\", \"apache spark\"])\n",
        "rdd_palabras = rdd_texto.flatMap(lambda linea: linea.split(\" \"))\n",
        "# [\"hola mundo\", \"apache spark\"] ‚Üí [\"hola\", \"mundo\", \"apache\", \"spark\"]\n",
        "\n",
        "# reduceByKey() - Agregar por clave (causa shuffle)\n",
        "rdd_pares = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
        "rdd_agregado = rdd_pares.reduceByKey(lambda a, b: a + b)\n",
        "# [(\"a\",1), (\"b\",2), (\"a\",3)] ‚Üí [(\"a\",4), (\"b\",2)]\n",
        "\n",
        "‚ö†Ô∏è IMPORTANTE: Todas estas son LAZY (no ejecutan hasta una acci√≥n)\n",
        "\"\"\"\n",
        "\n",
        "print(transformaciones_rdd)\n",
        "\n",
        "# ===== ACCIONES (Trigger) =====\n",
        "print(\"\\n3Ô∏è‚É£ ACCIONES RDD (Ejecutan las transformaciones):\")\n",
        "print()\n",
        "\n",
        "acciones_rdd = \"\"\"\n",
        "# collect() - Traer todos los datos al Driver\n",
        "resultado = rdd.collect()\n",
        "# ‚ö†Ô∏è Peligroso con datasets grandes (OOM)\n",
        "\n",
        "# count() - Contar elementos\n",
        "total = rdd.count()\n",
        "\n",
        "# take(n) - Traer primeros n elementos\n",
        "primeros_5 = rdd.take(5)\n",
        "\n",
        "# reduce() - Reducir a un valor √∫nico\n",
        "suma = rdd.reduce(lambda a, b: a + b)\n",
        "\n",
        "# saveAsTextFile() - Guardar a archivo\n",
        "rdd.saveAsTextFile(\"/output/resultado\")\n",
        "\n",
        "# foreach() - Ejecutar funci√≥n en cada elemento (para side effects)\n",
        "rdd.foreach(lambda x: print(x))\n",
        "\"\"\"\n",
        "\n",
        "print(acciones_rdd)\n",
        "```\n",
        "\n",
        "#### **Ejemplo Completo: Word Count con RDD**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "EJEMPLO CL√ÅSICO: WORD COUNT CON RDD\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìù WORD COUNT CON RDD - PASO A PASO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Simulaci√≥n conceptual (el c√≥digo real ser√≠a en Databricks)\n",
        "\n",
        "word_count_rdd = \"\"\"\n",
        "# Paso 1: Leer archivo de texto\n",
        "texto_rdd = sc.textFile(\"/data/libros/*.txt\")\n",
        "# Lazy - no ejecuta a√∫n\n",
        "\n",
        "# Paso 2: Dividir l√≠neas en palabras (flatMap)\n",
        "palabras_rdd = texto_rdd.flatMap(lambda linea: linea.split(\" \"))\n",
        "# Cada l√≠nea se convierte en m√∫ltiples palabras\n",
        "# Lazy\n",
        "\n",
        "# Paso 3: Crear pares (palabra, 1)\n",
        "pares_rdd = palabras_rdd.map(lambda palabra: (palabra, 1))\n",
        "# \"hola\" ‚Üí (\"hola\", 1)\n",
        "# Lazy\n",
        "\n",
        "# Paso 4: Reducir por palabra (suma conteos)\n",
        "conteo_rdd = pares_rdd.reduceByKey(lambda a, b: a + b)\n",
        "# (\"hola\", 1) + (\"hola\", 1) + (\"hola\", 1) ‚Üí (\"hola\", 3)\n",
        "# Lazy, pero causa SHUFFLE\n",
        "\n",
        "# Paso 5: Ordenar por frecuencia (descendente)\n",
        "ordenado_rdd = conteo_rdd.sortBy(lambda x: x[1], ascending=False)\n",
        "# Lazy, causa SHUFFLE\n",
        "\n",
        "# Paso 6: Tomar top 10 (ACCI√ìN - ejecuta todo)\n",
        "top_10 = ordenado_rdd.take(10)\n",
        "# ‚ö° AQU√ç se ejecuta toda la pipeline\n",
        "\n",
        "# Resultado\n",
        "for palabra, conteo in top_10:\n",
        "    print(f\"{palabra}: {conteo}\")\n",
        "\"\"\"\n",
        "\n",
        "print(word_count_rdd)\n",
        "\n",
        "# Visualizaci√≥n del flujo\n",
        "flujo_visual = \"\"\"\n",
        "VISUALIZACI√ìN DEL FLUJO:\n",
        "\n",
        "DATOS ORIGINALES:\n",
        "[l√≠nea1, l√≠nea2, l√≠nea3, ...]\n",
        "\n",
        "‚Üì flatMap(split)\n",
        "\n",
        "[\"palabra1\", \"palabra2\", \"palabra3\", ...]\n",
        "\n",
        "‚Üì map(x => (x, 1))\n",
        "\n",
        "[(\"palabra1\", 1), (\"palabra2\", 1), (\"palabra1\", 1), ...]\n",
        "\n",
        "‚Üì reduceByKey(+)  [SHUFFLE]\n",
        "\n",
        "[(\"palabra1\", 5), (\"palabra2\", 3), ...]\n",
        "\n",
        "‚Üì sortBy(conteo)  [SHUFFLE]\n",
        "\n",
        "[(\"palabra1\", 5), (\"palabra2\", 3), (\"palabra3\", 2), ...]\n",
        "\n",
        "‚Üì take(10)  [ACCI√ìN]\n",
        "\n",
        "Top 10 palabras m√°s frecuentes\n",
        "\"\"\"\n",
        "\n",
        "print(flujo_visual)\n",
        "```\n",
        "\n",
        "#### **Problemas con RDDs**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "¬øPOR QU√â YA NO USAMOS RDDS?\n",
        "\"\"\"\n",
        "\n",
        "problemas_rdd = {\n",
        "    '1. Sin optimizaci√≥n autom√°tica': \"\"\"\n",
        "        # RDD\n",
        "        rdd.filter(...).map(...).filter(...)\n",
        "        ‚Üí Spark ejecuta exactamente como escribiste\n",
        "        ‚Üí No optimiza el orden de operaciones\n",
        "        \n",
        "        # DataFrame\n",
        "        df.filter(...).select(...).filter(...)\n",
        "        ‚Üí Catalyst Optimizer reordena autom√°ticamente\n",
        "        ‚Üí Ejecuta el plan m√°s eficiente\n",
        "    \"\"\",\n",
        "    \n",
        "    '2. Sin esquema (schema)': \"\"\"\n",
        "        # RDD: Spark no sabe qu√© contiene\n",
        "        rdd = sc.parallelize([{\"nombre\": \"Juan\", \"edad\": 25}])\n",
        "        ‚Üí Spark trata esto como \"objeto gen√©rico\"\n",
        "        ‚Üí No puede optimizar\n",
        "        \n",
        "        # DataFrame: Spark conoce la estructura\n",
        "        df = spark.createDataFrame([{\"nombre\": \"Juan\", \"edad\": 25}])\n",
        "        ‚Üí Spark sabe: columna \"nombre\" (string), \"edad\" (int)\n",
        "        ‚Üí Puede optimizar consultas\n",
        "    \"\"\",\n",
        "    \n",
        "    '3. API compleja': \"\"\"\n",
        "        # RDD: Funciones lambda complejas\n",
        "        rdd.map(lambda x: x[0]).filter(lambda x: x > 10)\n",
        "        \n",
        "        # DataFrame: SQL intuitivo\n",
        "        df.select(\"nombre\").filter(col(\"edad\") > 10)\n",
        "        df.select(\"nombre\").where(\"edad > 10\")  # SQL string\n",
        "    \"\"\",\n",
        "    \n",
        "    '4. Rendimiento': \"\"\"\n",
        "        BENCHMARK (mismo dataset, misma operaci√≥n):\n",
        "        \n",
        "        RDD:        100 segundos\n",
        "        DataFrame:   15 segundos  (6.6x m√°s r√°pido)\n",
        "        \n",
        "        ¬øPor qu√©? Optimizaci√≥n Catalyst + Tungsten execution\n",
        "    \"\"\",\n",
        "    \n",
        "    '5. Solo para Scala/Python avanzado': \"\"\"\n",
        "        RDD requiere pensar en bajo nivel\n",
        "        DataFrames/SQL son universales (analistas, ingenieros, cient√≠ficos)\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "print(\"\\n‚ùå PROBLEMAS CON RDDS:\")\n",
        "print(\"=\"*60)\n",
        "for problema, explicacion in problemas_rdd.items():\n",
        "    print(f\"\\n{problema}\")\n",
        "    print(explicacion)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 2: DataFrame - La Abstracci√≥n Moderna**\n",
        "\n",
        "#### **¬øQu√© es un DataFrame?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DATAFRAME = Tabla distribuida con esquema\n",
        "\"\"\"\n",
        "\n",
        "dataframe_definicion = {\n",
        "    'concepto': 'Tabla con filas y columnas (como Pandas o SQL)',\n",
        "    'esquema': 'Spark conoce el tipo de cada columna',\n",
        "    'distribuido': 'Datos particionados en el cluster',\n",
        "    'inmutable': 'Como RDD, no se modifica (se crean nuevos DFs)',\n",
        "    'optimizado': 'Catalyst Optimizer mejora queries autom√°ticamente',\n",
        "    'api': 'Python, Scala, Java, R, SQL'\n",
        "}\n",
        "\n",
        "print(\"üìä DATAFRAME - LA ABSTRACCI√ìN PREFERIDA\")\n",
        "print(\"=\"*60)\n",
        "for key, value in dataframe_definicion.items():\n",
        "    print(f\"{key.capitalize()}: {value}\")\n",
        "```\n",
        "\n",
        "#### **Anatom√≠a de un DataFrame**\n",
        "\n",
        "```markdown\n",
        "üìä ESTRUCTURA DE UN DATAFRAME\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                DATAFRAME                            ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  SCHEMA (Esquema conocido por Spark)                ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ\n",
        "‚îÇ  ‚îÇ  nombre   ‚îÇ   edad   ‚îÇ  ciudad   ‚îÇ               ‚îÇ\n",
        "‚îÇ  ‚îÇ (String)  ‚îÇ  (Int)   ‚îÇ (String)  ‚îÇ               ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  PARTICIONES (Distribuidas)                         ‚îÇ\n",
        "‚îÇ                                                     ‚îÇ\n",
        "‚îÇ  Worker 1         Worker 2         Worker 3         ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
        "‚îÇ  ‚îÇ Partition 0 ‚îÇ ‚îÇ Partition 1 ‚îÇ ‚îÇ Partition 2 ‚îÇ    ‚îÇ\n",
        "‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ\n",
        "‚îÇ  ‚îÇJuan‚îÇ25‚îÇMad. ‚îÇ ‚îÇAna‚îÇ30‚îÇBarc. ‚îÇ ‚îÇLuis‚îÇ28‚îÇVale‚îÇ‚îÇ    ‚îÇ\n",
        "‚îÇ  ‚îÇMar√≠a‚îÇ32‚îÇSev.‚îÇ ‚îÇPedro‚îÇ27‚îÇBil.‚îÇ ‚îÇLaura‚îÇ35‚îÇMal‚îÇ‚îÇ    ‚îÇ\n",
        "‚îÇ  ‚îÇCarlos‚îÇ29‚îÇM. ‚îÇ ‚îÇSof√≠a‚îÇ26‚îÇMad‚îÇ‚îÇ ‚îÇMiguel‚îÇ31‚îÇB.‚îÇ‚îÇ    ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚úÖ Spark sabe: 3 columnas, tipos de datos, 3 particiones\n",
        "‚úÖ Puede optimizar operaciones bas√°ndose en esta info\n",
        "```\n",
        "\n",
        "#### **Crear DataFrames**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "FORMAS DE CREAR DATAFRAMES EN DATABRICKS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìù CREAR DATAFRAMES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== M√âTODO 1: Desde una lista de tuplas =====\n",
        "print(\"\\n1Ô∏è‚É£ Desde lista de tuplas:\")\n",
        "\n",
        "codigo_1 = \"\"\"\n",
        "# Datos\n",
        "datos = [\n",
        "    (\"Juan\", 25, \"Madrid\"),\n",
        "    (\"Mar√≠a\", 32, \"Sevilla\"),\n",
        "    (\"Carlos\", 29, \"Madrid\")\n",
        "]\n",
        "\n",
        "# Crear DataFrame con esquema inferido\n",
        "df = spark.createDataFrame(datos, [\"nombre\", \"edad\", \"ciudad\"])\n",
        "\n",
        "# Ver resultado\n",
        "df.show()\n",
        "\n",
        "# Output:\n",
        "# +------+----+-------+\n",
        "# |nombre|edad| ciudad|\n",
        "# +------+----+-------+\n",
        "# |  Juan|  25| Madrid|\n",
        "# | Mar√≠a|  32|Sevilla|\n",
        "# |Carlos|  29| Madrid|\n",
        "# +------+----+-------+\n",
        "\"\"\"\n",
        "print(codigo_1)\n",
        "\n",
        "# ===== M√âTODO 2: Desde lista de diccionarios =====\n",
        "print(\"\\n2Ô∏è‚É£ Desde lista de diccionarios:\")\n",
        "\n",
        "codigo_2 = \"\"\"\n",
        "# M√°s expl√≠cito y legible\n",
        "datos = [\n",
        "    {\"nombre\": \"Juan\", \"edad\": 25, \"ciudad\": \"Madrid\"},\n",
        "    {\"nombre\": \"Mar√≠a\", \"edad\": 32, \"ciudad\": \"Sevilla\"},\n",
        "    {\"nombre\": \"Carlos\", \"edad\": 29, \"ciudad\": \"Madrid\"}\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(datos)\n",
        "df.show()\n",
        "\"\"\"\n",
        "print(codigo_2)\n",
        "\n",
        "# ===== M√âTODO 3: Desde archivo CSV =====\n",
        "print(\"\\n3Ô∏è‚É£ Desde archivo CSV:\")\n",
        "\n",
        "codigo_3 = \"\"\"\n",
        "# Leer CSV con opciones\n",
        "df = spark.read \\\\\n",
        "    .format(\"csv\") \\\\\n",
        "    .option(\"header\", \"true\") \\\\\n",
        "    .option(\"inferSchema\", \"true\") \\\\\n",
        "    .load(\"/data/clientes.csv\")\n",
        "\n",
        "# O m√°s corto\n",
        "df = spark.read.csv(\"/data/clientes.csv\", header=True, inferSchema=True)\n",
        "\"\"\"\n",
        "print(codigo_3)\n",
        "\n",
        "# ===== M√âTODO 4: Desde archivo Parquet =====\n",
        "print(\"\\n4Ô∏è‚É£ Desde Parquet (formato columnar optimizado):\")\n",
        "\n",
        "codigo_4 = \"\"\"\n",
        "df = spark.read.parquet(\"/data/ventas.parquet\")\n",
        "\n",
        "# Parquet ya incluye el esquema, no necesitas inferirlo\n",
        "\"\"\"\n",
        "print(codigo_4)\n",
        "\n",
        "# ===== M√âTODO 5: Desde Delta Lake =====\n",
        "print(\"\\n5Ô∏è‚É£ Desde Delta Lake (RECOMENDADO en Databricks):\")\n",
        "\n",
        "codigo_5 = \"\"\"\n",
        "# Leer tabla Delta\n",
        "df = spark.read.format(\"delta\").load(\"/delta/clientes\")\n",
        "\n",
        "# O m√°s directo\n",
        "df = spark.read.table(\"clientes\")  # Si est√° en el cat√°logo\n",
        "\n",
        "# O con SQL\n",
        "df = spark.sql(\"SELECT * FROM clientes\")\n",
        "\"\"\"\n",
        "print(codigo_5)\n",
        "\n",
        "# ===== M√âTODO 6: Desde JSON =====\n",
        "print(\"\\n6Ô∏è‚É£ Desde JSON:\")\n",
        "\n",
        "codigo_6 = \"\"\"\n",
        "df = spark.read.json(\"/data/logs/*.json\")\n",
        "\n",
        "# Spark infiere esquema autom√°ticamente de JSON\n",
        "\"\"\"\n",
        "print(codigo_6)\n",
        "\n",
        "# ===== M√âTODO 7: Con esquema expl√≠cito =====\n",
        "print(\"\\n7Ô∏è‚É£ Con esquema expl√≠cito (mejor pr√°ctica producci√≥n):\")\n",
        "\n",
        "codigo_7 = \"\"\"\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Definir esquema\n",
        "esquema = StructType([\n",
        "    StructField(\"nombre\", StringType(), nullable=False),\n",
        "    StructField(\"edad\", IntegerType(), nullable=False),\n",
        "    StructField(\"ciudad\", StringType(), nullable=True)\n",
        "])\n",
        "\n",
        "# Crear DataFrame con esquema\n",
        "df = spark.createDataFrame(datos, schema=esquema)\n",
        "\n",
        "# Ventaja: Control total, sin inferencia (m√°s r√°pido en producci√≥n)\n",
        "\"\"\"\n",
        "print(codigo_7)\n",
        "```\n",
        "\n",
        "#### **Operaciones con DataFrames**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "OPERACIONES COMUNES CON DATAFRAMES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîß OPERACIONES CON DATAFRAMES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== SELECCI√ìN =====\n",
        "print(\"\\n1Ô∏è‚É£ SELECCI√ìN DE COLUMNAS:\")\n",
        "\n",
        "seleccion = \"\"\"\n",
        "# Seleccionar columnas\n",
        "df.select(\"nombre\", \"edad\")\n",
        "\n",
        "# Con expresiones\n",
        "from pyspark.sql.functions import col, upper\n",
        "\n",
        "df.select(\n",
        "    col(\"nombre\"),\n",
        "    upper(col(\"ciudad\")).alias(\"ciudad_upper\")\n",
        ")\n",
        "\n",
        "# Equivalente SQL\n",
        "df.selectExpr(\"nombre\", \"UPPER(ciudad) as ciudad_upper\")\n",
        "\"\"\"\n",
        "print(seleccion)\n",
        "\n",
        "# ===== FILTRADO =====\n",
        "print(\"\\n2Ô∏è‚É£ FILTRADO:\")\n",
        "\n",
        "filtrado = \"\"\"\n",
        "# M√©todo 1: Con objetos Column\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.filter(col(\"edad\") > 25)\n",
        "df.filter((col(\"edad\") > 25) & (col(\"ciudad\") == \"Madrid\"))\n",
        "\n",
        "# M√©todo 2: Con strings SQL (m√°s legible)\n",
        "df.filter(\"edad > 25\")\n",
        "df.filter(\"edad > 25 AND ciudad = 'Madrid'\")\n",
        "\n",
        "# where() es sin√≥nimo de filter()\n",
        "df.where(\"edad > 25\")\n",
        "\"\"\"\n",
        "print(filtrado)\n",
        "\n",
        "# ===== AGREGAR COLUMNAS =====\n",
        "print(\"\\n3Ô∏è‚É£ AGREGAR/MODIFICAR COLUMNAS:\")\n",
        "\n",
        "agregar = \"\"\"\n",
        "from pyspark.sql.functions import col, lit, when\n",
        "\n",
        "# Agregar columna nueva\n",
        "df.withColumn(\"edad_en_meses\", col(\"edad\") * 12)\n",
        "\n",
        "# Modificar columna existente\n",
        "df.withColumn(\"ciudad\", upper(col(\"ciudad\")))\n",
        "\n",
        "# Columna con l√≥gica condicional\n",
        "df.withColumn(\"categoria\",\n",
        "    when(col(\"edad\") < 18, \"Menor\")\n",
        "    .when(col(\"edad\") < 65, \"Adulto\")\n",
        "    .otherwise(\"Senior\")\n",
        ")\n",
        "\n",
        "# Agregar columna literal\n",
        "df.withColumn(\"pais\", lit(\"Espa√±a\"))\n",
        "\"\"\"\n",
        "print(agregar)\n",
        "\n",
        "# ===== AGREGACIONES =====\n",
        "print(\"\\n4Ô∏è‚É£ AGREGACIONES:\")\n",
        "\n",
        "agregaciones = \"\"\"\n",
        "from pyspark.sql.functions import avg, max, min, count, sum\n",
        "\n",
        "# Agregaci√≥n simple\n",
        "df.agg(\n",
        "    avg(\"edad\").alias(\"edad_promedio\"),\n",
        "    max(\"edad\").alias(\"edad_maxima\"),\n",
        "    count(\"*\").alias(\"total\")\n",
        ")\n",
        "\n",
        "# GroupBy\n",
        "df.groupBy(\"ciudad\").agg(\n",
        "    count(\"*\").alias(\"num_personas\"),\n",
        "    avg(\"edad\").alias(\"edad_promedio\")\n",
        ")\n",
        "\n",
        "# Equivalente SQL\n",
        "df.groupBy(\"ciudad\").count()\n",
        "\"\"\"\n",
        "print(agregaciones)\n",
        "\n",
        "# ===== JOINS =====\n",
        "print(\"\\n5Ô∏è‚É£ JOINS:\")\n",
        "\n",
        "joins = \"\"\"\n",
        "# Inner join (por defecto)\n",
        "df1.join(df2, df1.id == df2.id)\n",
        "df1.join(df2, \"id\")  # Si la columna tiene el mismo nombre\n",
        "\n",
        "# Left join\n",
        "df1.join(df2, \"id\", \"left\")\n",
        "\n",
        "# Tipos de join: inner, left, right, outer, cross, semi, anti\n",
        "\"\"\"\n",
        "print(joins)\n",
        "\n",
        "# ===== ORDENAR =====\n",
        "print(\"\\n6Ô∏è‚É£ ORDENAR:\")\n",
        "\n",
        "ordenar = \"\"\"\n",
        "from pyspark.sql.functions import col, desc\n",
        "\n",
        "# Ascendente\n",
        "df.orderBy(\"edad\")\n",
        "df.orderBy(col(\"edad\").asc())\n",
        "\n",
        "# Descendente\n",
        "df.orderBy(col(\"edad\").desc())\n",
        "\n",
        "# M√∫ltiples columnas\n",
        "df.orderBy(\"ciudad\", col(\"edad\").desc())\n",
        "\"\"\"\n",
        "print(ordenar)\n",
        "```\n",
        "\n",
        "#### **Ejemplo Completo: Word Count con DataFrame**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "WORD COUNT CON DATAFRAME (vs RDD anterior)\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìù WORD COUNT CON DATAFRAME\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "word_count_df = \"\"\"\n",
        "from pyspark.sql.functions import explode, split, col\n",
        "\n",
        "# Paso 1: Leer archivo\n",
        "df_texto = spark.read.text(\"/data/libros/*.txt\")\n",
        "# Schema: [value: string] - una columna llamada \"value\"\n",
        "\n",
        "# Paso 2: Dividir en palabras\n",
        "df_palabras = df_texto.select(\n",
        "    explode(split(col(\"value\"), \" \")).alias(\"palabra\")\n",
        ")\n",
        "# explode() convierte array en filas\n",
        "# split() divide string en array\n",
        "\n",
        "# Paso 3: Agrupar y contar\n",
        "df_conteo = df_palabras.groupBy(\"palabra\").count()\n",
        "\n",
        "# Paso 4: Ordenar\n",
        "df_ordenado = df_conteo.orderBy(col(\"count\").desc())\n",
        "\n",
        "# Paso 5: Mostrar top 10\n",
        "df_ordenado.show(10)\n",
        "\n",
        "# Output:\n",
        "# +--------+-----+\n",
        "# | palabra|count|\n",
        "# +--------+-----+\n",
        "# |     the| 5420|\n",
        "# |      of| 3890|\n",
        "# |     and| 3120|\n",
        "# |      to| 2890|\n",
        "# |       a| 2560|\n",
        "# |      in| 2340|\n",
        "# |      is| 1980|\n",
        "# |    that| 1870|\n",
        "# |      it| 1690|\n",
        "# |     for| 1560|\n",
        "# +--------+-----+\n",
        "\n",
        "‚úÖ Mucho m√°s legible que RDD\n",
        "‚úÖ Optimizado autom√°ticamente por Catalyst\n",
        "‚úÖ M√°s r√°pido en ejecuci√≥n\n",
        "\"\"\"\n",
        "\n",
        "print(word_count_df)\n",
        "```\n",
        "\n",
        "#### **DataFrame vs RDD: Comparaci√≥n Directa**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "COMPARACI√ìN LADO A LADO\n",
        "\"\"\"\n",
        "\n",
        "comparacion = \"\"\"\n",
        "TAREA: Filtrar personas mayores de 25 a√±os en Madrid y contar\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "RDD (C√≥digo complejo, sin optimizaci√≥n)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "rdd = sc.parallelize([\n",
        "    {\"nombre\": \"Juan\", \"edad\": 30, \"ciudad\": \"Madrid\"},\n",
        "    {\"nombre\": \"Mar√≠a\", \"edad\": 22, \"ciudad\": \"Madrid\"},\n",
        "    {\"nombre\": \"Carlos\", \"edad\": 28, \"ciudad\": \"Barcelona\"}\n",
        "])\n",
        "\n",
        "resultado = rdd \\\\\n",
        "    .filter(lambda x: x[\"edad\"] > 25) \\\\\n",
        "    .filter(lambda x: x[\"ciudad\"] == \"Madrid\") \\\\\n",
        "    .count()\n",
        "\n",
        "print(resultado)  # 1\n",
        "\n",
        "‚ùå Problemas:\n",
        "   - Lambdas complejas\n",
        "   - Sin optimizaci√≥n autom√°tica\n",
        "   - Ejecuta 2 filter por separado\n",
        "   - Sin pushdown de predicados\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "DATAFRAME (C√≥digo simple, optimizado)\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    {\"nombre\": \"Juan\", \"edad\": 30, \"ciudad\": \"Madrid\"},\n",
        "    {\"nombre\": \"Mar√≠a\", \"edad\": 22, \"ciudad\": \"Madrid\"},\n",
        "    {\"nombre\": \"Carlos\", \"edad\": 28, \"ciudad\": \"Barcelona\"}\n",
        "])\n",
        "\n",
        "# Opci√≥n 1: API DataFrame\n",
        "resultado = df \\\\\n",
        "    .filter((col(\"edad\") > 25) & (col(\"ciudad\") == \"Madrid\")) \\\\\n",
        "    .count()\n",
        "\n",
        "# Opci√≥n 2: SQL puro (a√∫n m√°s simple)\n",
        "df.createOrReplaceTempView(\"personas\")\n",
        "resultado = spark.sql('''\n",
        "    SELECT COUNT(*)\n",
        "    FROM personas\n",
        "    WHERE edad > 25 AND ciudad = 'Madrid'\n",
        "''').collect()[0][0]\n",
        "\n",
        "print(resultado)  # 1\n",
        "\n",
        "‚úÖ Ventajas:\n",
        "   - C√≥digo m√°s legible\n",
        "   - Catalyst combina ambos filtros en uno\n",
        "   - Predicate pushdown (filtra al leer)\n",
        "   - 3-10x m√°s r√°pido en ejecuci√≥n\n",
        "\"\"\"\n",
        "\n",
        "print(comparacion)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 3: Dataset - La Abstracci√≥n Type-Safe (Solo Scala/Java)**\n",
        "\n",
        "#### **¬øQu√© es un Dataset?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DATASET = DataFrame + Type Safety\n",
        "\"\"\"\n",
        "\n",
        "dataset_explicacion = \"\"\"\n",
        "üì¶ DATASET (Solo Scala/Java)\n",
        "\n",
        "Dataset combina:\n",
        "‚úÖ Optimizaci√≥n de DataFrame (Catalyst)\n",
        "‚úÖ Type-safety de RDD (errores en compilaci√≥n, no runtime)\n",
        "\n",
        "Ejemplo en Scala:\n",
        "\n",
        "// Definir case class (tipo)\n",
        "case class Persona(nombre: String, edad: Int, ciudad: String)\n",
        "\n",
        "// Crear Dataset tipado\n",
        "val ds: Dataset[Persona] = spark.read.json(\"/data\").as[Persona]\n",
        "\n",
        "// Operations type-safe\n",
        "ds.filter(_.edad > 25)  // Compilador sabe que 'edad' existe\n",
        "ds.map(_.nombre)        // Compilador sabe que retorna String\n",
        "\n",
        "// Error en compilaci√≥n (no en runtime)\n",
        "ds.filter(_.edad_invalida > 25)  // ‚ùå Error: 'edad_invalida' no existe\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "‚ö†Ô∏è EN PYTHON NO HAY DATASETS\n",
        "   Python es din√°micamente tipado, no hay type-safety en compilaci√≥n\n",
        "   En PySpark, siempre trabajas con DataFrames\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "CUANDO USAR DATASET (Scala):\n",
        "‚úÖ Aplicaciones cr√≠ticas que necesitan type-safety\n",
        "‚úÖ Grandes equipos (catches errores temprano)\n",
        "‚úÖ APIs complejas (el tipo documenta el c√≥digo)\n",
        "\n",
        "CUANDO USAR DATAFRAME:\n",
        "‚úÖ Python (√∫nica opci√≥n)\n",
        "‚úÖ SQL (m√°s simple)\n",
        "‚úÖ An√°lisis exploratorio\n",
        "‚úÖ BI y reporting\n",
        "‚úÖ 95% de los casos\n",
        "\"\"\"\n",
        "\n",
        "print(dataset_explicacion)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparaci√≥n Completa: RDD vs DataFrame vs Dataset**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "COMPARACI√ìN EXHAUSTIVA\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "comparacion_tabla = pd.DataFrame({\n",
        "    'Caracter√≠stica': [\n",
        "        'Lenguajes',\n",
        "        'Type-safe',\n",
        "        'Optimizaci√≥n',\n",
        "        'API',\n",
        "        'Performance',\n",
        "        'Uso en 2025',\n",
        "        'Curva aprendizaje',\n",
        "        'Errores detectados',\n",
        "        'Spark UI'\n",
        "    ],\n",
        "    'RDD': [\n",
        "        'Python, Scala, Java',\n",
        "        '‚ùå No',\n",
        "        '‚ùå Manual',\n",
        "        'Bajo nivel (lambdas)',\n",
        "        '‚≠ê‚≠ê Lento',\n",
        "        '1% (legacy)',\n",
        "        'üî¥ Alta',\n",
        "        'Runtime',\n",
        "        'Limitada'\n",
        "    ],\n",
        "    'DataFrame': [\n",
        "        'Python, Scala, Java, R, SQL',\n",
        "        '‚ùå No',\n",
        "        '‚úÖ Catalyst + Tungsten',\n",
        "        'Alto nivel (SQL-like)',\n",
        "        '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê R√°pido',\n",
        "        '95% (est√°ndar)',\n",
        "        'üü¢ Baja',\n",
        "        'Runtime',\n",
        "        'Excelente'\n",
        "    ],\n",
        "    'Dataset': [\n",
        "        'Solo Scala, Java',\n",
        "        '‚úÖ S√≠',\n",
        "        '‚úÖ Catalyst + Tungsten',\n",
        "        'Alto nivel + tipos',\n",
        "        '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê R√°pido',\n",
        "        '4% (Scala apps)',\n",
        "        'üü° Media',\n",
        "        'Compile-time',\n",
        "        'Excelente'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nüìä COMPARACI√ìN COMPLETA\")\n",
        "print(\"=\"*60)\n",
        "print(comparacion_tabla.to_string(index=False))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Trabajando con DataFrames**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "EJERCICIO PR√ÅCTICO: CREAR Y MANIPULAR DATAFRAMES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüß™ PR√ÅCTICA: OPERACIONES CON DATAFRAMES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== CREAR DATASET DE EJEMPLO =====\n",
        "print(\"\\n1Ô∏è‚É£ Crear DataFrame de ejemplo:\")\n",
        "\n",
        "# Simular datos de una tienda online\n",
        "datos_ventas = [\n",
        "    {\"producto\": \"Laptop\", \"precio\": 1000, \"cantidad\": 2, \"ciudad\": \"Madrid\"},\n",
        "    {\"producto\": \"Mouse\", \"precio\": 20, \"cantidad\": 5, \"ciudad\": \"Barcelona\"},\n",
        "    {\"producto\": \"Teclado\", \"precio\": 50, \"cantidad\": 3, \"ciudad\": \"Madrid\"},\n",
        "    {\"producto\": \"Monitor\", \"precio\": 300, \"cantidad\": 1, \"ciudad\": \"Valencia\"},\n",
        "    {\"producto\": \"Laptop\", \"precio\": 1000, \"cantidad\": 1, \"ciudad\": \"Barcelona\"},\n",
        "    {\"producto\": \"Mouse\", \"precio\": 20, \"cantidad\": 10, \"ciudad\": \"Madrid\"},\n",
        "]\n",
        "\n",
        "# En Databricks, har√≠as:\n",
        "# df_ventas = spark.createDataFrame(datos_ventas)\n",
        "# df_ventas.show()\n",
        "\n",
        "print(\"DataFrame creado con ventas de productos\")\n",
        "\n",
        "# ===== OPERACIONES COMUNES =====\n",
        "print(\"\\n2Ô∏è‚É£ Operaciones que practicar√≠as:\")\n",
        "\n",
        "ejercicios = \"\"\"\n",
        "# A. Calcular total de venta (precio * cantidad)\n",
        "df_ventas = df_ventas.withColumn(\"total\", col(\"precio\") * col(\"cantidad\"))\n",
        "\n",
        "# B. Ventas por ciudad\n",
        "df_por_ciudad = df_ventas.groupBy(\"ciudad\").agg(\n",
        "    sum(\"total\").alias(\"total_ventas\"),\n",
        "    count(\"*\").alias(\"num_transacciones\")\n",
        ")\n",
        "\n",
        "# C. Top 3 productos por ingresos\n",
        "df_por_producto = df_ventas.groupBy(\"producto\").agg(\n",
        "    sum(\"total\").alias(\"ingresos_totales\")\n",
        ").orderBy(col(\"ingresos_totales\").desc()).limit(3)\n",
        "\n",
        "# D. Filtrar ventas > 100‚Ç¨\n",
        "df_ventas_grandes = df_ventas.filter(col(\"total\") > 100)\n",
        "\n",
        "# E. Agregar categor√≠a de venta\n",
        "df_ventas = df_ventas.withColumn(\"categoria\",\n",
        "    when(col(\"total\") < 50, \"Peque√±a\")\n",
        "    .when(col(\"total\") < 500, \"Mediana\")\n",
        "    .otherwise(\"Grande\")\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "print(ejercicios)\n",
        "\n",
        "# ===== RESULTADOS ESPERADOS =====\n",
        "print(\"\\n3Ô∏è‚É£ Resultados esperados:\")\n",
        "\n",
        "resultados = \"\"\"\n",
        "A. Con columna 'total':\n",
        "+-------+------+--------+---------+-----+\n",
        "|producto|precio|cantidad|   ciudad|total|\n",
        "+--------+------+--------+---------+-----+\n",
        "|  Laptop| 1000|       2|   Madrid| 2000|\n",
        "|   Mouse|   20|       5|Barcelona|  100|\n",
        "|Teclado |   50|       3|   Madrid|  150|\n",
        "| Monitor|  300|       1| Valencia|  300|\n",
        "|  Laptop| 1000|       1|Barcelona| 1000|\n",
        "|   Mouse|   20|      10|   Madrid|  200|\n",
        "+--------+------+--------+---------+-----+\n",
        "\n",
        "B. Ventas por ciudad:\n",
        "+---------+------------+-----------------+\n",
        "|   ciudad|total_ventas|num_transacciones|\n",
        "+---------+------------+-----------------+\n",
        "|   Madrid|        2350|                3|\n",
        "|Barcelona|        1100|                2|\n",
        "| Valencia|         300|                1|\n",
        "+---------+------------+-----------------+\n",
        "\n",
        "C. Top 3 productos:\n",
        "+--------+----------------+\n",
        "|producto|ingresos_totales|\n",
        "+--------+----------------+\n",
        "|  Laptop|            3000|\n",
        "|   Mouse|             300|\n",
        "| Monitor|             300|\n",
        "+--------+----------------+\n",
        "\"\"\"\n",
        "\n",
        "print(resultados)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 2.2 - Conversi√≥n RDD ‚Üî DataFrame**\n",
        "\n",
        "```markdown\n",
        "### Ejercicio: Entiende las diferencias\n",
        "\n",
        "**Tarea**: Dado el siguiente c√≥digo RDD, reescr√≠belo usando DataFrame\n",
        "\n",
        "```python\n",
        "# C√ìDIGO RDD (proporcionado)\n",
        "rdd = sc.parallelize([\n",
        "    (\"Juan\", 1000),\n",
        "    (\"Mar√≠a\", 1500),\n",
        "    (\"Juan\", 500),\n",
        "    (\"Pedro\", 2000),\n",
        "    (\"Mar√≠a\", 800)\n",
        "])\n",
        "\n",
        "# Calcular total por persona\n",
        "resultado_rdd = rdd.reduceByKey(lambda a, b: a + b)\n",
        "top_3 = resultado_rdd.takeOrdered(3, key=lambda x: -x[1])\n",
        "```\n",
        "\n",
        "**Tu soluci√≥n con DataFrame**:\n",
        "```python\n",
        "# Completar aqu√≠\n",
        "datos = [\n",
        "    # ...\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(...)\n",
        "\n",
        "# ...\n",
        "```\n",
        "\n",
        "**Preguntas**:\n",
        "1. ¬øCu√°l es m√°s legible? ¬øPor qu√©?\n",
        "2. ¬øCu√°l ser√° m√°s r√°pido? ¬øPor qu√©?\n",
        "3. ¬øCu√°ndo usar√≠as RDD sobre DataFrame?\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **RDD** = Bajo nivel, control total, sin optimizaci√≥n (legacy)\n",
        "2. **DataFrame** = Alto nivel, optimizado, 95% de casos de uso\n",
        "3. **Dataset** = Type-safe, solo Scala/Java, para apps empresariales\n",
        "4. **En Databricks/PySpark ‚Üí SIEMPRE usa DataFrames**\n",
        "5. **DataFrame tiene esquema** ‚Üí Spark optimiza autom√°ticamente\n",
        "6. **Catalyst Optimizer** hace DataFrames 3-10x m√°s r√°pidos que RDDs\n",
        "7. **API DataFrame** es intuitiva (similar a SQL y Pandas)\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **Nunca uses RDD en c√≥digo nuevo**\n",
        "   - Solo si mantienes c√≥digo legacy\n",
        "   - DataFrames son superiores en todo\n",
        "\n",
        "‚úÖ **Aprende SQL si usas DataFrames**\n",
        "   - Muchas operaciones son m√°s claras en SQL\n",
        "   - `df.filter(\"edad > 25\")` vs `df.filter(col(\"edad\") > 25)`\n",
        "\n",
        "‚úÖ **Usa .explain() para entender planes**\n",
        "   - `df.explain()` muestra c√≥mo Spark ejecutar√° tu query\n",
        "   - √ötil para debugging y optimizaci√≥n\n",
        "\n",
        "‚úÖ **Convierte RDD ‚Üí DataFrame si es necesario**\n",
        "   - `df = rdd.toDF([\"col1\", \"col2\"])`\n",
        "   - Ganas optimizaci√≥n inmediata\n",
        "\n",
        "‚úÖ **DataFrames = SQL distribuido**\n",
        "   - Si sabes SQL, ya sabes DataFrames\n",
        "   - Todo lo de SQL funciona en DataFrames\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para el siguiente punto **2.3 Transformaciones y acciones en Spark (lazy evaluation)**?"
      ],
      "metadata": {
        "id": "NNKepX1PYBPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3 Transformaciones y acciones en Spark (lazy evaluation)**\n",
        "\n",
        "#### **Introducci√≥n: El Secreto de la Velocidad de Spark**\n",
        "\n",
        "Uno de los conceptos m√°s importantes (y a veces confuso) de Spark es la **lazy evaluation** (evaluaci√≥n perezosa). Entender esto es crucial para escribir c√≥digo Spark eficiente.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "LA PARADOJA DE SPARK\n",
        "\"\"\"\n",
        "\n",
        "paradoja = \"\"\"\n",
        "ü§î PARADOJA:\n",
        "\n",
        "# Escribes este c√≥digo:\n",
        "df = spark.read.parquet(\"/data/100GB.parquet\")  # ¬øSe leen 100 GB?\n",
        "df_filtrado = df.filter(\"edad > 25\")             # ¬øSe filtra ahora?\n",
        "df_seleccionado = df_filtrado.select(\"nombre\")   # ¬øSe selecciona ahora?\n",
        "\n",
        "print(\"¬øCu√°nto tiempo tard√≥?\")\n",
        "# Respuesta: ¬°Milisegundos! üò≤\n",
        "\n",
        "# Pero... ¬øc√≥mo puede procesar 100 GB en milisegundos?\n",
        "\n",
        "üéØ RESPUESTA: ¬°NO LO HIZO!\n",
        "\n",
        "Spark solo \"registr√≥\" qu√© quieres hacer, pero NO ejecut√≥ nada.\n",
        "Solo cuando ejecutas una ACCI√ìN, Spark realmente trabaja.\n",
        "\n",
        "Este concepto se llama LAZY EVALUATION\n",
        "\"\"\"\n",
        "\n",
        "print(paradoja)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 1: Transformaciones (Lazy)**\n",
        "\n",
        "#### **¬øQu√© son las Transformaciones?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "TRANSFORMACIONES = Operaciones que crean un nuevo DataFrame\n",
        "\"\"\"\n",
        "\n",
        "transformaciones_definicion = {\n",
        "    'concepto': 'Operaciones que definen C√ìMO transformar los datos',\n",
        "    'ejecuci√≥n': '‚ùå NO se ejecutan inmediatamente (lazy)',\n",
        "    'retorno': 'Nuevo DataFrame (inmutable)',\n",
        "    'prop√≥sito': 'Construir un plan de ejecuci√≥n',\n",
        "    'ejemplos': ['select', 'filter', 'groupBy', 'join', 'orderBy', 'withColumn']\n",
        "}\n",
        "\n",
        "print(\"üîÑ TRANSFORMACIONES (LAZY)\")\n",
        "print(\"=\"*60)\n",
        "for key, value in transformaciones_definicion.items():\n",
        "    print(f\"{key.capitalize()}: {value}\")\n",
        "```\n",
        "\n",
        "#### **Categor√≠as de Transformaciones**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "TIPOS DE TRANSFORMACIONES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìö CATEGOR√çAS DE TRANSFORMACIONES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== 1. NARROW TRANSFORMATIONS (Sin Shuffle) =====\n",
        "print(\"\\n1Ô∏è‚É£ NARROW TRANSFORMATIONS (R√°pidas, sin shuffle):\")\n",
        "\n",
        "narrow = \"\"\"\n",
        "NARROW = Cada partici√≥n de entrada genera UNA partici√≥n de salida\n",
        "         No requiere movimiento de datos entre workers\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Partition 0 ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ Partition 0'‚îÇ\n",
        "‚îÇ  [1,2,3]    ‚îÇ         ‚îÇ  [2,4,6]    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Partition 1 ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ Partition 1'‚îÇ\n",
        "‚îÇ  [4,5,6]    ‚îÇ         ‚îÇ  [8,10,12]  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚úÖ Cada worker procesa sus datos independientemente\n",
        "‚úÖ No hay comunicaci√≥n entre workers\n",
        "‚úÖ Muy eficiente\n",
        "\n",
        "EJEMPLOS:\n",
        "\"\"\"\n",
        "\n",
        "ejemplos_narrow = \"\"\"\n",
        "# select() - Seleccionar columnas\n",
        "df.select(\"nombre\", \"edad\")\n",
        "\n",
        "# filter() / where() - Filtrar filas\n",
        "df.filter(col(\"edad\") > 25)\n",
        "df.where(\"ciudad = 'Madrid'\")\n",
        "\n",
        "# withColumn() - Agregar/modificar columna\n",
        "df.withColumn(\"edad_doble\", col(\"edad\") * 2)\n",
        "\n",
        "# drop() - Eliminar columnas\n",
        "df.drop(\"columna_innecesaria\")\n",
        "\n",
        "# withColumnRenamed() - Renombrar\n",
        "df.withColumnRenamed(\"edad\", \"age\")\n",
        "\n",
        "# map() - Transformaci√≥n fila por fila (RDD-style)\n",
        "df.rdd.map(lambda row: row.edad * 2)\n",
        "\n",
        "# flatMap() - Similar a map pero retorna m√∫ltiples valores\n",
        "df.select(explode(split(\"texto\", \" \")))\n",
        "\n",
        "# union() - Unir DataFrames (mismo esquema)\n",
        "df1.union(df2)\n",
        "\n",
        "# sample() - Muestreo\n",
        "df.sample(fraction=0.1)\n",
        "\"\"\"\n",
        "\n",
        "print(narrow)\n",
        "print(ejemplos_narrow)\n",
        "\n",
        "# ===== 2. WIDE TRANSFORMATIONS (Con Shuffle) =====\n",
        "print(\"\\n2Ô∏è‚É£ WIDE TRANSFORMATIONS (Lentas, requieren shuffle):\")\n",
        "\n",
        "wide = \"\"\"\n",
        "WIDE = M√∫ltiples particiones de entrada pueden contribuir a\n",
        "       UNA partici√≥n de salida. Requiere SHUFFLE.\n",
        "\n",
        "ANTES DEL SHUFFLE:\n",
        "Worker 1: [(\"Madrid\", 1), (\"Barcelona\", 2)]\n",
        "Worker 2: [(\"Madrid\", 3), (\"Valencia\", 4)]\n",
        "\n",
        "SHUFFLE (reagrupar por ciudad):\n",
        "\n",
        "DESPU√âS DEL SHUFFLE:\n",
        "Worker 1: [(\"Madrid\", 1), (\"Madrid\", 3)]      ‚Üê De workers 1 y 2\n",
        "Worker 2: [(\"Barcelona\", 2), (\"Valencia\", 4)]\n",
        "\n",
        "‚ö†Ô∏è Datos se mueven por la red entre workers\n",
        "‚ö†Ô∏è Operaci√≥n costosa pero a veces necesaria\n",
        "\n",
        "EJEMPLOS:\n",
        "\"\"\"\n",
        "\n",
        "ejemplos_wide = \"\"\"\n",
        "# groupBy() - Agrupar\n",
        "df.groupBy(\"ciudad\").count()\n",
        "df.groupBy(\"ciudad\").agg(avg(\"edad\"))\n",
        "\n",
        "# join() - Unir DataFrames\n",
        "df1.join(df2, \"id\")\n",
        "df1.join(df2, df1.id == df2.customer_id, \"left\")\n",
        "\n",
        "# distinct() - Eliminar duplicados\n",
        "df.distinct()\n",
        "df.dropDuplicates([\"nombre\", \"edad\"])\n",
        "\n",
        "# orderBy() / sort() - Ordenar globalmente\n",
        "df.orderBy(\"edad\")\n",
        "df.sort(col(\"salario\").desc())\n",
        "\n",
        "# repartition() - Cambiar n√∫mero de particiones\n",
        "df.repartition(100)\n",
        "df.repartition(\"ciudad\")  # Particionar por columna\n",
        "\n",
        "# coalesce() - Reducir particiones (sin shuffle completo)\n",
        "df.coalesce(10)  # M√°s eficiente que repartition para reducir\n",
        "\n",
        "# intersect() - Intersecci√≥n de DataFrames\n",
        "df1.intersect(df2)\n",
        "\n",
        "# subtract() - Diferencia\n",
        "df1.subtract(df2)\n",
        "\"\"\"\n",
        "\n",
        "print(wide)\n",
        "print(ejemplos_wide)\n",
        "```\n",
        "\n",
        "#### **Transformaciones m√°s Usadas**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "TOP 10 TRANSFORMACIONES M√ÅS UTILIZADAS\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n‚≠ê TOP 10 TRANSFORMACIONES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "top_transformaciones = \"\"\"\n",
        "1. select() - Seleccionar columnas\n",
        "   df.select(\"nombre\", \"edad\")\n",
        "   df.select(col(\"nombre\"), (col(\"edad\") * 2).alias(\"edad_doble\"))\n",
        "\n",
        "2. filter() / where() - Filtrar filas\n",
        "   df.filter(col(\"edad\") > 25)\n",
        "   df.where(\"edad > 25 AND ciudad = 'Madrid'\")\n",
        "\n",
        "3. withColumn() - Agregar/modificar columna\n",
        "   df.withColumn(\"categoria\",\n",
        "       when(col(\"edad\") < 18, \"Menor\")\n",
        "       .when(col(\"edad\") < 65, \"Adulto\")\n",
        "       .otherwise(\"Senior\"))\n",
        "\n",
        "4. groupBy() + agg() - Agregaciones\n",
        "   df.groupBy(\"ciudad\").agg(\n",
        "       count(\"*\").alias(\"total\"),\n",
        "       avg(\"edad\").alias(\"edad_promedio\"),\n",
        "       max(\"salario\").alias(\"salario_max\")\n",
        "   )\n",
        "\n",
        "5. join() - Combinar DataFrames\n",
        "   df_empleados.join(df_departamentos, \"dept_id\", \"left\")\n",
        "\n",
        "6. orderBy() / sort() - Ordenar\n",
        "   df.orderBy(col(\"salario\").desc(), \"nombre\")\n",
        "\n",
        "7. drop() - Eliminar columnas\n",
        "   df.drop(\"columna_temporal\", \"otra_columna\")\n",
        "\n",
        "8. dropDuplicates() - Eliminar duplicados\n",
        "   df.dropDuplicates()  # Todas las columnas\n",
        "   df.dropDuplicates([\"email\"])  # Por columnas espec√≠ficas\n",
        "\n",
        "9. union() / unionByName() - Combinar DataFrames verticalmente\n",
        "   df1.union(df2)  # Mismo orden de columnas\n",
        "   df1.unionByName(df2)  # Por nombre de columna (m√°s seguro)\n",
        "\n",
        "10. explode() - Convertir arrays/maps en filas\n",
        "    df.select(\"id\", explode(\"lista_items\").alias(\"item\"))\n",
        "\"\"\"\n",
        "\n",
        "print(top_transformaciones)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 2: Acciones (Eager)**\n",
        "\n",
        "#### **¬øQu√© son las Acciones?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ACCIONES = Operaciones que EJECUTAN el plan y retornan resultados\n",
        "\"\"\"\n",
        "\n",
        "acciones_definicion = {\n",
        "    'concepto': 'Operaciones que EJECUTAN las transformaciones pendientes',\n",
        "    'ejecuci√≥n': '‚úÖ Se ejecutan INMEDIATAMENTE (eager)',\n",
        "    'retorno': 'Datos al Driver o almacenamiento (NO DataFrame)',\n",
        "    'prop√≥sito': 'Materializar resultados',\n",
        "    'trigger': 'Disparan toda la pipeline de transformaciones',\n",
        "    'ejemplos': ['show', 'count', 'collect', 'write', 'take', 'first']\n",
        "}\n",
        "\n",
        "print(\"\\n‚ö° ACCIONES (EAGER)\")\n",
        "print(\"=\"*60)\n",
        "for key, value in acciones_definicion.items():\n",
        "    print(f\"{key.capitalize()}: {value}\")\n",
        "```\n",
        "\n",
        "#### **Acciones m√°s Comunes**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "ACCIONES PRINCIPALES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìã ACCIONES M√ÅS COMUNES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "acciones_comunes = \"\"\"\n",
        "1. show(n) - Mostrar primeras n filas (default: 20)\n",
        "   df.show()\n",
        "   df.show(5)\n",
        "   df.show(10, truncate=False)  # Sin truncar strings largos\n",
        "   \n",
        "   ‚ö†Ô∏è Trae datos al Driver (cuidado con datasets grandes)\n",
        "\n",
        "2. count() - Contar filas\n",
        "   total = df.count()\n",
        "   print(f\"Total filas: {total}\")\n",
        "   \n",
        "   ‚úÖ No trae datos al Driver, solo el n√∫mero\n",
        "\n",
        "3. collect() - Traer TODOS los datos al Driver\n",
        "   filas = df.collect()  # Lista de Row objects\n",
        "   for fila in filas:\n",
        "       print(fila.nombre, fila.edad)\n",
        "   \n",
        "   ‚ö†Ô∏è PELIGROSO: Puede causar OutOfMemory si dataset es grande\n",
        "   ‚ö†Ô∏è NUNCA uses collect() en producci√≥n con datos grandes\n",
        "\n",
        "4. take(n) - Traer primeras n filas al Driver\n",
        "   primeras_10 = df.take(10)\n",
        "   \n",
        "   ‚úÖ M√°s seguro que collect() (l√≠mite expl√≠cito)\n",
        "\n",
        "5. first() / head() - Primera fila\n",
        "   primera_fila = df.first()\n",
        "   print(primera_fila.nombre)\n",
        "   \n",
        "   ‚úÖ Equivalente a take(1)[0]\n",
        "\n",
        "6. write - Escribir a almacenamiento\n",
        "   df.write.parquet(\"/output/datos.parquet\")\n",
        "   df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/tabla\")\n",
        "   \n",
        "   ‚úÖ Escribe distribuido (no pasa por Driver)\n",
        "\n",
        "7. saveAsTable() - Guardar como tabla\n",
        "   df.write.saveAsTable(\"mi_tabla\")\n",
        "   \n",
        "   ‚úÖ Disponible en cat√°logo de Databricks\n",
        "\n",
        "8. foreach() / foreachPartition() - Ejecutar funci√≥n en cada fila/partici√≥n\n",
        "   df.foreach(lambda row: enviar_a_api(row))\n",
        "   \n",
        "   ‚ö†Ô∏è Para side effects (escribir a DB externa, llamar APIs)\n",
        "\n",
        "9. reduce() - Reducir a un valor √∫nico\n",
        "   suma_edades = df.select(\"edad\").rdd.map(lambda r: r[0]).reduce(lambda a,b: a+b)\n",
        "   \n",
        "   ‚ö†Ô∏è Raro con DataFrames, m√°s com√∫n con RDDs\n",
        "\n",
        "10. toPandas() - Convertir a Pandas DataFrame\n",
        "    pdf = df.toPandas()\n",
        "    \n",
        "    ‚ö†Ô∏è PELIGROSO: Trae TODO al Driver\n",
        "    ‚úÖ √ötil para datasets peque√±os o despu√©s de agregaciones\n",
        "\"\"\"\n",
        "\n",
        "print(acciones_comunes)\n",
        "```\n",
        "\n",
        "#### **Acciones: Cu√°ndo Usarlas**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "GU√çA DE USO DE ACCIONES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüí° CU√ÅNDO USAR CADA ACCI√ìN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "guia_acciones = \"\"\"\n",
        "‚úÖ DESARROLLO / EXPLORACI√ìN:\n",
        "   ‚Ä¢ show() - Ver primeras filas r√°pidamente\n",
        "   ‚Ä¢ take(10) - Inspeccionar datos\n",
        "   ‚Ä¢ count() - Verificar tama√±o\n",
        "   ‚Ä¢ describe() - Estad√≠sticas descriptivas\n",
        "\n",
        "‚úÖ PRODUCCI√ìN / ETL:\n",
        "   ‚Ä¢ write() - Persistir resultados (Delta, Parquet)\n",
        "   ‚Ä¢ saveAsTable() - Guardar en cat√°logo\n",
        "   ‚Ä¢ foreach() - Side effects controlados\n",
        "\n",
        "‚ùå EVITAR EN PRODUCCI√ìN:\n",
        "   ‚Ä¢ collect() - ¬°NUNCA con datos grandes!\n",
        "   ‚Ä¢ toPandas() - Solo despu√©s de agregaciones fuertes\n",
        "   \n",
        "‚ö†Ô∏è REGLA DE ORO:\n",
        "   Si tu dataset no cabe en la memoria del Driver ‚Üí NO uses collect()\n",
        "   \n",
        "   Driver t√≠pico: 16-64 GB RAM\n",
        "   Dataset t√≠pico en big data: 100 GB - varios TB\n",
        "   \n",
        "   collect() causar√°: OutOfMemoryError üí•\n",
        "\"\"\"\n",
        "\n",
        "print(guia_acciones)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Parte 3: Lazy Evaluation - El Coraz√≥n de Spark**\n",
        "\n",
        "#### **¬øQu√© es Lazy Evaluation?**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "LAZY EVALUATION = Evaluaci√≥n Perezosa\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüõèÔ∏è LAZY EVALUATION EXPLICADA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "lazy_explicacion = \"\"\"\n",
        "CONCEPTO:\n",
        "Spark NO ejecuta transformaciones inmediatamente.\n",
        "En su lugar, construye un \"plan de ejecuci√≥n\" y solo\n",
        "lo ejecuta cuando encuentra una ACCI√ìN.\n",
        "\n",
        "ANALOG√çA: Lista de compras\n",
        "\n",
        "‚ùå EJECUCI√ìN INMEDIATA (No lazy):\n",
        "Tu: \"Compra leche\"\n",
        "Amigo: *Va al super, compra leche, vuelve*\n",
        "Tu: \"Compra pan\"\n",
        "Amigo: *Va al super, compra pan, vuelve*\n",
        "Tu: \"Compra huevos\"\n",
        "Amigo: *Va al super, compra huevos, vuelve*\n",
        "\n",
        "‚Üí 3 viajes al supermercado üò´\n",
        "\n",
        "‚úÖ EVALUACI√ìN LAZY (Spark):\n",
        "Tu: \"Compra leche\" ‚Üí Anota en lista\n",
        "Tu: \"Compra pan\" ‚Üí Anota en lista\n",
        "Tu: \"Compra huevos\" ‚Üí Anota en lista\n",
        "Tu: \"Ahora ve al super\" (ACCI√ìN)\n",
        "Amigo: *Va una vez, compra todo, vuelve*\n",
        "\n",
        "‚Üí 1 viaje al supermercado üòé\n",
        "\n",
        "BENEFICIOS:\n",
        "1. Optimizaci√≥n autom√°tica (ver toda la pipeline)\n",
        "2. Evita trabajo innecesario\n",
        "3. Ejecuci√≥n eficiente\n",
        "\"\"\"\n",
        "\n",
        "print(lazy_explicacion)\n",
        "```\n",
        "\n",
        "#### **Ejemplo Paso a Paso: Lazy Evaluation**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "EJEMPLO PR√ÅCTICO: LAZY EVALUATION EN ACCI√ìN\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüî¨ EJEMPLO: LAZY EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ejemplo_lazy = \"\"\"\n",
        "# Datos: 1 mill√≥n de registros de empleados (100 MB)\n",
        "\n",
        "import time\n",
        "\n",
        "# ===== PASO 1: Leer datos (TRANSFORMACI√ìN - LAZY) =====\n",
        "inicio = time.time()\n",
        "df = spark.read.parquet(\"/data/empleados.parquet\")\n",
        "tiempo_lectura = time.time() - inicio\n",
        "\n",
        "print(f\"Tiempo lectura: {tiempo_lectura:.3f} segundos\")\n",
        "# Output: Tiempo lectura: 0.001 segundos  ‚Üê ¬°Solo milisegundos!\n",
        "\n",
        "# ü§î ¬øC√≥mo ley√≥ 100 MB en 1ms?\n",
        "# Respuesta: ¬°NO LO HIZO! Solo ley√≥ metadata\n",
        "\n",
        "# ===== PASO 2: Filtrar (TRANSFORMACI√ìN - LAZY) =====\n",
        "inicio = time.time()\n",
        "df_filtrado = df.filter(\"salario > 50000\")\n",
        "tiempo_filtro = time.time() - inicio\n",
        "\n",
        "print(f\"Tiempo filtro: {tiempo_filtro:.3f} segundos\")\n",
        "# Output: Tiempo filtro: 0.001 segundos  ‚Üê ¬°Tambi√©n instant√°neo!\n",
        "\n",
        "# ü§î ¬øFiltr√≥ 1M registros en 1ms?\n",
        "# Respuesta: ¬°NO! Solo agreg√≥ \"filtro\" al plan\n",
        "\n",
        "# ===== PASO 3: Seleccionar columnas (TRANSFORMACI√ìN - LAZY) =====\n",
        "inicio = time.time()\n",
        "df_seleccionado = df_filtrado.select(\"nombre\", \"salario\", \"departamento\")\n",
        "tiempo_select = time.time() - inicio\n",
        "\n",
        "print(f\"Tiempo select: {tiempo_select:.3f} segundos\")\n",
        "# Output: Tiempo select: 0.001 segundos\n",
        "\n",
        "# ===== PASO 4: Agrupar (TRANSFORMACI√ìN - LAZY) =====\n",
        "inicio = time.time()\n",
        "df_agrupado = df_seleccionado.groupBy(\"departamento\").agg(\n",
        "    avg(\"salario\").alias(\"salario_promedio\")\n",
        ")\n",
        "tiempo_groupby = time.time() - inicio\n",
        "\n",
        "print(f\"Tiempo groupBy: {tiempo_groupby:.3f} segundos\")\n",
        "# Output: Tiempo groupBy: 0.001 segundos\n",
        "\n",
        "# HASTA AQU√ç: ¬°TODO INSTANT√ÅNEO!\n",
        "# Spark solo construy√≥ el PLAN, no ejecut√≥ nada\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"PLAN DE EJECUCI√ìN (lo que Spark registr√≥):\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "1. Leer: /data/empleados.parquet\n",
        "2. Filtrar: salario > 50000\n",
        "3. Seleccionar: nombre, salario, departamento\n",
        "4. Agrupar por: departamento\n",
        "5. Agregar: promedio(salario)\n",
        "\"\"\")\n",
        "\n",
        "# ===== PASO 5: ACCI√ìN - ¬°AQU√ç SE EJECUTA TODO! =====\n",
        "inicio = time.time()\n",
        "resultado = df_agrupado.show()  # ‚Üê ACCI√ìN\n",
        "tiempo_total = time.time() - inicio\n",
        "\n",
        "print(f\"\\\\nTiempo ACCI√ìN (show): {tiempo_total:.2f} segundos\")\n",
        "# Output: Tiempo ACCI√ìN: 3.45 segundos\n",
        "\n",
        "# ‚ö° AHORA S√ç se ejecut√≥ toda la pipeline:\n",
        "#    Leer ‚Üí Filtrar ‚Üí Seleccionar ‚Üí Agrupar ‚Üí Mostrar\n",
        "\n",
        "print(\"\"\"\n",
        "+-------------+------------------+\n",
        "| departamento|salario_promedio  |\n",
        "+-------------+------------------+\n",
        "|           IT|           75000.0|\n",
        "|       Ventas|           62000.0|\n",
        "|          RH |           58000.0|\n",
        "+-------------+------------------+\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"RESUMEN:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Tiempo definiendo transformaciones: ~0.004 segundos\")\n",
        "print(f\"Tiempo ejecutando (acci√≥n):         ~3.45 segundos\")\n",
        "print(f\"\\\\n‚úÖ Spark optimiz√≥ y ejecut√≥ todo en UN SOLO PASO\")\n",
        "\"\"\"\n",
        "\n",
        "print(ejemplo_lazy)\n",
        "```\n",
        "\n",
        "#### **DAG: Directed Acyclic Graph**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "DAG = Plan de Ejecuci√≥n de Spark\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüó∫Ô∏è DAG (DIRECTED ACYCLIC GRAPH)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "dag_explicacion = \"\"\"\n",
        "Cuando defines transformaciones, Spark construye un DAG:\n",
        "Un grafo que representa las dependencias entre operaciones.\n",
        "\n",
        "EJEMPLO: Pipeline de an√°lisis de ventas\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    DAG                              ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                     ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
        "‚îÇ  ‚îÇ  READ   ‚îÇ (Leer CSV de ventas)                 ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
        "‚îÇ       ‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
        "‚îÇ  ‚îÇ FILTER  ‚îÇ (fecha >= '2025-01-01')              ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
        "‚îÇ       ‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
        "‚îÇ  ‚îÇ SELECT  ‚îÇ (producto, cantidad, precio)          ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
        "‚îÇ       ‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                      ‚îÇ\n",
        "‚îÇ  ‚îÇwithColumn‚îÇ (total = cantidad * precio)          ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                      ‚îÇ\n",
        "‚îÇ       ‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
        "‚îÇ  ‚îÇ GROUPBY ‚îÇ (producto)  ‚Üê CAUSA SHUFFLE           ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
        "‚îÇ       ‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
        "‚îÇ  ‚îÇ   AGG   ‚îÇ (sum(total))                          ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
        "‚îÇ       ‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
        "‚îÇ  ‚îÇ ORDERBY ‚îÇ (total DESC)  ‚Üê CAUSA SHUFFLE         ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
        "‚îÇ       ‚îÇ                                             ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
        "‚îÇ  ‚îÇ  SHOW   ‚îÇ ‚Üê ACCI√ìN (ejecuta todo el DAG)       ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "STAGES (Spark divide el DAG en stages):\n",
        "\n",
        "STAGE 1: Read ‚Üí Filter ‚Üí Select ‚Üí withColumn\n",
        "         (Todo narrow, sin shuffle)\n",
        "\n",
        "STAGE 2: GroupBy ‚Üí Agg\n",
        "         (Requiere shuffle - nueva stage)\n",
        "\n",
        "STAGE 3: OrderBy\n",
        "         (Requiere shuffle - nueva stage)\n",
        "\n",
        "STAGE 4: Show\n",
        "         (Recolectar resultados al Driver)\n",
        "\"\"\"\n",
        "\n",
        "print(dag_explicacion)\n",
        "```\n",
        "\n",
        "#### **Optimizaciones de Catalyst**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CATALYST OPTIMIZER: El cerebro detr√°s de Spark SQL\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüß† CATALYST OPTIMIZER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "catalyst_explicacion = \"\"\"\n",
        "Catalyst es el optimizador de Spark que mejora autom√°ticamente\n",
        "tu c√≥digo ANTES de ejecutarlo.\n",
        "\n",
        "EJEMPLO DE OPTIMIZACI√ìN:\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# TU C√ìDIGO (Naive):\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "df = spark.read.parquet(\"/data/ventas/\")  # 500 GB, 50 columnas\n",
        "\n",
        "df_resultado = df \\\\\n",
        "    .select(\"*\") \\\\                          # Seleccionar todas\n",
        "    .filter(\"fecha >= '2025-01-01'\") \\\\      # Filtrar por fecha\n",
        "    .select(\"producto\", \"cantidad\", \"precio\") \\\\  # Solo 3 columnas\n",
        "    .filter(\"precio > 100\")                  # Filtrar por precio\n",
        "\n",
        "df_resultado.count()\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# LO QUE CATALYST OPTIMIZA (Autom√°ticamente):\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# 1. COLUMN PRUNING (Poda de columnas)\n",
        "#    \"Solo necesita 3 columnas, no las 50\"\n",
        "#    ‚Üí Lee solo producto, cantidad, precio, fecha desde disco\n",
        "#    Ahorro: 47 columnas no le√≠das = ~90% menos datos\n",
        "\n",
        "# 2. PREDICATE PUSHDOWN (Empujar filtros)\n",
        "#    \"Los filtros se aplican mejor al leer\"\n",
        "#    ‚Üí Aplica ambos filtros durante la lectura\n",
        "#    Ahorro: Lee menos datos desde disco\n",
        "\n",
        "# 3. FILTER FUSION (Fusi√≥n de filtros)\n",
        "#    \"Dos filtros consecutivos = un filtro combinado\"\n",
        "#    ‚Üí fecha >= '2025-01-01' AND precio > 100\n",
        "#    Ahorro: Un solo paso en lugar de dos\n",
        "\n",
        "# 4. PROJECTION PUSHDOWN\n",
        "#    \"No necesita 'fecha' en el resultado final\"\n",
        "#    ‚Üí No mantiene 'fecha' en memoria despu√©s del filtro\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# PLAN OPTIMIZADO FINAL:\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Plan real que ejecuta Spark:\n",
        "\n",
        "1. Leer SOLO: producto, cantidad, precio, fecha (no las 50)\n",
        "2. Mientras lee, FILTRAR: fecha >= '2025-01-01' AND precio > 100\n",
        "3. Despu√©s de filtrar, DROP fecha (no se necesita)\n",
        "4. Contar resultados\n",
        "\n",
        "RESULTADO:\n",
        "- Original ingenuo: Lee 500 GB\n",
        "- Optimizado Catalyst: Lee ~50 GB (filtros tempranos) + solo 3 columnas\n",
        "- Speedup: 10x m√°s r√°pido ‚ö°\n",
        "- Sin cambiar tu c√≥digo ‚ú®\n",
        "\"\"\"\n",
        "\n",
        "print(catalyst_explicacion)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **Pr√°ctica - Identificando Transformaciones y Acciones**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "EJERCICIO: IDENTIFICA TRANSFORMACIONES VS ACCIONES\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüß™ EJERCICIO PR√ÅCTICO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ejercicio = \"\"\"\n",
        "# Dado el siguiente c√≥digo, identifica:\n",
        "# T = Transformaci√≥n (lazy)\n",
        "# A = Acci√≥n (eager, ejecuta)\n",
        "\n",
        "df = spark.read.csv(\"/data/ventas.csv\", header=True)  # ___\n",
        "\n",
        "df2 = df.filter(\"cantidad > 0\")                       # ___\n",
        "\n",
        "df3 = df2.select(\"producto\", \"cantidad\", \"precio\")    # ___\n",
        "\n",
        "df4 = df3.withColumn(\"total\", col(\"cantidad\") * col(\"precio\"))  # ___\n",
        "\n",
        "df_agrupado = df4.groupBy(\"producto\").sum(\"total\")    # ___\n",
        "\n",
        "print(\"Hasta aqu√≠, ¬øse ejecut√≥ algo?\")                # ___\n",
        "\n",
        "df_agrupado.show()                                     # ___\n",
        "\n",
        "df_agrupado.write.parquet(\"/output/resumen.parquet\")  # ___\n",
        "\n",
        "total_productos = df_agrupado.count()                 # ___\n",
        "\n",
        "print(f\"Total: {total_productos}\")                    # ___\n",
        "\n",
        "top_5 = df_agrupado.orderBy(\"sum(total)\", ascending=False).take(5)  # ___\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# RESPUESTAS:\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "spark.read.csv(...)                 ‚Üí T (lazy)\n",
        "df.filter(...)                      ‚Üí T (lazy)\n",
        "df.select(...)                      ‚Üí T (lazy)\n",
        "df.withColumn(...)                  ‚Üí T (lazy)\n",
        "df.groupBy(...).sum(...)            ‚Üí T (lazy)\n",
        "print(\"Hasta aqu√≠...\")              ‚Üí No es Spark (Python)\n",
        "df_agrupado.show()                  ‚Üí A (ejecuta toda la pipeline hasta aqu√≠)\n",
        "df_agrupado.write.parquet(...)      ‚Üí A (ejecuta y escribe)\n",
        "df_agrupado.count()                 ‚Üí A (ejecuta y cuenta)\n",
        "print(f\"Total...\")                  ‚Üí No es Spark (Python)\n",
        "df_agrupado.orderBy(...).take(5)    ‚Üí A (take es acci√≥n)\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# ¬øCU√ÅNTAS VECES SE EJECUT√ì LA PIPELINE COMPLETA?\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "3 VECES:\n",
        "1. show() - ejecuta todo hasta groupBy\n",
        "2. write.parquet() - ejecuta todo de nuevo\n",
        "3. count() - ejecuta todo de nuevo\n",
        "4. take(5) - ejecuta todo de nuevo (con orderBy adicional)\n",
        "\n",
        "‚ö†Ô∏è PROBLEMA: ¬°4 ejecuciones del mismo procesamiento!\n",
        "\n",
        "üí° SOLUCI√ìN: Usar CACHE\n",
        "\"\"\"\n",
        "\n",
        "print(ejercicio)\n",
        "```\n",
        "\n",
        "#### **Optimizaci√≥n con Cache/Persist**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "CACHE/PERSIST: Evitar re-c√≥mputo\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüíæ OPTIMIZACI√ìN CON CACHE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "cache_explicacion = \"\"\"\n",
        "Cuando una acci√≥n ejecuta, Spark NO guarda los resultados\n",
        "intermedios por defecto. Si ejecutas otra acci√≥n, Spark\n",
        "re-ejecuta TODA la pipeline desde el inicio.\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# SIN CACHE (Ineficiente):\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "df_procesado = df.filter(...).select(...).groupBy(...)  # Lazy\n",
        "\n",
        "df_procesado.show()         # Ejecuta toda la pipeline\n",
        "df_procesado.count()        # Re-ejecuta TODA la pipeline\n",
        "df_procesado.write.parquet(...)  # Re-ejecuta OTRA VEZ\n",
        "\n",
        "‚Üí 3 ejecuciones completas üò´\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# CON CACHE (Eficiente):\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "df_procesado = df.filter(...).select(...).groupBy(...)\n",
        "\n",
        "# IMPORTANTE: cache() es una transformaci√≥n lazy\n",
        "df_procesado = df_procesado.cache()\n",
        "\n",
        "# Primera acci√≥n: ejecuta y guarda en memoria\n",
        "df_procesado.show()         # Ejecuta y cachea\n",
        "\n",
        "# Siguientes acciones: leen de cache (r√°pido)\n",
        "df_procesado.count()        # Lee de cache ‚ö°\n",
        "df_procesado.write.parquet(...)  # Lee de cache ‚ö°\n",
        "\n",
        "‚Üí 1 ejecuci√≥n + 2 lecturas de cache üòé\n",
        "\n",
        "# Liberar cache cuando termines\n",
        "df_procesado.unpersist()\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# PERSIST con niveles de almacenamiento:\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "from pyspark import StorageLevel\n",
        "\n",
        "# Solo memoria (default de cache)\n",
        "df.persist(StorageLevel.MEMORY_ONLY)\n",
        "\n",
        "# Memoria + disco (si no cabe en RAM)\n",
        "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "# Serializado (usa menos RAM pero m√°s lento)\n",
        "df.persist(StorageLevel.MEMORY_ONLY_SER)\n",
        "\n",
        "# Replicado (fault tolerance extra)\n",
        "df.persist(StorageLevel.MEMORY_AND_DISK_2)  # 2 copias\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# ¬øCU√ÅNDO USAR CACHE?\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "‚úÖ USA CACHE cuando:\n",
        "   - Vas a usar el mismo DataFrame m√∫ltiples veces\n",
        "   - El DataFrame es resultado de transformaciones costosas\n",
        "   - Iteraciones (ML training)\n",
        "\n",
        "‚ùå NO USES CACHE cuando:\n",
        "   - Solo usas el DataFrame una vez\n",
        "   - El DataFrame es muy grande y no cabe en memoria\n",
        "   - Es el resultado de lectura simple (ya optimizado)\n",
        "\n",
        "‚ö†Ô∏è REGLA:\n",
        "   Si haces 2+ acciones en el mismo DF ‚Üí considera cache\n",
        "\"\"\"\n",
        "\n",
        "print(cache_explicacion)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Ejercicio 2.3 - Optimizaci√≥n de C√≥digo**\n",
        "\n",
        "```markdown\n",
        "### Ejercicio: Optimiza este c√≥digo\n",
        "\n",
        "**C√≥digo Original** (ineficiente):\n",
        "\n",
        "```python\n",
        "# Leer datos\n",
        "df_ventas = spark.read.parquet(\"/data/ventas/\")\n",
        "\n",
        "# An√°lisis 1: Ventas totales\n",
        "total_ventas = df_ventas.agg(sum(\"monto\")).collect()[0][0]\n",
        "print(f\"Total ventas: {total_ventas}\")\n",
        "\n",
        "# An√°lisis 2: Ventas por producto\n",
        "df_por_producto = df_ventas.groupBy(\"producto\").sum(\"monto\")\n",
        "df_por_producto.show()\n",
        "\n",
        "# An√°lisis 3: Top 10 productos\n",
        "top_10 = df_por_producto.orderBy(col(\"sum(monto)\").desc()).take(10)\n",
        "\n",
        "# An√°lisis 4: Exportar reporte\n",
        "df_por_producto.write.csv(\"/output/reporte.csv\")\n",
        "```\n",
        "\n",
        "**Problemas identificados**:\n",
        "1. _____________\n",
        "2. _____________\n",
        "3. _____________\n",
        "\n",
        "**Tu c√≥digo optimizado**:\n",
        "```python\n",
        "# Escribe aqu√≠ tu versi√≥n optimizada\n",
        "```\n",
        "\n",
        "**Explicaci√≥n de mejoras**:\n",
        "1. _____________\n",
        "2. _____________\n",
        "3. _____________\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Puntos Clave para Recordar**\n",
        "\n",
        "1. **Transformaciones = Lazy** (no ejecutan, construyen plan)\n",
        "2. **Acciones = Eager** (ejecutan el plan completo)\n",
        "3. **Narrow transformations** no requieren shuffle (r√°pidas)\n",
        "4. **Wide transformations** requieren shuffle (lentas pero necesarias)\n",
        "5. **Lazy evaluation** permite optimizaci√≥n autom√°tica\n",
        "6. **Catalyst Optimizer** mejora tu c√≥digo autom√°ticamente\n",
        "7. **DAG** es el plan de ejecuci√≥n que Spark construye\n",
        "8. **Cache/Persist** evita re-computaci√≥n cuando usas un DF m√∫ltiples veces\n",
        "9. **collect()** es peligroso con datos grandes (OOM)\n",
        "10. **Usa .explain()** para ver el plan de ejecuci√≥n\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Tips Profesionales**\n",
        "\n",
        "‚úÖ **Usa .explain() para debugging**\n",
        "```python\n",
        "df.filter(...).groupBy(...).explain(True)\n",
        "# Muestra: Parsed, Analyzed, Optimized y Physical plans\n",
        "```\n",
        "\n",
        "‚úÖ **Minimiza acciones en desarrollo**\n",
        "```python\n",
        "# Mal: m√∫ltiples show() mientras desarrollas\n",
        "df.filter(...).show()\n",
        "df.select(...).show()\n",
        "df.groupBy(...).show()\n",
        "\n",
        "# Bien: construye toda la pipeline, un show() al final\n",
        "df.filter(...).select(...).groupBy(...).show()\n",
        "```\n",
        "\n",
        "‚úÖ **Persist DataFrames usados m√∫ltiples veces**\n",
        "```python\n",
        "df_procesado = df.filter(...).groupBy(...).cache()\n",
        "# Ahora √∫salo m√∫ltiples veces sin re-computaci√≥n\n",
        "```\n",
        "\n",
        "‚úÖ **Monitorea Spark UI**\n",
        "```python\n",
        "# En Databricks, el Spark UI muestra:\n",
        "# - Cu√°ntos stages\n",
        "# - Cu√°ntos shuffles\n",
        "# - Tiempo por operaci√≥n\n",
        "# ‚Üí Identifica cuellos de botella\n",
        "```\n",
        "\n",
        "‚úÖ **Evita collect() en producci√≥n**\n",
        "```python\n",
        "# Mal\n",
        "todos_los_datos = df.collect()  # ¬°OOM con datos grandes!\n",
        "\n",
        "# Bien\n",
        "df.write.parquet(\"/output/\")    # Escribe distribuido\n",
        "# O\n",
        "primeros_1000 = df.limit(1000).collect()  # L√≠mite seguro\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Has completado el Tema 2! üéâ**\n",
        "\n",
        "Ahora entiendes:\n",
        "- Las tres abstracciones de Spark (RDD, DataFrame, Dataset)\n",
        "- La diferencia entre transformaciones y acciones\n",
        "- Lazy evaluation y c√≥mo Spark optimiza autom√°ticamente\n",
        "- Cu√°ndo usar cache para optimizar\n",
        "\n",
        "---\n",
        "\n",
        "¬øListo para comenzar el **M√≥dulo 2: Primeros Pasos Pr√°cticos** con el **Tema 3: Configuraci√≥n del Entorno**? üöÄ"
      ],
      "metadata": {
        "id": "A_qB9M-uYEFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "92-18590Zm6q"
      }
    }
  ]
}